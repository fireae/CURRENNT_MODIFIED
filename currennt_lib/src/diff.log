[1mdiff --git a/Configuration.cpp b/Configuration.cpp[m
[1mindex 6617f4a..675ee55 100644[m
[1m--- a/Configuration.cpp[m
[1m+++ b/Configuration.cpp[m
[36m@@ -128,6 +128,8 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    commonOptions.add_options()[m
        ("help",                                                                              [m
	 "shows this help message")[m
	[32m("version",[m
[32m	 "shows the version")[m
        ("options_file",       [m
	 po::value(&optionsFile),                                       [m
	 "reads the command line options from the file")[m
[36m@@ -194,7 +196,7 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
	 std::string([m
	      std::string("Option for inference in VAE. When z is 2-Dim and vaeManifold=1, ") +[m
	      std::string("vae layer will read in the code z from fraction.outputs()")).c_str())[m
	;
[m
    po::options_description trainingOptions("Training options");[m
    trainingOptions.add_options()[m
[36m@@ -505,7 +507,16 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
	 po::value(&m_exInputExt) ->default_value(""),[m
	 "External inut extension")[m
	("ExtInputDim",[m
	 po::value(&m_exInputDim) [32m->default_value(0),[m
[32m	 "External input dimension")[m
[32m	("ExtInputDirs",[m
[32m	 po::value(&m_exInputDirs) ->default_value(""),[m
[32m	 "External input directory")[m
[32m	("ExtInputExts",[m
[32m	 po::value(&m_exInputExts) ->default_value(""),[m
[32m	 "External inut extension")[m
[32m	("ExtInputDims",[m
[32m	 po::value(&m_exInputDims)[m ->default_value(""),
	 "External input dimension")[m
        ;[m
[m
[36m@@ -588,6 +599,13 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
        exit(0);[m
    }[m
[m
    [32mif (vm.count("version")){[m
[32m	//std::cout << "2017/09/02: checked the bug on wavNetCore" << std::endl;[m
[32m	std::cout << "2017/09/07: hack on reducing the memory usage for wavNetCore, CNN" << std::endl;[m
[32m	std::cout << "2017/09/11: add multiple external files, leakyReLU" << std::endl;[m
[32m	exit(0);[m
[32m    }[m
    
    // load options from autosave[m
    if (!m_continueFile.empty()) {[m
        try {[m
[36m@@ -771,16 +789,19 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
	}[m
    }[m
[m
    [31mif[m[32m/*if[m (m_auxDataDir.size() > 0){
	std::cout << "\tUsing auxilary data. ";[m
	if (m_parallelSequences > 1){[m
	    std::cout << "Parallel training will be turned off." << std::endl;[m
	    m_parallelSequences = 1;[m
	}	[m
	std::cout << std::endl;[m
	[31m}[m[32m}*/[m
[m
    [32mif (m_scheduleSampOpt == 6){[m
[32m	m_parallelSequences = 1; // for beamsize generation in computeForwardPassGen()[m
[32m    }[m
    
    if (m_feedForwardOutputFile.size() > 0 &&[m
	m_mdnVarFixEpochNum > 0 && m_mdnVarFixEpochNum < 999){[m
	std::cout << "\nGeneration mode, mdnVarFixEpochNum is not used";[m
[36m@@ -809,6 +830,11 @@[m [mbool Configuration::trainingMode() const[m
    return m_trainingMode;[m
}[m
[m
[32mbool Configuration::generatingMode() const[m
[32m{[m
[32m    return !m_trainingMode;[m
[32m}[m

bool Configuration::hybridOnlineBatch() const[m
{[m
    return m_hybridOnlineBatch;[m
[36m@@ -1319,11 +1345,25 @@[m [mconst std::string& Configuration::exInputExt() const[m
{[m
    return m_exInputExt;[m
}[m
const [31mstd::string&[m[32mint&[m Configuration::exInputDim() const
{[m
    return m_exInputDim;[m
}[m
[m
[32mconst std::string& Configuration::exInputDirs() const[m
[32m{[m
[32m    return m_exInputDirs;[m
[32m}[m
[32mconst std::string& Configuration::exInputExts() const[m
[32m{[m
[32m    return m_exInputExts;[m
[32m}[m
[32mconst std::string& Configuration::exInputDims() const[m
[32m{[m
[32m    return m_exInputDims;[m
[32m}[m


const int& Configuration::verboseLevel() const[m
{[m
    return m_verbose;[m
[1mdiff --git a/Configuration.cpp~ b/Configuration.cpp~[m
[1mindex 629ee3d..6ad4954 100644[m
[1m--- a/Configuration.cpp~[m
[1m+++ b/Configuration.cpp~[m
[36m@@ -50,15 +50,22 @@[m [mstd::string serializeOptions(const po::variables_map &vm)[m
[m
    for (po::variables_map::const_iterator it = vm.begin(); it != vm.end(); ++it) {[m
        if (it->second.value().type() == typeid(bool))[m
            s += [31mit->first[m[32m(it->first[m + '=' + 
		  [31mboost::lexical_cast<std::string>(boost::any_cast<bool>(it->second.value()));[m[32mboost::lexical_cast<std::string>(boost::any_cast<bool>(it->second.value())));[m
        else if (it->second.value().type() == typeid(unsigned))[m
            s += [31mit->first[m[32m(it->first[m + '=' + 
		  [31mboost::lexical_cast<std::string>(boost::any_cast<unsigned>(it->second.value()));[m[32mboost::lexical_cast<std::string>(boost::any_cast<unsigned>(it->second.value())));[m
        else if (it->second.value().type() == typeid(float))[m
            s += [31mit->first[m[32m(it->first[m + '=' + 
		  [31mboost::lexical_cast<std::string>(boost::any_cast<float>(it->second.value()));[m[32mboost::lexical_cast<std::string>(boost::any_cast<float>(it->second.value())));[m
        else if (it->second.value().type() == typeid(double))[m
            s += [31mit->first[m[32m(it->first[m + '=' + 
		  [31mboost::lexical_cast<std::string>(boost::any_cast<double>(it->second.value()));[m[32mboost::lexical_cast<std::string>(boost::any_cast<double>(it->second.value())));[m
        else if (it->second.value().type() == typeid(std::string))[m
            s += [31mit->first[m[32m(it->first[m + '=' + [31mboost::any_cast<std::string>(it->second.value());[m[32mboost::any_cast<std::string>(it->second.value()));[m
[32m        else if (it->second.value().type() == typeid(int))[m
[32m            s += (it->first + '=' + [m
[32m		  boost::lexical_cast<std::string>(boost::any_cast<int>(it->second.value())));[m
[m
        s += ";;;";[m
    }[m
[36m@@ -119,83 +126,421 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    // create the command line options[m
    po::options_description commonOptions("Common options");[m
    commonOptions.add_options()[m
        ("help",                                                                              
	 "shows this help message")
	[32m("version",[m
[32m	 "shows the version")[m
        ("options_file",       
	 po::value(&optionsFile),                                       
	 "reads the command line options from the file")
        ("network",            
	 po::value(&m_networkFile)      ->default_value("network.jsn"), 
	 "sets the file containing the layout and weights of the neural network")
        ("cuda",               
	 po::value(&m_useCuda)          ->default_value(true),          
	 "use CUDA to accelerate the computations")
        ("list_devices",       
	 po::value(&m_listDevices)      ->default_value(false),         
	 "display list of CUDA devices and exit")
        ("parallel_sequences", 
	 po::value(&m_parallelSequences)->default_value(1),             
	 "sets the number of parallel calculated sequences")
        ("random_seed",        
	 po::value(&m_randomSeed)       ->default_value(0u),            
	 "sets the seed for the random number generator (0 = auto)")
	[32m("verbose",        [m
[32m	 po::value(&m_verbose)       ->default_value(0),            [m
[32m	 "information output level: 0 (default), 1")[m
        ;[m
[m
    po::options_description feedForwardOptions("Forward pass options");[m
    feedForwardOptions.add_options()[m
        ("ff_output_format", 
	 po::value(&feedForwardFormatString)->default_value("single_csv"),  
	 "output format for output layer activations (htk, csv or single_csv)")
        ("ff_output_file", 
	 [31mpo::value(&m_feedForwardOutputFile)->default_value("ff_output.csv"),[m[32mpo::value(&m_feedForwardOutputFile)->default_value(""),[m 
	 "sets the name of the output [31mfile / directory in forward pass mode (directory for htk / csv modes)")[m[32mdirectory")[m
        ("ff_output_kind", 
	 po::value(&m_outputFeatureKind)->default_value(9),                   
	 "sets the parameter kind in case of HTK output (9: user, consult HTK book for details)")
        ("feature_period", 
	 po::value(&m_featurePeriod)->default_value(10),                      
	 "sets the feature period in case of HTK output (in seconds)")
        ("ff_input_file",  
	 po::value(&feedForwardInputFileList),                                
	 "sets the name(s) of the input file(s) in forward pass mode")
        ("revert_std",     
	 po::value(&m_revertStd)->default_value(true),                        
	 [31m"if[m[32m"for[m regression [32mtask, de-normalize the generated data using mean and variance in data.nc")[m
[32m	/* Add 16-04-08 to tap in the output of arbitary layer */[m
[32m	("output_from",    [m
[32m	 po::value(&m_outputTapLayer)->default_value(-1),                     [m
[32m	 "from which layer to get the output ? (input layer[m is [31mperformed, unstandardize[m[32m0. Default from[m the output [31mactivations so that features are on[m[32mlayer) ")[m
[32m	("output_from_gate",[m
[32m	 po::value(&m_outputGateOut)->default_value(false),                  [m
[32m	 std::string([m
[32m	      std::string("if the output layer is a gate layer, get output from gate instead") +[m
[32m	      std::string("of transformation units? (default false)")).c_str())[m
[32m	("mdnUVSigThreshold",[m
[32m	 po::value(&m_mdnUVSigThreshold)->default_value(0.5),                  [m
[32m	 std::string("Threhold for uvsigmoid (0.5)").c_str())[m
[32m	("mdnSoftmaxGenMethod",[m
[32m	 po::value(&m_mdnSoftMaxGenMethod)->default_value(0),[m
[32m	 std::string("Method to generate from softmax (0: one-hot (def); 1: soft merge)").c_str())[m
[32m	("fakeEpochNum",[m
[32m	 po::value(&m_fakeEpochNum)->default_value(-1),[m
[32m	 "")[m
[32m	("vaeGenMethod",[m
[32m	 po::value(&m_vaePlotManifold)->default_value(0),[m
[32m	 std::string([m
[32m	      std::string("Option for inference in VAE. When z is 2-Dim and vaeManifold=1, ") +[m
[32m	      std::string("vae layer will read in[m the [31moriginal targets' scale")[m[32mcode z from fraction.outputs()")).c_str())[m
	;
[m
    po::options_description trainingOptions("Training options");[m
    trainingOptions.add_options()[m
        ("train",               
	 po::value(&m_trainingMode)     ->default_value(false),                 
	 "enables the training mode")
	[32m("print_weight_to",        [m
[32m	 po::value(&m_printWeightPath)  ->default_value(""),                 [m
[32m	 "print the weight to binary file")[m
[32m	("print_weight_opt",[m
[32m	 po::value(&m_printWeightOpt)   ->default_value(1),[m
[32m	 std::string([m
[32m	     std::string("option for printing weight. 0: only weights (default) and macro; ")+[m
[32m	     std::string("1: weights, macro with layertype. For hts_engine.") +[m
[32m	     std::string("2: translate *.autosave to *.jsn")).c_str())[m
        ("stochastic", 
	 po::value(&m_hybridOnlineBatch)->default_value(false),                          
	 "enables weight updates after every mini-batch of parallel calculated sequences")
        ("hybrid_online_batch", 
	 po::value(&m_hybridOnlineBatch)->default_value(false),                 
	 "same as --stochastic (for compatibility)")
        ("shuffle_fractions",   
	 po::value(&m_shuffleFractions) ->default_value(false),                 
	 "shuffles mini-batches in stochastic gradient descent")
        ("shuffle_sequences",   
	 po::value(&m_shuffleSequences) ->default_value(false),                 
	 "shuffles sequences within and across mini-batches")
        ("max_epochs",          
	 po::value(&m_maxEpochs)        ->default_value(DEFAULT_UINT_MAX),      
	 "sets the maximum number of training epochs")
        ("max_epochs_no_best",  
	 po::value(&m_maxEpochsNoBest)  ->default_value(20),                    
	 "sets the maximum number of[31mtraining[m epochs in which no new lowest error could be achieved")
        ("validate_every",      
	 po::value(&m_validateEvery)    ->default_value(1),                     
	 "sets the number of epochs until the validation error is computed")
        ("test_every",          
	 po::value(&m_testEvery)        ->default_value(1),                     
	 "sets the number of epochs until the test error is computed")
        ("optimizer",           
	 po::value(&optimizerString)    ->default_value("steepest_descent"),    
	 "sets the optimizer used for updating the weights")
        ("learning_rate",       
	 po::value(&m_learningRate)     ->default_value((real_t)1e-5, "1e-5"),  
	 "sets the learning rate for the steepest descent optimizer")
        ("momentum",            
	 po::value(&m_momentum)         ->default_value((real_t)0.9,  "0.9"),   
	 "sets the momentum for the steepest descent optimizer")
        ("weight_noise_sigma",  
	 po::value(&m_weightNoiseSigma) ->default_value((real_t)0),            
	 [31m"sets[m[32mstd::string([m
[32m	      std::string("sets[m the standard deviation of the weight noise added [31mfor[m[32mfor") + [m
[32m	      std::string("[m the gradient calculation on every [31mbatch")[m[32mbatch")).c_str())[m
        ("save_network",        
	 po::value(&m_trainedNetwork)   ->default_value("trained_network.jsn"), 
	 "sets the file name of the trained network that will be produced")
	
	/* Add 16-02-22 Wang: for WE updating */[m
	("welearning_rate",     
	 po::value(&m_weLearningRate)   ->default_value((real_t)-1, "0"),         
	 "sets the learning rate for [31mwe")[m[32mwe.")[m
	("mseWeight",           
	 [31mpo::value(&m_readMseWeight)[m[32mpo::value(&m_mseWeightPath)[m    ->default_value(""),                      
	 "path to the[31mMSE[m weight [32mfor calculating the SSE and back-propagation[m (binary float data)")
	[32m("LRDecayRate",       [m
[32m	 po::value(&m_lr_decay_rate)    ->default_value(0.1),                     [m
[32m	 "The rate to decay learning rate (default 0.1). Use Optimizer=4")[m
[32m	/* Add 04-13 Wang: for weight mask*/[m
[32m	("weight_mask",         [m
[32m	 po::value(&m_weightMaskPath)   ->default_value(""),                      [m
[32m	 std::string([m
[32m	      std::string("path to the network transformation matrix mask. The number of data") + [m
[32m	      std::string(" in this file should be equal to the network parameters")).c_str())[m
[32m	("weight_mask_opt",         [m
[32m	 po::value(&m_weightMaskOpt)   ->default_value(0),                      [m
[32m	 std::string([m
[32m	      std::string("Option to read and use the weight mask\n") + [m
[32m	      std::string("\n\t0: the weight mask for normal NN weight (default)") +[m
[32m	      std::string("\n\t1: the weight mask for embedded vectors") +[m
[32m	      std::string("\n\t2: the weight mask for embedded vectors and NN weight")).c_str())[m
[32m	[m
[32m	/* Add 0504 Wang: for MDN flag*/[m
[32m	("mdn_config",          [m
[32m	 po::value(&m_mdnFlagPath)      ->default_value(""),                      [m
[32m	 "path to the MDN flag. ")[m
[32m	("mdn_samplePara",      [m
[32m	 po::value(&m_mdnSamplingPara)  ->default_value((real_t)-4.0, "-4.0"),    [m
[32m	 std::string([m
[32m	      std::string("parameter for MDN sampling. \n") + [m
[32m	      std::string("mdn_samplePara > 0: sampling with var scaled by mdn_samplePara.\n") +[m
[32m	      std::string("mdn_samplePara = -1.0: generate the parameter of the distribution.\n")+ [m
[32m	      std::string("mdn_samplePara < -1.0: not use mdn and mdn generation.")).c_str())[m
[32m	("mdn_EMGenIter",       [m
[32m	 po::value(&m_EMGenIter)        ->default_value(5, "5"),                  [m
[32m	 "Number of iterations for EM generation in MDN (default 5). ")[m
[32m	("varInitPara",         [m
[32m	 po::value(&m_varInitPara)      ->default_value(0.5, "0.5"), [m
[32m	 "Parameter to initialize the bias of MDN mixture unit (default 0.5)")[m
[32m	("vFloorPara",          [m
[32m	 po::value(&m_vFloorPara)       ->default_value(0.0001, "0.0001"), [m
[32m	 "Variance scale parameter for the variance floor (default 0.0001)")[m
[32m	("wInitPara",           [m
[32m	 po::value(&m_wInitPara)        ->default_value(1.0, "1.0"), [m
[32m	 "Internal use")[m
[32m	("tieVariance",         [m
[32m	 po::value(&m_tiedVariance)     ->default_value(true,"false"), [m
[32m	 std::string([m
[32m	      std::string("Whether the variance should be tied across dimension?(default false)")+ [m
[32m	      std::string("It will be ignored if tieVarianceFlag is in the model file (.autosave)")[m
[32m	      ).c_str())[m
[32m	("mdn_sampleParaVec",   [m
[32m	 po::value(&m_mdnVarScaleGen)   ->default_value(""), [m
[32m	 std::string([m
[32m	      std::string("The binary vector of coef to scale variance of the mixture model.") + [m
[32m	      std::string("The length of vector should be equal to the dimension of output of")+[m
[32m	      std::string(" the network. Sigmoid and softmax unit will ignore it")).c_str())[m
[32m	("mdn_secondOutput",[m
[32m	 po::value(&m_secondOutputOpt)  ->default_value(""),[m
[32m	 std::string([m
[32m	      std::string("Control the second output of MDN for feedback. A string of 1/0.")  +[m
[32m	      std::string("1: this MDNUnit use para to feedback. 0: use output")).c_str())[m
[32m	("mdnDyn",            [m
[32m	 po::value(&m_mdnDyn)           ->default_value(""), [m
[32m	 std::string([m
[32m	      std::string("Type of MDN dynamic model.") + [m
[32m	      std::string("Please specify a string of digit as num1_num2_num3..., where the")   +[m
[32m	      std::string("number of digit is equal to the number of MDN units in MDN layer.")  +[m
[32m	      std::string("\n\t0: normal MDN; \n\t1: 1-order AR; \n\t2: context-dependent AR;") + [m
[32m	      std::string("\n\t3: 2-order AR (default 0)") + [m
[32m	      std::string("\nNote, it is ignored if trainableFlag is in the model (.autosave).")+[m
[32m	      std::string("\n      2 is used for context-dependent AR")).c_str())[m
[32m	("tanhAutoReg",       [m
[32m	 po::value(&m_tanhAutoregressive) ->default_value("1"), [m
[32m	 std::string([m
[32m	      std::string("What kind of strategy to learn AR model.") + [m
[32m	      std::string("\n\t0: plain\n\t1:tanh-based 1st order (real poles AR)") +[m
[32m	      std::string("\n\t2:tanh-based 2nd order filter (complex poles AR)")).c_str())[m
[32m	("ReserverZeroFilter", [m
[32m	 po::value(&m_setDynFilterZero)   ->default_value(0), [m
[32m	 "Reserved option for MDN Mixture Dyn units. Don't use it if you don't know it.")[m
[32m	("arrmdnLearning",     [m
[32m	 po::value(&m_arrmdnLearning)     ->default_value(0), [m
[32m	 "An option to set the learning rate for ARRMDN. Don't use it if you don't know the code")[m
[32m	("arrmdnInitVar",      [m
[32m	 po::value(&m_ARRMDNInitVar)      ->default_value(0.01), [m
[32m	 "The variance of Gaussian distribution for initialization the AR parameter")[m
[32m	("arrmdnUpdateInterval", [m
[32m	 po::value(&m_ARRMDNUpdateInterval)->default_value(-1), [m
[32m	 std::string([m
[32m	      std::string("Option for the classical form AR model learning.(default not use) ") + [m
[32m	      std::string("Increase the order of AR model every N epochs ")).c_str())[m
[32m	("Optimizer",            [m
[32m	 po::value(&m_optimizerOption)     ->default_value(0), [m
[32m	 std::string([m
[32m	      std::string("Optimization technique: ") + [m
[32m	      std::string("\n\t0: normal gradient descent SGD (default)") + [m
[32m	      std::string("\n\t1: AdaGrad (except the Trainable MDNLayer).") + [m
[32m	      std::string("\n\t2: Average SGC over the utterance.") + [m
[32m	      std::string("\n\t3: SGD then AdaGrad (together with --OptimizerSecondLR)")+[m
[32m	      std::string("\n\t4: SGD decay the learning rate after validation failed")).c_str())[m
[32m	("OptimizerSecondLR",    [m
[32m	 po::value(&m_secondLearningRate)  ->default_value(0.01), [m
[32m	 "Optimizer==3, it requirs additional learning rate for AdaGrad (0.01 default)")[m
[32m	("ScheduleSampOpt",[m
[32m	 po::value(&m_scheduleSampOpt)  ->default_value(0),[m
[32m	 "Method for schedule sampling. Default 0 (not use schedule sampling)")[m
[32m	("ScheduleSampPara",[m
[32m	 po::value(&m_scheduleSampPara) ->default_value(0),[m
[32m	 "Parameter for schedule sampling. Default 0")[m
[32m	("runningMode",[m
[32m	 po::value(&m_runningMode)      ->default_value(0),[m
[32m	 "Training mode of CURRENNT.\n\t0: default\n\t1: skip layers with 0 LR during backprop")[m
[32m	("mdnVarFixEpochNum",[m
[32m	 po::value(&m_mdnVarFixEpochNum)->default_value(-1),[m
[32m	 "Fix the variance of mdn (GMM) as 1 for this number of epochs. Default (not use)")[m
        ;[m
[m
    po::options_description autosaveOptions("Autosave options");[m
    autosaveOptions.add_options()[m
        ("autosave",        
	 [31mpo::value(&m_autosave)->default_value(false),[m[32mpo::value(&m_autosave)            ->default_value(false),[m 
	 "enables autosave after every epoch")
        ("autosave_best",        
	 [31mpo::value(&m_autosaveBest)->default_value(false),[m[32mpo::value(&m_autosaveBest)        ->default_value(false),[m 
	 "enables autosave on best validation error")
        ("autosave_prefix", 
	 po::value(&m_autosavePrefix),                 
	 "prefix for autosave files; e.g. 'abc/mynet-' [31mwill lead to file names like[m[32m->[m 'mynet-epoch005.autosave' in [31mthe directory[m[32mdir[m 'abc'")
        ("continue",        
	 po::value(&m_continueFile),                   
	 "continues training from an autosave file")
        ;[m
[m
    po::options_description dataFilesOptions("Data file options");[m
    dataFilesOptions.add_options()[m
        ("train_file",        
	 po::value(&trainingFileList),                                 
	 "sets the *.nc file(s) containing the training sequences")
        ("val_file",          
	 po::value(&validationFileList),                               
	 "sets the *.nc file(s) containing the validation sequences")
        ("test_file",         
	 po::value(&testFileList),                                     
	 "sets the *.nc file(s) containing the test sequences")
        ("train_fraction",    
	 po::value(&m_trainingFraction)  ->default_value((real_t)1), 
	 "sets the fraction of the training set to use")
        ("val_fraction",      
	 po::value(&m_validationFraction)->default_value((real_t)1), 
	 "sets the fraction of the validation set to use")
        ("test_fraction",     
	 po::value(&m_testFraction)      ->default_value((real_t)1), 
	 "sets the fraction of the test set to use")
        ("truncate_seq",      
	 po::value(&m_truncSeqLength)    ->default_value(0),         
	 "enables training sequence truncation to given maximum length (0 to disable)")
        ("input_noise_sigma", 
	 po::value(&m_inputNoiseSigma)   ->default_value((real_t)0), 
	 "sets the standard deviation of the input noise for training sets")
        ("input_left_context", 
	 po::value(&m_inputLeftContext) ->default_value(0), 
	 "sets the number of left context frames (first frame is duplicated as necessary)")
        ("input_right_context", 
	 po::value(&m_inputRightContext)->default_value(0), 
	 "sets the number of right context frames (last frame is duplicated as necessary)")
        ("output_time_lag",   
	 po::value(&m_outputTimeLag)->default_value(0),              
	 [31m"sets the time[m[32m"time[m lag [31min the[m[32mfor[m training targets (0 = predict current frame, 1 = predict [31mprevious frame,[m[32mprevious,[m etc.)")
        ("cache_path",        
	 po::value(&m_cachePath)         ->default_value(""),        
	 "sets the cache path where the .nc data is cached for random access")
	
	/* Add 16-02-22 Wang: for WE updating */[m
	("weExternal",          
	 po::value(&m_weUpdate)          ->default_value(false),    
	 "whether update the input word embedding vectors (false)")
	("weIDDim",           
	 po::value(&m_weIDDim)           ->default_value(-1),       
	 "the WE index is the ?-th dimension of the input vector? (-1)")
	("weDim",             
	 po::value(&m_weDim)             ->default_value(0),        
	 "the dimension of the word embedding vectors (0)")
	("weBank",            
	 po::value(&m_weBank)            ->default_value(""),       
	 "the path to the word vectors")
	("trainedModel",      
	 po::value(&m_trainedParameter)  ->default_value(""), 
	 "the path to the trained model paratemeter")
	[32m("trainedModelCtr",   [m
[32m	 po::value(&m_trainedParameterCtr) ->default_value(""), [m
[32m	 std::string([m
[32m	      std::string("trainedModel controller") +[m
[32m	      std::string(". A string of 0/1/2/3 whose length is #layer of NN.") + [m
[32m	      std::string("\n 0: not read this layer") + [m
[32m	      std::string("\n 1: read this layer if number of weights matches") +[m
[32m	      std::string("\n 2: assume column number is the same") + [m
[32m	      std::string("\n 3: assume row numbe is the same") + [m
[32m	      std::string("\n (default: void, read in all parameters in option 1)")).c_str())[m
[32m	("datamv",            [m
[32m	 po::value(&m_datamvPath)          ->default_value(""), [m
[32m	 "the path to the data mv file.")[m
[32m	("txtChaDim",         [m
[32m	 po::value(&m_chaDimLstmCharW)     ->default_value(0), [m
[32m	 "OBSOLETE: the dimension of the bag of character for LstmCharW")[m
[32m	("txtBank",           [m
[32m	 po::value(&m_chaBankPath)         ->default_value(""),       [m
[32m	 "OBSOLETE:the path to the character vectors for LstmCharW")[m
[32m	("weNoiseStartDim",   [m
[32m	 po::value(&m_weNoiseStartDim)     ->default_value(-2), [m
[32m	 "OBSOLETE:the first dimension that will be added with noise in the input layer")[m
[32m	("weNoiseEndDim", [m
[32m	 po::value(&m_weNoiseEndDim)       ->default_value(-1), [m
[32m	 "OBSOLETE:the last dimension that  will be addded with noise in the input layer")[m
[32m	("weNoiseDev",    [m
[32m	 po::value(&m_weNoiseDev)          ->default_value(0.1), [m
[32m	 "standard deviation of the noise that will be added to the word vectors (default 0.1)")[m
[32m	("targetDataType", [m
[32m	 po::value(&m_KLDOutputDataType)   ->default_value(-1),   [m
[32m	 std::string([m
[32m	      std::string("the type of the target data.") +[m
[32m	      std::string("\n\t1: linear domain, zero-mean, uni-var") +[m
[32m	      std::string("\n\t2: log domain, zero-mean, uni-var\n")).c_str())[m
[32m	("KLDLRfactor",    [m
[32m	 po::value(&m_lrFactor)            ->default_value(1),   [m
[32m	 "the factor to scale the training criterion and gradient for KLD. default 1.0")[m
[32m	("AuxDataPath",                   [m
[32m	 po::value(&m_auxDataDir)       ->default_value(""),[m
[32m	 "Auxillary data path. Path to the directory of data")[m
[32m	("AuxDataType",                   [m
[32m	 po::value(&m_auxDataTyp)       ->default_value(-1),[m
[32m	 "Auxillary data type: 0 float, 1 int, 2 char")[m
[32m	("AuxDataExt",                   [m
[32m	 po::value(&m_auxDataExt)       ->default_value(""),[m
[32m	 "Auxillary data extension, e.g. .bin, .sp")[m
[32m	("AuxDataDim",                   [m
[32m	 po::value(&m_auxDataDim)       ->default_value(-1),[m
[32m	 "Auxillary data dimension")[m
[32m	("probDataDir",                   [m
[32m	 po::value(&m_probDataDir)       ->default_value(""),[m
[32m	 "Probabilistic file directory")[m
[32m	("probDataDim",                   [m
[32m	 po::value(&m_probDataDim)       ->default_value(-1),[m
[32m	 "Probabilistic data dimension")[m
[32m	("ExtInputDir",[m
[32m	 po::value(&m_exInputDir) ->default_value(""),[m
[32m	 "External input directory")[m
[32m	("ExtInputExt",[m
[32m	 po::value(&m_exInputExt) ->default_value(""),[m
[32m	 "External inut extension")[m
[32m	("ExtInputDim",[m
[32m	 po::value(&m_exInputDim) ->default_value(0),[m
[32m	 "External input dimension")[m
        ;[m
[m
    po::options_description weightsInitializationOptions("Weight initialization options");[m
    weightsInitializationOptions.add_options()[m
        ("weights_dist",         
	 po::value(&weightsDistString)   ->default_value("uniform"),            
	 "sets the distribution type of the initial weights (uniform or normal)")
        ("weights_uniform_min",  
	 po::value(&m_weightsUniformMin) ->default_value((real_t)-0.1, "-0.1"), 
	 "sets the minimum value of the uniform distribution")
        ("weights_uniform_max",  
	 po::value(&m_weightsUniformMax) ->default_value((real_t)+0.1, "0.1"),  
	 "sets the maximum value of the uniform distribution")
        ("weights_normal_sigma", 
	 po::value(&m_weightsNormalSigma)->default_value((real_t)0.1, "0.1"),   
	 "sets the standard deviation of the normal distribution")
        ("weights_normal_mean",  
	 po::value(&m_weightsNormalMean) ->default_value((real_t)0.0, "0"),     
	 "sets the mean of the normal distribution")
	[32m/* Add 16-04-02 Wang: for initiliaizing the bias for gate of Highway network */[m
[32m	("highway_gate_bias",    [m
[32m	 po::value(&m_highwayBias) -> default_value((real_t)-1.50, "-1.50"),    [m
[32m	 "The bias for the sigmoid function in the gate of highway block (default -1.50)")[m
[32m	("lstm_forget_gate_bias",    [m
[32m	 po::value(&m_lstmForgetIni) -> default_value((real_t)0.0, "0.0"),    [m
[32m	 "The bias to the output of forget gate in LSTM during initialization (default 0.0)")[m
[32m	("aggregate_syn_opt",[m
[32m	 po::value(&m_flagAggregateSyn) ->default_value(0),[m
[32m	 std::string([m
[32m	      std::string("Option for generation using aggregation in feedback layer") +[m
[32m	      std::string("\n 0: no aggregation for generation (even it is used in training)") + [m
[32m	      std::string("\n 1: use aggregation if it is used. (default is 0)")).c_str())[m
        ;[m
[m
    po::positional_options_description positionalOptions;[m
[36m@@ -220,14 +565,17 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
            optionsFile = vm["options_file"].as<std::string>();[m
            std::ifstream file(optionsFile.c_str(), std::ifstream::in);[m
            if (!file.is_open())[m
                throw std::runtime_error(std::string("Could not open options file '") + 
					 optionsFile + "'");
            po::store(po::parse_config_file(file, allOptions), vm);[m
        }[m
        po::notify(vm);[m
    }[m
    catch (const std::exception &e) {[m
        if [31m(!vm.count("help"))[m[32m(!vm.count("help")){[m
            std::cout << "Error while parsing the command line and/or options file: [31m"[m[32m";[m
[32m	    std::cout[m << e.what() << std::endl;
	[32m}[m
[m
        std::cout << "Usage: currennt [options] [options-file]" << std::endl;[m
        std::cout << visibleOptions;[m
[36m@@ -242,6 +590,12 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
        exit(0);[m
    }[m
[m
    [32mif (vm.count("version")){[m
[32m	//std::cout << "2017/09/02: checked the bug on wavNetCore" << std::endl;[m
[32m	std::cout << "2017/09/06: hack on reducing the memory usage for wavNetCore" << std::endl;[m
[32m	exit(0);[m
[32m    }[m
    
    // load options from autosave[m
    if (!m_continueFile.empty()) {[m
        try {[m
[36m@@ -252,7 +606,8 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
            po::notify(vm);[m
        }[m
        catch (const std::exception &e) {[m
            std::cout << "Error while restoring configuration from autosave file: [31m"[m[32m";[m
[32m	    std::cout[m << e.what() << std::endl;
[m
            exit(1);[m
        }[m
[36m@@ -262,13 +617,21 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    m_serializedOptions = internal::serializeOptions(vm);[m
[m
    // split the training file options[m
    boost::algorithm::split(m_trainingFiles, trainingFileList, 
			    boost::algorithm::is_any_of(";,"), 
			    boost::algorithm::token_compress_on);
    if (!validationFileList.empty())[m
        boost::algorithm::split(m_validationFiles, validationFileList, 
				boost::algorithm::is_any_of(";,"), 
				boost::algorithm::token_compress_on);
    if (!testFileList.empty())[m
        boost::algorithm::split(m_testFiles, testFileList, 
				boost::algorithm::is_any_of(";,"), 
				boost::algorithm::token_compress_on);
    if (!feedForwardInputFileList.empty())[m
        boost::algorithm::split(m_feedForwardInputFiles, feedForwardInputFileList, 
				boost::algorithm::is_any_of(";,"), 
				boost::algorithm::token_compress_on);
[m
    // check the optimizer string[m
    if (optimizerString == "rprop")[m
[36m@@ -276,7 +639,8 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    else if (optimizerString == "steepest_descent")[m
        m_optimizer = OPTIMIZER_STEEPESTDESCENT;[m
    else {[m
        std::cout << "ERROR: Invalid optimizer. Possible values: steepest_descent, [31mrprop."[m[32mrprop.";[m
[32m	std::cout[m << std::endl;
        exit(1);[m
    }[m
[m
[36m@@ -292,7 +656,8 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    else if (weightsDistString == "uninorm")[m
	m_weightsDistribution = DISTRIBUTION_UNINORMALIZED;[m
    else {[m
        std::cout << "ERROR: Invalid initial weights distribution [31mtype. Possible[m[32mtype.";[m
[32m	std::cout << "Possible[m values: normal, uniform." << std::endl;
        exit(1);[m
    }[m
[m
[36m@@ -304,7 +669,8 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    else if (feedForwardFormatString == "htk")[m
        m_feedForwardFormat = FORMAT_HTK;[m
    else {[m
        std::cout << "ERROR: Invalid feedforward format [31mstring. Possible[m[32mstring."; [m
[32m	std::cout << "Possible[m values: single_csv, csv, htk." << std::endl;
        exit(1);[m
    }[m
[m
[36m@@ -323,77 +689,113 @@[m [mConfiguration::Configuration(int argc, const char *argv[])[m
    }[m
[m
    // print information about active command line options[m
    [32mstd::cout << "Configuration Infor:" << std::endl;[m
    if (m_trainingMode) {[m
        std::cout << [31m"Started[m[32m"\tTraining Mode: Started[m in [31m"[m[32m";[m
[32m	std::cout[m << (m_hybridOnlineBatch ? "hybrid online/batch" : "batch") <<[31m" training mode." <<[m std::endl;
[m
        if [31m(m_shuffleFractions)[m[32m(m_shuffleFractions){[m
            std::cout << [31m"Mini-batches ("[m[32m"\t\tMini-batches (parallel "[m << m_parallelSequences << " sequences [31meach)[m[32meach)";[m
[32m	    std::cout << "[m will be shuffled during training." << std::endl;
	[32m}[m
        if [31m(m_shuffleSequences)[m[32m(m_shuffleSequences){[m
            std::cout << [31m"Sequences will be[m[32m"\t\tSequences[m shuffled within and across [31mmini-batches during training."[m[32mmini-batches.\n"[m << std::endl;
	[32m}[m
        if (m_inputNoiseSigma != [31m(real_t)0)[m[32m(real_t)0){[m
            std::cout << [31m"Using[m[32m"\t\tUsing[m input noise with [31ma standard deviation[m[32mstd.[m of " << m_inputNoiseSigma <<[31m"." <<[m std::endl;
	[32m}[m
        std::cout << [31m"The trained[m[32m"\t\tWritting[m network[31mwill be written[m  to '" << m_trainedNetwork << "'." << std::endl;
        if (boost::filesystem::exists(m_trainedNetwork))[m
            std::cout << [31m"WARNING: The output file[m[32m"\t\tWARNING: overwriting[m '" << m_trainedNetwork << [31m"' already exists. It[m[32m"'" << std::endl;[m
[32m	[m
[32m    }else if(m_printWeightPath.size()>0){[m
[32m	std::cout << "\tStarted in printing mode. ";[m
[32m	std::cout << "Weight[m will be [31moverwritten!"[m[32mprint to " << m_printWeightPath[m << std::endl;
	
    [31m}[m
[31m    else[m[32m}else[m {
        std::cout << [31m"Started[m[32m"\tStarted[m in forward pass mode." << std::endl;
        std::cout << [31m"The forward pass[m[32m"\tWritting[m output[31mwill be written[m to '" << m_feedForwardOutputFile << "'." << std::endl;
        if (boost::filesystem::exists(m_feedForwardOutputFile))[m
            std::cout << [31m"WARNING: The output file[m[32m"\t\tWARNING: overwriting[m '" << m_feedForwardOutputFile <<[31m"' already exists. It will be overwritten!" <<[m std::endl;
    }[m
[m
    if (m_trainingMode && !m_validationFiles.empty())[m
        std::cout << [31m"Validation error will be calculated[m[32m"\tValidation[m every " << m_validateEvery << " epochs." << std::endl;
    if (m_trainingMode && !m_testFiles.empty())[m
        std::cout << [31m"Test error will be calculated[m[32m"\tTest[m  every " << m_testEvery << " epochs." << std::endl;
[m
    if (m_trainingMode) {[m
        std::cout << [31m"Training will be stopped";[m[32m"\n\tTraining epoch number maximum: ";[m
        if (m_maxEpochs != std::numeric_limits<unsigned>::max())[m
            std::cout <<[31m" after " <<[m m_maxEpochs << [31m" epochs or";[m[32mstd::endl;[m
        std::cout << [31m" if there is[m[32m"\n\tTraining epoch number[m no[31mnew[m lowest validation [31merror within "[m[32merror: ";[m
[32m	std::cout[m << m_maxEpochsNoBest <<[31m" epochs." <<[m std::endl;
    }[m
    [m
    if (m_autosave) {[m
        std::cout << [31m"Autosave[m[32m"\tAutosave[m after EVERY EPOCH enabled." << std::endl;
    }[m
    if (m_autosaveBest) {[m
        std::cout << [31m"Autosave[m[32m"\tAutosave[m on BEST VALIDATION ERROR enabled." << std::endl;
    }[m
[m
    if [31m(m_useCuda)[m[32m(m_useCuda){[m
        std::cout << [31m"Utilizing[m[32m"\tUtilizing[m the GPU [31mfor computations with "[m[32mon ";[m
[32m	std::cout[m << m_parallelSequences << " sequences in parallel." << std::endl;
    [31melse[m[32m}else[m
        std::cout << [31m"WARNING:[m[32m"\tWARNING:[m CUDA option not set. Computations[31mwill be performed[m on the CPU!" << std::endl;
[m
    if (m_trainingMode) {[m
	[32mstd::cout << "\n\tInitialization method:" << std::endl;[m
        if (m_weightsDistribution == [31mDISTRIBUTION_NORMAL)[m[32mDISTRIBUTION_NORMAL){[m
            std::cout << [31m"Normal distribution[m[32m"\t\tNormal dist.[m with [31mmean="[m[32mmean, std:"; [m
[32m	    std::cout[m << m_weightsNormalMean <<[31m" and sigma=" <<[m m_weightsNormalSigma;
        [31melse[m[32m}else[m if (m_weightsDistribution == DISTRIBUTION_UNINORMALIZED)
	    std::cout << [31m"Uniform distribution[m[32m"\t\tUniform dist.[m with layer-wise range" << std::endl;
	[31melse[m[32melse{[m
            std::cout << [31m"Uniform distribution[m[32m"\t\tUniform dist.[m with range [31m["[m[32m[";[m
[32m	    std::cout[m << m_weightsUniformMin << ", " << m_weightsUniformMax << "]";
	[32m}[m
	std::cout << [31m". Random[m[32m"\n\t\tRandom[m seed: " << m_randomSeed << std::endl;
	
    }[m

    [32mif (m_mseWeightPath.size()>0){[m
[32m	std::cout << "\tUsing MSE Weight: " << m_mseWeightPath  << std::endl;[m
[32m    }[m

    [m
    [32m// Checking[m 
    /* Add 16-02-22 Wang: for WE updating */[m
    if (m_weUpdate){[m
	// for checking:[m
	if (m_inputNoiseSigma > 0.0){[m
	    std::cout [31m<< "WARNING: the external[m[32m<<"\tWARNING: input[m vectors are [31mutilized, noise on[m[32mused,[m input [31mwill be[m[32mnoise is[m turned [31moff" <<[m[32moff"<<[m std::endl;
	    m_inputNoiseSigma = 0.0;[m
	}[m
	if (m_weIDDim < 0 || m_weDim < 1 || m_weBank.size()<1){[m
	    std::cout << [31m"ERROR:[m[32m"\tERROR:[m Invalid configuration for WE updating" << std::endl;
	    exit(1);[m
	}[m
    }[m
[m
    [32m/*if (m_auxDataDir.size() > 0){[m
[32m	std::cout << "\tUsing auxilary data. ";[m
	if [31m(m_mseWeightPath.size()>0){[m[32m(m_parallelSequences > 1){[m
	    std::cout << [31m"Using MSE Weight: "[m[32m"Parallel training will be turned off."[m << [31mm_mseWeightPath[m[32mstd::endl;[m
[32m	    m_parallelSequences = 1;[m
[32m	}	[m
[32m	std::cout[m << std::endl;
	[32m}*/[m
[32m[m
[32m    if (m_scheduleSampOpt == 6){[m
[32m	m_parallelSequences = 1;[m
[32m    }[m
[32m    [m
[32m    if (m_feedForwardOutputFile.size() > 0 &&[m
[32m	m_mdnVarFixEpochNum > 0 && m_mdnVarFixEpochNum < 999){[m
[32m	std::cout << "\nGeneration mode, mdnVarFixEpochNum is not used";[m
[32m	m_mdnVarFixEpochNum = -1;[m
    }[m
    [m
    std::cout << std::endl;[m
[36m@@ -418,6 +820,11 @@[m [mbool Configuration::trainingMode() const[m
    return m_trainingMode;[m
}[m
[m
[32mbool Configuration::generatingMode() const[m
[32m{[m
[32m    return !m_trainingMode;[m
[32m}[m

bool Configuration::hybridOnlineBatch() const[m
{[m
    return m_hybridOnlineBatch;[m
[36m@@ -641,16 +1048,31 @@[m [mconst std::string& Configuration::weBankPath() const[m
    return m_weBank;[m
}[m
[m
[32mconst std::string& Configuration::chaBankPath() const[m
[32m{[m
[32m    return m_chaBankPath;[m
[32m}[m


const std::string& Configuration::mseWeightPath() const[m
{[m
    return m_mseWeightPath;[m
}[m
[m
[32mconst std::string& Configuration::weightMaskPath() const[m
[32m{[m
[32m    return m_weightMaskPath;[m
[32m}[m
[m
const std::string& Configuration::trainedParameterPath() const[m
{[m
    return m_trainedParameter;[m
}[m

[32mconst std::string& Configuration::trainedParameterCtr() const[m
[32m{[m
[32m    return m_trainedParameterCtr;[m
[32m}[m
    [m
const unsigned& Configuration::weIDDim() const[m
{[m
[36m@@ -660,6 +1082,12 @@[m [mconst unsigned& Configuration::weDim() const[m
{[m
    return m_weDim;[m
}[m

[32mconst unsigned& Configuration::txtChaDim() const[m
[32m{[m
[32m    return m_chaDimLstmCharW;[m
[32m}[m

bool Configuration::weUpdate() const[m
{[m
    return m_weUpdate;[m
[36m@@ -675,3 +1103,265 @@[m [mbool Configuration::revertStd() const[m
{[m
    return m_revertStd;[m
}[m

[32mconst real_t& Configuration::highwayGateBias() const[m
[32m{[m
[32m    return m_highwayBias;[m
[32m}[m
[32m[m
[32mconst int& Configuration::outputFromWhichLayer() const[m
[32m{[m
[32m    return m_outputTapLayer;[m
[32m}[m
[32m[m
[32m[m
[32mconst bool& Configuration::outputFromGateLayer() const[m
[32m{[m
[32m    return m_outputGateOut;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::lrDecayRate() const[m
[32m{[m
[32m    return m_lr_decay_rate;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::mdnFlagPath() const[m
[32m{[m
[32m    return m_mdnFlagPath;[m
[32m}[m
[32m[m
[32mbool Configuration::mdnFlag() const[m
[32m{[m
[32m    return m_mdnFlagPath.length()>0;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::mdnDyn() const[m
[32m{[m
[32m    return m_mdnDyn;[m
[32m}[m
[32m[m
[32m[m
[32mconst std::string& Configuration::datamvPath() const[m
[32m{[m
[32m    return m_datamvPath;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::mdnPara() const[m
[32m{[m
[32m    return m_mdnSamplingPara;[m
[32m}[m
[32m[m
[32mconst int& Configuration::EMIterNM() const[m
[32m{[m
[32m    return m_EMGenIter;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::getVarInitPara() const[m
[32m{[m
[32m    return m_varInitPara;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::getVFloorPara() const[m
[32m{[m
[32m    return m_vFloorPara;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::getWInitPara() const[m
[32m{[m
[32m    return m_wInitPara;[m
[32m}[m
[32m[m
[32mconst bool& Configuration::getTiedVariance() const[m
[32m{[m
[32m    return m_tiedVariance;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::printWeightPath() const[m
[32m{[m
[32m    return m_printWeightPath;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::mdnVarScaleGen() const[m
[32m{[m
[32m    return m_mdnVarScaleGen;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::tanhAutoregressive() const[m
[32m{[m
[32m    return m_tanhAutoregressive;[m
[32m}[m
[32m[m
[32mconst int& Configuration::zeroFilter() const[m
[32m{[m
[32m    return m_setDynFilterZero;[m
[32m}[m
[32m[m
[32mconst int& Configuration::arrmdnLearning() const[m
[32m{[m
[32m    return m_arrmdnLearning;[m
[32m}[m
[32m[m
[32mconst int& Configuration::weNoiseStartDim() const[m
[32m{[m
[32m    return m_weNoiseStartDim;[m
[32m}[m
[32m[m
[32mconst int& Configuration::weNoiseEndDim() const[m
[32m{[m
[32m    return m_weNoiseEndDim;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::weNoiseDev() const[m
[32m{[m
[32m    return m_weNoiseDev;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::arRMDNInitVar() const[m
[32m{[m
[32m    return m_ARRMDNInitVar;[m
[32m}[m
[32m[m
[32mconst int& Configuration::arRMDNUpdateInterval() const[m
[32m{[m
[32m    return m_ARRMDNUpdateInterval;[m
[32m}[m
[32m[m
[32m[m
[32mconst int& Configuration::KLDOutputDataType() const[m
[32m{[m
[32m    return m_KLDOutputDataType;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::lrFactor() const[m
[32m{[m
[32m    return m_lrFactor;[m
[32m}[m
[32m[m
[32m[m
[32mconst unsigned& Configuration::optimizerOption() const[m
[32m{[m
[32m    return m_optimizerOption;[m
[32m}[m
[32m[m
[32m[m
[32mconst real_t& Configuration::optimizerSecondLR() const[m
[32m{[m
[32m    return m_secondLearningRate;[m
[32m}[m
[32m[m
[32m[m
[32mconst std::string& Configuration::auxillaryDataDir() const[m
[32m{[m
[32m    return m_auxDataDir;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::auxillaryDataExt() const[m
[32m{[m
[32m    return m_auxDataExt;[m
[32m}[m
[32m    [m
[32mconst int& Configuration::auxillaryDataTyp() const[m
[32m{[m
[32m    return m_auxDataTyp;[m
[32m}[m
[32m    [m
[32mconst int& Configuration::auxillaryDataDim() const[m
[32m{[m
[32m    return m_auxDataDim;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::secondOutputOpt() const[m
[32m{[m
[32m    return m_secondOutputOpt;[m
[32m}[m
[32m[m
[32mconst int& Configuration::printWeightOpt() const[m
[32m{[m
[32m    return m_printWeightOpt;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::lstmForgetIni() const[m
[32m{[m
[32m    return m_lstmForgetIni;[m
[32m}[m
[32m[m
[32mconst int& Configuration::probDataDim() const[m
[32m{[m
[32m    return m_probDataDim;[m
[32m}[m
[32m[m
[32mconst std::string& Configuration::probDataDir() const[m
[32m{[m
[32m    return m_probDataDir;[m
[32m}[m
[32m[m
[32mconst int& Configuration::aggregateOpt() const[m
[32m{[m
[32m    return m_flagAggregateSyn;[m
[32m}[m
[32m[m
[32mconst int& Configuration::weightMaskOpt() const[m
[32m{[m
[32m    return m_weightMaskOpt;[m
[32m}[m
[32m[m
[32mconst int& Configuration::scheduleSampOpt() const[m
[32m{[m
[32m    return m_scheduleSampOpt;[m
[32m}[m
[32m[m
[32mconst int& Configuration::scheduleSampPara() const[m
[32m{[m
[32m    return m_scheduleSampPara;[m
[32m}[m
[32m[m
[32mconst real_t& Configuration::mdnUVSigThreshold() const[m
[32m{[m
[32m    return m_mdnUVSigThreshold;[m
[32m}[m
[32m[m
[32m[m
[32mconst int& Configuration::mdnSoftMaxGenMethod() const[m
[32m{[m
[32m    return m_mdnSoftMaxGenMethod;[m
[32m}[m
[32m[m
[32m[m
[32mconst std::string& Configuration::exInputDir() const[m
[32m{[m
[32m    return m_exInputDir;[m
[32m}[m
[32mconst std::string& Configuration::exInputExt() const[m
[32m{[m
[32m    return m_exInputExt;[m
[32m}[m
[32mconst int& Configuration::exInputDim() const[m
[32m{[m
[32m    return m_exInputDim;[m
[32m}[m
[32m[m
[32mconst int& Configuration::verboseLevel() const[m
[32m{[m
[32m    return m_verbose;[m
[32m}[m
[32m[m
[32mconst int& Configuration::fakeEpochNum() const[m
[32m{[m
[32m    return m_fakeEpochNum;[m
[32m}[m
[32m[m
[32m[m
[32mconst int& Configuration::runningMode()  const[m
[32m{[m
[32m    return m_runningMode;[m
[32m}[m
[32m[m
[32mconst int& Configuration::mdnVarUpdateEpoch() const[m
[32m{[m
[32m    return m_mdnVarFixEpochNum;[m
[32m}[m
[32m[m
[32mconst int& Configuration::vaePlotManifold() const[m
[32m{[m
[32m    return m_vaePlotManifold;[m
[32m}[m
[1mdiff --git a/Configuration.hpp b/Configuration.hpp[m
[1mindex 1d6db05..b8f7203 100644[m
[1m--- a/Configuration.hpp[m
[1m+++ b/Configuration.hpp[m
[36m@@ -170,7 +170,10 @@[m [mprivate:[m
    /* Add 20170326 external input*/[m
    std::string m_exInputDir;[m
    std::string m_exInputExt;[m
    [31mstd::string[m[32mint[m         m_exInputDim;
    [32mstd::string m_exInputDirs;[m
[32m    std::string m_exInputExts;[m
[32m    std::string m_exInputDims;[m
[m
    /* Add 20170404 */[m
    int         m_verbose;[m
[36m@@ -215,7 +218,7 @@[m [mprivate:[m
    std::string m_autosavePrefix;[m
    std::string m_continueFile;[m
    std::string m_cachePath;[m
    
    std::vector<std::string> m_trainingFiles;[m
    std::vector<std::string> m_validationFiles;[m
    std::vector<std::string> m_testFiles;[m
[36m@@ -255,7 +258,8 @@[m [mpublic:[m
     * @return True if the NN shall be trained[m
     */[m
    bool trainingMode() const;[m
    [32mbool generatingMode() const;[m
    
    const std::string& printWeightPath() const;[m
    const int& printWeightOpt() const;[m
    /**[m
[36m@@ -681,7 +685,10 @@[m [mpublic:[m
[m
    const std::string& exInputDir() const;[m
    const std::string& exInputExt() const;[m
    const [31mstd::string&[m[32mint&[m exInputDim() const;
    [32mconst std::string& exInputDirs() const;[m
[32m    const std::string& exInputExts() const;[m
[32m    const std::string& exInputDims() const;[m
[m
    const int& verboseLevel() const;[m
[m
[1mdiff --git a/LayerFactory.cu b/LayerFactory.cu[m
[1mindex c568701..3f89e74 100644[m
[1m--- a/LayerFactory.cu[m
[1m+++ b/LayerFactory.cu[m
[36m@@ -54,7 +54,7 @@[m
#include "layers/RnnLayer.hpp"[m
#include "layers/ParaLayer.hpp"[m
#include "layers/FeedBackLayer.hpp"[m
[32m#include "layers/wavNetCore.hpp"[m
#include <stdexcept>[m
[m
[m
[36m@@ -64,7 +64,6 @@[m [mlayers::Layer<TDevice>* LayerFactory<TDevice>::createLayer([m
		const helpers::JsonValue &layerChild,[m
		const helpers::JsonValue &weightsSection, [m
		int parallelSequences, int maxSeqLength, [m
[31m		int chaDim, int maxTxtLength,[m
		layers::Layer<TDevice> *precedingLayer[m
		)[m
{[m
[36m@@ -109,7 +108,8 @@[m [mlayers::Layer<TDevice>* LayerFactory<TDevice>::createLayer([m
        return new FeatMatchLayer<TDevice>(layerChild, *precedingLayer);    [m
    else if (layerType == "vae")[m
        return new VaeMiddleLayer<TDevice>(layerChild, *precedingLayer);    [m
    [32melse if (layerType == "wavnetc")[m
[32m    	return new WavNetCore<TDevice>(layerChild, weightsSection, *precedingLayer);[m
    /*[m
    // not implemented yet[m
    else if (layerType == "lstmw")[m
[36m@@ -119,7 +119,6 @@[m [mlayers::Layer<TDevice>* LayerFactory<TDevice>::createLayer([m
    	return new LstmLayerCharW<TDevice>(layerChild, weightsSection, *precedingLayer, [m
					   chaDim, maxTxtLength, true);[m
    */[m
[31m    // [m
    else if (layerType == "sse"                       || layerType == "weightedsse"  || [m
	     layerType == "rmse"                      || layerType == "ce"  || [m
	     layerType == "wf"                        || layerType == "binary_classification" ||[m
[36m@@ -129,7 +128,6 @@[m [mlayers::Layer<TDevice>* LayerFactory<TDevice>::createLayer([m
	// dynamic_cast<layers::TrainableLayer<TDevice>*>(precedingLayer);[m
        //if (!precedingTrainableLayer)[m
    	//    throw std::runtime_error("Cannot add post output layer after a non trainable layer");[m
[31m[m
        if (layerType == "sse")[m
    	    return new SsePostOutputLayer<TDevice>(layerChild, *precedingLayer);[m
        else if (layerType == "kld")[m
[36m@@ -166,9 +164,6 @@[m [mlayers::Layer<TDevice>* LayerFactory<TDevice>::createSkipAddLayer([m
					   )[m
{[m
    using namespace layers;[m
[31m    [m
[31m    /* Add 0405 Add skip ini */[m
[31m    //if (layerType != "skipadd"){[m
    if (layerType != "skipadd" && layerType != "skipini" && layerType != "skipcat"){[m
	printf("Impossible bug\n");[m
	throw std::runtime_error(std::string("The layer is not skipadd"));[m
[1mdiff --git a/LayerFactory.hpp b/LayerFactory.hpp[m
[1mindex e40a0b3..ddcbeed 100644[m
[1m--- a/LayerFactory.hpp[m
[1m+++ b/LayerFactory.hpp[m
[36m@@ -54,8 +54,6 @@[m [mpublic:[m
        const helpers::JsonValue &weightsSection,[m
        int                       parallelSequences, [m
        int                       maxSeqLength,[m
[31m	int                       chaDim,          // Add 0620, for txt data LstmCharW[m
[31m	int                       maxTxtLength,    // Add 0620, for txt data LstmCharW[m
        layers::Layer<TDevice>   *precedingLayer = NULL[m
        );[m
[m
[1mdiff --git a/MacroDefine.hpp b/MacroDefine.hpp[m
[1mindex aafc8e7..c54cb5e 100644[m
[1m--- a/MacroDefine.hpp[m
[1m+++ b/MacroDefine.hpp[m
[36m@@ -53,13 +53,14 @@[m
#define NN_FEEDBACK_SCHEDULE_MIN 0.000 // Minimal value for the schedule sampling prob parameter[m
#define NN_FEEDBACK_SCHEDULE_SIG 20    // K in 1/(1+exp((x-K))/Para)[m

// Schedule sampling [31mmethod[m[32mand sequence model[m code
#define NN_FEEDBACK_GROUND_TRUTH 0     // use ground truth directly[m
#define NN_FEEDBACK_DROPOUT_1N   1     // dropout, set to 1/N[m
#define NN_FEEDBACK_DROPOUT_ZERO 2     // dropout, set to zero[m
#define NN_FEEDBACK_SC_SOFT      3     // schedule sampling, use soft vector[m
#define NN_FEEDBACK_SC_MAXONEHOT 4     // schedule sampling, use one hot vector of the max prob[m
#define NN_FEEDBACK_SC_RADONEHOT 5     // schedule sampling, use one random output[m
[32m#define NN_FEEDBACK_BEAMSEARCH   6     // beam search (for generation)[m

// Softmax generation method[m
#define NN_SOFTMAX_GEN_BEST      0[m
[36m@@ -77,7 +78,7 @@[m
#define NN_STATE_GAN_DIS_GENDATA       2[m
#define NN_STATE_GAN_GEN               0[m
#define NN_STATE_GAN_GEN_FEATMAT       3[m
#define [31mNN_STATE_GAN_GENERATION_STAGE[m[32mNN_STATE_GENERATION_STAGE[m      4
#define NN_STATE_GAN_NOGAN             5[m

/*** For Normal layers ***/[m
[1mdiff --git a/NeuralNetwork.cpp b/NeuralNetwork.cpp[m
[1mindex b84ebba..7b99a30 100644[m
[1m--- a/NeuralNetwork.cpp[m
[1m+++ b/NeuralNetwork.cpp[m
[36m@@ -33,6 +33,7 @@[m
#include "helpers/misFuncs.hpp"[m
#include <vector>[m
#include <stdexcept>[m
[32m#include <algorithm>[m
#include <cassert>[m

#include <boost/foreach.hpp>[m
[36m@@ -41,36 +42,414 @@[m
#include <boost/random/mersenne_twister.hpp>[m


[32m/* ----- Definition for beam-search generation ----- */[m
[32m/*   Internal class defined for NeuralNetwork only   */[m
[32m/* --------------------------------------------------*/[m
[32mnamespace beamsearch{[m

[32m    // Search state[m
[32m    template <typename TDevice>[m
[32m    class searchState[m
[32m    {[m
[32m	typedef typename TDevice::real_vector real_vector;[m
[32m	typedef typename Cpu::real_vector cpu_real_vector;[m
[32m	typedef typename TDevice::int_vector  int_vector;[m
[32m	typedef typename Cpu::int_vector      cpu_int_vector;[m

[32m    private:[m
[32m	int          m_stateID;    // ID of the current state[m
[32m	real_t       m_prob;       // probability[m
[32m	int          m_timeStep;[m
[32m	int_vector   m_stateTrace; // trace of the state ID[m
[32m	real_vector  m_probTrace;  // trace of the probability distribution[m

[32m	std::vector<int>         m_netStateSize;   // pointer in m_netState[m
[32m	std::vector<real_vector> m_netState;     // hidden variables of the network[m
[32m	[m
[32m	[m
[32m    public:[m
[32m	searchState();[m
[32m	searchState(std::vector<int> &netStateSize, const int maxSeqLength, const int stateNM);[m
[32m	~searchState();[m

[32m	const int      getStateID();[m
[32m	const real_t   getProb();[m
[32m	const int      getStateID(const int id);[m
[32m	const real_t   getProb(const int id);[m
[32m	const int      getTimeStep();[m
[32m	int_vector&    getStateTrace();[m
[32m	real_vector&   getProbTrace();[m
[32m	real_vector&   getNetState(const int id);[m
[32m	[m
[32m	void setStateID(const int stateID);[m
[32m	void setTimeStep(const int timeStep);[m
[32m	void setProb(const real_t prob);[m
[32m	void mulProb(const real_t prob);[m
[32m	void setStateTrace(const int time, const int stateID);[m
[32m	void setProbTrace(const int time, const real_t prob);[m
[32m	void setNetState(const int layerID, real_vector& state);[m
[32m	void liteCopy(searchState<TDevice>& sourceState);[m
[32m	void fullCopy(searchState<TDevice>& sourceState);[m
[32m	void print();[m
[32m    };[m

[32m    template <typename TDevice>[m
[32m    searchState<TDevice>::searchState()[m
[32m	: m_stateID(-1)[m
[32m	, m_prob(1.0)[m
[32m	, m_timeStep(-1)[m
[32m    {[m
[32m	m_netState.clear();[m
[32m	m_stateTrace.clear();[m
[32m	m_probTrace.clear();[m
[32m	m_netStateSize.clear();[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    searchState<TDevice>::searchState(std::vector<int> &netStateSize,[m
[32m				      const int maxSeqLength, const int stateNM)[m
[32m	: m_stateID(-1)[m
[32m	, m_prob(0.0)[m
[32m	, m_timeStep(-1)[m
[32m    {[m
[32m	m_netState.resize(netStateSize.size());[m
[32m	[m
[32m	cpu_real_vector tmp;[m
[32m	int tmpBuf = 0;[m
[32m	for (int i = 0; i < netStateSize.size(); i++) {[m
[32m	    tmpBuf += netStateSize[i];[m
[32m	    tmp.resize(netStateSize[i], 0.0);[m
[32m	    m_netState[i] = tmp;[m
[32m	}[m

[32m	m_netStateSize = netStateSize;[m

[32m	cpu_real_vector tmp2(maxSeqLength * stateNM, 0.0);[m
[32m	m_probTrace = tmp2;[m

[32m	cpu_int_vector tmp3(maxSeqLength, 0);[m
[32m	m_stateTrace = tmp3;[m
[32m	[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    searchState<TDevice>::~searchState()[m
[32m    {[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const int searchState<TDevice>::getStateID()[m
[32m    {[m
[32m	return m_stateID;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const real_t searchState<TDevice>::getProb()[m
[32m    {[m
[32m	return m_prob;[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    const int searchState<TDevice>::getTimeStep()[m
[32m    {[m
[32m	return m_timeStep;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const int searchState<TDevice>::getStateID(const int id)[m
[32m    {[m
[32m	if (id >= m_stateTrace.size())[m
[32m	    throw std::runtime_error("state ID is larger than expected");[m
[32m	return m_stateTrace[id];[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    typename searchState<TDevice>::int_vector& searchState<TDevice>::getStateTrace()[m
[32m    {[m
[32m	return m_stateTrace;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    typename searchState<TDevice>::real_vector& searchState<TDevice>::getProbTrace()[m
[32m    {[m
[32m	return m_probTrace;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::liteCopy(searchState<TDevice>& sourceState)[m
[32m    {[m
[32m	m_stateID    = sourceState.getStateID();[m
[32m	m_prob       = sourceState.getProb();[m
[32m	m_timeStep   = sourceState.getTimeStep();[m
[32m	thrust::copy(sourceState.getStateTrace().begin(),[m
[32m		     sourceState.getStateTrace().end(), m_stateTrace.begin());[m
[32m	thrust::copy(sourceState.getProbTrace().begin(),[m
[32m		     sourceState.getProbTrace().end(), m_probTrace.begin());[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::fullCopy(searchState<TDevice>& sourceState)[m
[32m    {[m
[32m	m_stateID    = sourceState.getStateID();[m
[32m	m_prob       = sourceState.getProb();[m
[32m	m_timeStep   = sourceState.getTimeStep();[m
[32m	thrust::copy(sourceState.getStateTrace().begin(),[m
[32m		     sourceState.getStateTrace().end(), m_stateTrace.begin());[m
[32m	thrust::copy(sourceState.getProbTrace().begin(),[m
[32m		     sourceState.getProbTrace().end(), m_probTrace.begin());[m
[32m	for (int i = 0; i < m_netStateSize.size(); i++){[m
[32m	    this->setNetState(i, sourceState.getNetState(i));[m
[32m	}[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const real_t searchState<TDevice>::getProb(const int id)[m
[32m    {[m
[32m	if (id >= m_probTrace.size())[m
[32m	    throw std::runtime_error("prob ID is larger than expected");[m
[32m	return m_probTrace[id];[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    typename searchState<TDevice>::real_vector& searchState<TDevice>::getNetState(const int id)[m
[32m    {[m
[32m	if (id >= m_netStateSize.size())[m
[32m	    throw std::runtime_error("layer ID is larger than expected");[m
[32m	return m_netState[id];[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setStateID(const int stateID)[m
[32m    {[m
[32m	m_stateID = stateID;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setTimeStep(const int timeStep)[m
[32m    {[m
[32m	m_timeStep = timeStep;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setProb(const real_t prob)[m
[32m    {[m
[32m	m_prob = prob;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::mulProb(const real_t prob)[m
[32m    {[m
[32m	if (prob < 1.1754944e-038f)[m
[32m	    m_prob += (-1e30f);[m
[32m	else[m
[32m	    m_prob += std::log(prob);[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setStateTrace(const int time, const int stateID)[m
[32m    {[m
[32m	if (time >= m_stateTrace.size())[m
[32m	    throw std::runtime_error("setStateTrace, time is larger than expected");[m
[32m	m_stateTrace[time] = stateID;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setProbTrace(const int time, const real_t prob)[m
[32m    {[m
[32m	if (time >= m_probTrace.size())[m
[32m	    throw std::runtime_error("setProbTrace, time is larger than expected");[m
[32m	m_probTrace[time] = prob;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setNetState(const int layerID, real_vector& state)[m
[32m    {[m
[32m	if (layerID >= m_netStateSize.size())[m
[32m	    throw std::runtime_error("setNetState, time is larger than expected");[m
[32m	if (m_netStateSize[layerID] > 0)[m
[32m	    thrust::copy(state.begin(), state.begin()+m_netStateSize[layerID],[m
[32m			 m_netState[layerID].begin());[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::print()[m
[32m    {[m
[32m	printf("%d:%d\t%f\t", m_timeStep, m_stateID, m_prob);[m
[32m	//printf("%d", m_stateTrace.size());[m
[32m	cpu_int_vector tmp = m_stateTrace;[m
[32m	for (int i = 0; i <= m_timeStep; i++)[m
[32m	    printf("%d ", tmp[i]);[m
[32m	printf("\n");[m
[32m    }[m
[32m    [m

[32m    //[m
[32m    struct sortUnit{[m
[32m	real_t prob;[m
[32m	int    idx;[m
[32m    };[m
[32m    [m
[32m    bool compareFunc(sortUnit& a, sortUnit& b){[m
[32m	return a.prob >= b.prob;[m
[32m    }[m

[32m    [m
[32m    // Macro search state[m
[32m    template <typename TDevice>[m
[32m    class searchEngine[m
[32m    {[m
[32m	typedef typename TDevice::real_vector real_vector;[m
[32m	typedef typename Cpu::real_vector cpu_real_vector;[m
[32m	typedef typename TDevice::int_vector  int_vector;[m
[32m	typedef typename Cpu::int_vector      cpu_int_vector;[m
[32m	[m
[32m    private:[m
[32m	std::vector<searchState<TDevice> > m_stateSeq;[m
[32m	std::vector<sortUnit> m_sortUnit;[m
[32m	[m
[32m	int m_beamSize;[m
[32m	int m_stateLength;[m
[32m	int m_validStateNum;[m
[32m	[m
[32m    public:[m
[32m	searchEngine(const int beamSize);	[m
[32m	~searchEngine();[m
[32m	[m

[32m	void setState(const int id, searchState<TDevice> &state);[m
[32m	void setSortUnit(const int id, searchState<TDevice> &state);[m
[32m	void setValidBeamSize(const int num);[m
[32m	[m
[32m	void addState(searchState<TDevice> &state);[m
[32m	void sortSet(const int size);[m
[32m	void printBeam();[m
[32m	[m
[32m	searchState<TDevice>& retrieveState(const int id);[m
[32m	int  getBeamSize();[m
[32m	int  getValidBeamSize();[m

[32m    };[m

[32m    [m
[32m    template <typename TDevice>[m
[32m    searchEngine<TDevice>::searchEngine(const int beamSize)[m
[32m	: m_beamSize(beamSize)[m
[32m	, m_stateLength(0)[m
[32m	, m_validStateNum(0)[m
[32m    {[m
[32m	m_stateSeq.clear();[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    searchEngine<TDevice>::~searchEngine()[m
[32m    {[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::addState(searchState<TDevice> &state)[m
[32m    {[m
[32m	sortUnit tmp;[m
[32m	m_stateSeq.push_back(state);[m
[32m	m_sortUnit.push_back(tmp);[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::setState(const int id, searchState<TDevice> &state)[m
[32m    {[m
[32m	if (id > m_stateSeq.size())[m
[32m	    throw std::runtime_error("beam search state not found");[m
[32m	m_stateSeq[id].fullCopy(state);[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::setSortUnit(const int id, searchState<TDevice> &state)[m
[32m    {[m
[32m	if (id > m_sortUnit.size())[m
[32m	    throw std::runtime_error("beam search state not found");[m
[32m	m_sortUnit[id].prob = state.getProb();[m
[32m	m_sortUnit[id].idx  = id;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::setValidBeamSize(const int num)[m
[32m    {[m
[32m	m_validStateNum = num;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    int searchEngine<TDevice>::getBeamSize()[m
[32m    {[m
[32m	return m_beamSize;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    int searchEngine<TDevice>::getValidBeamSize()[m
[32m    {[m
[32m	return m_validStateNum;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    searchState<TDevice>& searchEngine<TDevice>::retrieveState(const int id)[m
[32m    {[m
[32m	if (id > m_stateSeq.size())[m
[32m	    throw std::runtime_error("beam search state not found");[m
[32m	return m_stateSeq[id];[m
[32m    }[m
[32m	[m
[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::sortSet(const int size)[m
[32m    {[m
[32m	m_validStateNum = (m_beamSize < size)?(m_beamSize):(size);[m
[32m	std::sort(m_sortUnit.begin(), m_sortUnit.begin() + size, compareFunc);[m
[32m	for (int i = 0; i < m_validStateNum; i++){[m
[32m	    if ((m_beamSize + m_sortUnit[i].idx) < m_stateSeq.size())[m
[32m		m_stateSeq[i] = m_stateSeq[m_beamSize + m_sortUnit[i].idx];[m
[32m	    else{[m
[32m		printf("beam search %d unit invalid", m_beamSize + m_sortUnit[i].idx);[m
[32m		throw std::runtime_error("beam search sort error");[m
[32m	    }[m
[32m	}[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::printBeam()[m
[32m    {[m
[32m	for (int i = 0; i < m_validStateNum; i++)[m
[32m	    m_stateSeq[i].print();[m
[32m    }[m
[32m}[m

[32mnamespace internal {[m
    
    bool invalidMiddleMDN(const std::string layerType){
	// check whether the next layer is skip layer
	return (layerType !="skipini" && layerType!="skipadd" && layerType!="skipcat" &&
		layerType !="operator");
    [32m}[m
}[m

[32m/* ----- Definition for NeuralNetwork  ----- */[m
[32m/* ------------------------------------------*/[m
template <typename TDevice>[m
NeuralNetwork<TDevice>::NeuralNetwork([m
 const helpers::JsonDocument &jsonDoc,[m
 int parallelSequences, [m
 int maxSeqLength,[m
[31m int chaDim,[m
[31m int maxTxtLength,[m
 int inputSizeOverride,[m
 int outputSizeOverride[m
 )[m
{[m
    try {[m
	
	[31m//[m[32m/* ----- initialization ----- */[m
	const Configuration &config = Configuration::instance();[m
	[m
        // check the layers and weight sections[m
        if (!jsonDoc->HasMember("layers"))[m
            throw std::runtime_error("Missing section 'layers'");[m
        rapidjson::Value &layersSection  = (*jsonDoc)["layers"];[m

        if (!layersSection.IsArray())[m
            throw std::runtime_error("Section 'layers' is not an array");[m

        helpers::JsonValue weightsSection;[m
        if (jsonDoc->HasMember("weights")) {[m
            if (!(*jsonDoc)["weights"].IsObject())[m
[36m@@ -78,51 +457,90 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
            weightsSection = helpers::JsonValue(&(*jsonDoc)["weights"]);[m
        }[m
	[m
[31m	int cnt = 0;[m
	[m
	// Add 1220, support to the FeedBackLayer[m
	std::vector<int> feedBacklayerId; //[31mlayer[m Idx [31mfor[m[32mof all[m the [31mFeedBackLayer[m[32mfeedBackLayers[m
	feedBacklayerId.clear();[m
[31m	bool flagMDNOutput      = false;[m
[31m	[m
[31m	m_firstFeedBackLayer    = -1;[m
[31m	m_middlePostOutputLayer = -1;[m
[31m	m_featMatchLayer        = -1;[m
[31m	m_vaeLayer              = -1;[m
	[m
	[31mm_trainingEpoch[m[32mm_firstFeedBackLayer[m    = -1;     [31mm_trainingFrac[m[32m// Idx of the first feedback layer[m
[32m	m_middlePostOutputLayer[m = -1;     [31mm_trainingState[m[32m// Idx of the PostOutputLayer inside the network[m
[32m	m_featMatchLayer[m        = -1;     [32m// Idx of the featMatching layer (for GAN)[m
[32m	m_vaeLayer              = -1;     // Idx of the VAE interface layer[m
[32m	int tmp_wavNetCore      = -1;     // Idx of the first waveNet core module (for waveNet)[m
[32m	bool flagSaveMemWavNet  = false;  // Flag to save the mem usage of wavenet in generation[m
[32m	int outputLayerIdx      = -1;     // Idx of the output layer (before postoutput)[m           
	[m
	[32mm_trainingEpoch         = -1;[m     // [31mextract[m[32minitialize[m the [31minformatio[m[32mtraining epoch counter[m
[32m	m_trainingFrac          = -1;     // initialize the training data counter[m
[32m	m_trainingState         = -1;     // initialize the training state[m of [31meach layer[m[32mthe network[m

[32m	/* ----- processing loop ----- */[m
[32m	// preloop to determine type o fnetwork[m
[32m	int counter = 0;[m
        for (rapidjson::Value::ValueIterator layerChild = layersSection.Begin(); [m
	     layerChild != layersSection.End();
	     ++layerChild, [31mcnt++){[m
[31m	    [m
[31m            printf("\nLayer (%d)", cnt);[m[32mcounter++){[m
	    [m
[31m	    // check the layer child type[m
            if (!layerChild->IsObject())[m
                throw std::runtime_error("A layer in the 'layers' array is not an object");[m

[31m            // extract the layer type and create the layer[m
            if (!layerChild->HasMember("type"))[m
                throw std::runtime_error("Missing value 'type' in layer description");[m
            [32mstd::string layerType = (*layerChild)["type"].GetString();[m

[32m	    // logging information[m
[32m	    if (layerType == "feedback"){[m
[32m		// feedback layer[m
[32m		feedBacklayerId.push_back(counter);[m
[32m		if (m_firstFeedBackLayer < 0) m_firstFeedBackLayer = counter;[m
[32m	    }else if (layerType == "middleoutput"){[m
[32m		// for GAN[m
[32m		m_middlePostOutputLayer = counter;[m
[32m	    }else if (layerType == "featmatch"){[m
[32m		// for GAN[m
[32m		m_featMatchLayer = counter;[m
[32m	    }else if (layerType == "vae"){[m
[32m		// for vae[m
[32m		m_vaeLayer       = counter;[m
[32m	    }else if (layerType == "wavnetc" && tmp_wavNetCore < 0){[m
[32m		// for the wavenet component[m
[32m		tmp_wavNetCore   = counter;[m

[32m		// if this is a wavenet and the generation stage requires[m
[32m		// loop over time, turn on the mem-save mode[m
[32m		if (!config.trainingMode() && m_firstFeedBackLayer > 0)[m
[32m		    flagSaveMemWavNet = true;[m
[32m	    }[m
[32m	}[m
[32m	outputLayerIdx = counter - 2; // an output before the postoutput layer[m

[32m	// loop to build each layer[m
[32m	counter = 0;[m
[32m        for (rapidjson::Value::ValueIterator layerChild = layersSection.Begin(); [m
[32m	     layerChild != layersSection.End();[m
[32m	     ++layerChild, counter++){[m
[32m	    [m
[32m            printf("\nLayer (%d)", counter);[m
	    [m
	    [32m// get layer name and type[m
            std::string layerName = (*layerChild)["name"].GetString();[m
	    printf(" [ %s ] ", layerName.c_str());[m
            std::string layerType = (*layerChild)["type"].GetString();[m
	    printf(" %s ", layerType.c_str());[m

            // [31moverride input/output sizes[m[32mupdate the input layer size[m
[32m	    // (I don't know why the original CURRENNT does this, by Xin)[m
[32m	    // this part is transparent to the user[m
            if (inputSizeOverride > 0 && layerType == "input"){[m
		// [31mwith WE[m[32mfor word embedding, input layer size can be changed here[m
		if (config.weUpdate() && config.trainingMode())[m
		    inputSizeOverride += (config.weDim() - 1);[m
		[32m// overwrite the layer size[m
		(*layerChild)["size"].SetInt(inputSizeOverride);[m
            }[m

	    [m
	    /*  [32mOriginal code of CURRENNT, I don't know why this is necessary[m
		Does not work yet, need another way to identify a) postoutput layer (last!) and 
                then the corresponging output layer and type![m
		if (outputSizeOverride > 0 && (*layerChild)["name"].GetString() == "output") {[m
		(*layerChild)["size"].SetInt(outputSizeOverride);[m
[36m@@ -131,23 +549,26 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
		(*layerChild)["size"].SetInt(outputSizeOverride);[m
		}[m
	    */[m

	    [32m// create a layer[m
            try {[m
		
            	layers::Layer<TDevice> *layer;[m
		[m
		/* [31mAdd 02-24 Wang for Residual Network*/[m
[31m		/*[m[32mOriginal code of CURRENNT[m
                if (m_layers.empty())[m
		   layer = LayerFactory<TDevice>::createLayer(layerType, 
		           &*layerChild, weightsSection, parallelSequences, maxSeqLength);
                else[m
		   layer = LayerFactory<TDevice>::createLayer(layerType, 
		           &*layerChild, weightsSection, parallelSequences, maxSeqLength, 
                           m_layers.back().get()); */
		[m
		
                [32mif (m_layers.empty())[m
[32m		{[m
		    // [32mcreate the[m first layer
		    [31mif (m_layers.empty()){[m[32m// check the layer type[m
		    if (layerType == "skipadd"           || layerType == "skipini"       ||[m
			layerType == "skipcat"           ||[m
			layerType == "skippara_logistic" || layerType == "skippara_relu" || [m
[36m@@ -157,19 +578,20 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
			printf("SkipAdd, SkipPara can not be the first hidden layer");[m
			throw std::runtime_error("Error in network.jsn: layer type error\n");[m
		    }[m
		    [32m// create the layer[m
		    layer = LayerFactory<TDevice>::createLayer([m
				layerType,     &*layerChild, [m
				weightsSection, parallelSequences, [m
				[31mmaxSeqLength,   chaDim,   maxTxtLength);[m[32mmaxSeqLength);[m

		[31m}else[m[32m}[m
[32m		else[m if(layerType == "skipadd"           || layerType == "skipini"       ||
			layerType == "skipcat"           ||
			layerType == "skippara_logistic" || layerType == "skippara_relu" || 
			layerType == "skippara_tanh"     || 
			layerType == "skippara_identity")
		{[m
		    
		    // SkipLayers: all the layers that link to the current skip layer[m
		    //  here, it includes the last skip layer and the previous normal [m
		    //  layer connected to this skip layer[m
[36m@@ -184,91 +606,103 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
		    //   check it in SkipParaLayer.cu[m


		    if (m_skipAddLayers.size() == [31m0){[m[32m0)[m
[32m		    {[m
			if (layerType == "skipini" || layerType == "skipadd" ||[m
			    layerType == [31m"skipcat"){[m[32m"skipcat")[m
[32m		        {[m
			    // do nothing[m
			[31m}else{[m[32m}[m
[32m			else[m
[32m			{[m
			    // skippara requires previous skip layers[m
			    throw std::runtime_error("Error: no skipini layer been found");[m
			}[m
		    [31m}else{[m[32m}[m
[32m		    else[m
[32m		    {[m
			if (layerType == [31m"skipini"){[m[32m"skipini")[m
[32m			{[m
			    // do nothing[m
			[31m}else[m[32m}[m
[32m			else[m if (layerType == "skipadd" || layerType == [31m"skipcat"){[m[32m"skipcat")[m
[32m			{[m
[32m			    // skipadd and skipcat can take multiple skiplayers[m
			    BOOST_FOREACH (layers::Layer<TDevice>* skiplayer, [31mm_skipAddLayers){[m[32mm_skipAddLayers)[m
[32m			    {[m
				SkipLayers.push_back(skiplayer);[m
			    }[m
			[31m}else{[m[32m}[m
[32m			else{[m
			    // skippara [32m(highway block) only takes one skiplayer as input source[m
			    SkipLayers.push_back(m_skipAddLayers.back());[m
			}[m
		    }[m

		    // [31mAdd[m[32ma skiplayer can additionally take[m the previous [32mlayer as input source.[m
[32m		    // note, this layer may be a[m normal layer [32mor a skip layer.[m 
		    SkipLayers.push_back(m_layers.back().get());[m
		    [32m// I should check whether the previous layer is still a skip layer[m
		    [m
		    if (layerType == "skipadd" || layerType == "skipini" || layerType == "skipcat")
		    [32m{[m
			layer = LayerFactory<TDevice>::createSkipAddLayer([m
				  layerType,     &*layerChild,[m
				  weightsSection, parallelSequences, [m
				  maxSeqLength,   SkipLayers);[m
		    [32m}[m
		    else[m
		    [32m{[m
			layer = LayerFactory<TDevice>::createSkipParaLayer([m
				  layerType,     &*layerChild,[m
				  weightsSection, parallelSequences, [m
				  maxSeqLength,   SkipLayers);[m
		    [32m}[m
		    // add the skipadd layer to [31mNetwork[m[32mthe[m buffer [32mof the network[m
		    m_skipAddLayers.push_back(layer);[m
		[m
		[31m}else{[m[32m}[m
[32m		else[m
[32m		{[m
		    // other layers types[m
                    layer = LayerFactory<TDevice>::createLayer([m
			       layerType,      &*layerChild,[m
			       weightsSection, parallelSequences, [m
			       maxSeqLength,[31mchaDim, maxTxtLength,[m 
			       m_layers.back().get());[m
[31m		    [m
[31m		    if (layerType == "mdn")[m
[31m			flagMDNOutput = true;[m
		}[m
		[m
[31m                m_layers.push_back(boost::shared_ptr<layers::Layer<TDevice> >(layer));[m

		// [31mWrite down[m[32mpost processing for[m the [31mID of specific layers[m[32mlayer[m
[32m		{[m
[32m		    // for wavenet, reduce the memory in generation[m
		    if [31m(layerType == "feedback"){[m[32m(flagSaveMemWavNet){[m
			// [32monly save the memory for layers between the[m feedback [32mand output[m layer[31mfeedBacklayerId.push_back(cnt);[m
			if [31m(m_firstFeedBackLayer[m[32m(counter[m < [31m0)[m
[31m			m_firstFeedBackLayer    = cnt;[m
[31m		}else if (layerType == "middleoutput"){[m
[31m		    // for GAN[m
[31m		    m_middlePostOutputLayer = cnt;[m
[31m		}else[m[32moutputLayerIdx && counter > m_firstFeedBackLayer){[m
			    if [31m(layerType == "featmatch"){[m[32m(counter != Configuration::instance().outputFromWhichLayer())[m
[32m				layer->reduceOutputBuffer();[m
[32m			}[m
[32m		    }[m
		    
		    // [32mother optimization methods can be used[m for [31mGAN[m
[31m		    m_featMatchLayer = cnt;[m
[31m		}else if (layerType == "vae"){[m
[31m		    m_vaeLayer       = cnt;[m[32mother types of network[m
		}[m

		[32m// save the layer[m
[32m                m_layers.push_back(boost::shared_ptr<layers::Layer<TDevice> >(layer));[m
	       		
            }[m
            catch (const std::exception &e) {[m
                throw std::runtime_error(std::string("Could not create layer: ") + e.what());[m
            }[m
        } [32m// Processing loop done[m
	[m
	[m
	[31m//[m[32m/* ----- post-processing[m check [31mif we have at least one input, one output and one post output layer[m[32m----- */[m
        if (m_layers.size() < 3)[m
            throw std::runtime_error("Error in network.jsn: there must be a hidden layer\n");[m

        // check if only the first layer is an input layer[m
        if (!dynamic_cast<layers::InputLayer<TDevice>*>(m_layers.front().get()))[m
            throw std::runtime_error("The first layer is not an input layer");[m

        for (size_t i = 1; i < m_layers.size(); ++i) {[m
            if (dynamic_cast<layers::InputLayer<TDevice>*>(m_layers[i].get()))[m
                throw std::runtime_error("Multiple input layers defined");[m
[36m@@ -292,17 +726,17 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
		    // tell the last postoutput layer about the existence of middleoutput[m
		    lastPOLayer->linkMiddleOutptu(midPOLayer);[m
		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_MIDDLEOUTPUT);[m
		    if [31m(invalidMiddleMDN(m_layers[i+1]->type()))[m[32m(internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
			throw std::runtime_error("No skipini/add/cat layer after middleoutput");[m
		    [m
		}else if (midPOLayer && midPOLayer->type() == "featmatch" && flagNetworkForGAN()){[m
		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_FEATMATCH);[m
		    if [31m(invalidMiddleMDN(m_layers[i+1]->type()))[m[32m(internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
			throw std::runtime_error("No skipini/add/cat layer after featmatch");[m
		    [m
		}else if (midPOLayer && midPOLayer->type() == "mdn" && flagNetworkForGAN()){[m
		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_NOTLASTMDN);[m
		    if [31m(invalidMiddleMDN(m_layers[i+1]->type()))[m[32m(internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
			throw std::runtime_error("No skipini/add/cat layer after MDN(GAN)");[m
		    [m
		}else if (midPOLayer && midPOLayer->type() == "vae"){[m
[36m@@ -310,7 +744,7 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
		    if (flagNetworkForGAN())[m
			throw std::runtime_error("GAN + VAE is not implemented");[m
		    [m
		    if [31m(invalidMiddleMDN(m_layers[i+1]->type()))[m[32m(internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
			throw std::runtime_error("No skipini/add/cat layer after VAE");[m
		    [m
		}else if (midPOLayer){[m
[36m@@ -318,10 +752,7 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m

		}else{[m
		    // no other postoutput layer[m
		}		    
	    }[m
	}[m
	[m
[36m@@ -337,10 +768,13 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
	[m
	// Link the target layer with the feedback layer[m
	if (!feedBacklayerId.empty()){[m
	    [32mif (flagNetworkForGAN())[m
[32m		throw std::runtime_error("GAN + Feedback is not implemented");[m

	    for (size_t i = 0; i<feedBacklayerId.size(); i++){[m
		m_layers[feedBacklayerId[i]]->linkTargetLayer(*(m_layers.back().get()));[m
	    }[m
	    // check the bi-directional [31mrnn[m[32mRNN after the feedback layer[m
	    for (size_t i = m_firstFeedBackLayer; i < m_layers.size()-1; i++){[m
		if (m_layers[i]->type()==std::string("brnn") ||[m
		    m_layers[i]->type()==std::string("blstm")){[m
[36m@@ -355,6 +789,15 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork([m
		}[m
	    }[m
	}[m

	[32m// Link the wavnet copmonents [m
[32m	if (tmp_wavNetCore > 0){[m
[32m	    for (size_t i = 0; i < m_layers.size(); ++i) {[m
[32m		if (m_layers[i]->type() == std::string("wavnetc")){[m
[32m		    m_layers[i]->linkTargetLayer(*(m_layers[tmp_wavNetCore].get()));[m
[32m		}[m
[32m	    }[m
[32m	}[m
	[m
    }[m
    catch (const std::exception &e) {[m
[36m@@ -552,14 +995,36 @@[m [mvoid NeuralNetwork<TDevice>::computeForwardPass(const int curMaxSeqLength,[m
			randNum.push_back(1);[m
		    }[m
		}[m
[31m		[m
[31m		// Drop out the feedback data randomly[m
[31m		typename TDevice::real_vector temp = randNum;[m
[31m		this->postOutputLayer().retrieveFeedBackData(temp, scheduleSampOpt);[m

		//
		[32mif (m_vaeLayer > 0){[m
[32m		    // If vae exists, don't dropout until after VAE layers[m
[32m		    // ComputeForward[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m			layer->computeForwardPass(m_trainingState);[m
[32m			if (layer->type() == "vae") break;[m
[32m		    }[m
[32m		    // Drop out the feedback data randomly[m
[32m		    typename TDevice::real_vector temp = randNum;[m
[32m		    this->postOutputLayer().retrieveFeedBackData(temp, scheduleSampOpt);[m
[32m		    // computeforward after VAE layers[m
[32m		    int cnt = 0;[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m			if (cnt > m_vaeLayer)[m
[32m			    layer->computeForwardPass(m_trainingState);[m
[32m			cnt++;[m
[32m		    }[m
[32m		    [m
[32m		}else{[m
[32m		    // Normal cases, dropout before feedback[m
[32m		    // Drop out the feedback data randomly[m
[32m		    typename TDevice::real_vector temp = randNum;[m
[32m		    this->postOutputLayer().retrieveFeedBackData(temp, scheduleSampOpt);[m

[32m		    //[m ComputeForward
		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)
			layer->computeForwardPass(m_trainingState);
		[32m}[m
		break;[m
	    }[m

[36m@@ -604,6 +1069,8 @@[m [mvoid NeuralNetwork<TDevice>::computeForwardPass(const int curMaxSeqLength,[m
		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
		    {[m
			if (cnt >= m_firstFeedBackLayer){[m
			    [32m// For Rnn and LSTM, prepareStepGeneration is necessary[m
[32m			    // to prepare the data matrix per frame[m
			    layer->prepareStepGeneration(timeStep); [m
			    layer->computeForwardPass(timeStep, m_trainingState);    [m
			}[m
[36m@@ -665,10 +1132,10 @@[m [mvoid NeuralNetwork<TDevice>::computeForwardPassGen(const int curMaxSeqLength,[m
	int scheduleSampPara= config.scheduleSampPara();[m
	printf("SSAMPOpt: %d, SSAMPPara: %d\n", scheduleSampOpt, scheduleSampPara);[m
	[m
	int    [31mmethodCode;[m[32mmethodCode    = 0;[m
	real_t [31msampThreshold;[m[32msampThreshold = 0.0;[m
	int    cnt           = 0;
	[32m//int    beamSize      = 0;[m
	[m
	// Forward computation for layers before Feedback[m
	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[36m@@ -711,37 +1178,176 @@[m [mvoid NeuralNetwork<TDevice>::computeForwardPassGen(const int curMaxSeqLength,[m
	    methodCode = NN_FEEDBACK_DROPOUT_ZERO;[m
	    sampThreshold = ((real_t)scheduleSampPara)/100;[m
	    break;[m
	    
	// [32mCase 3: beam search[m
[32m	case NN_FEEDBACK_BEAMSEARCH:[m
[32m	    methodCode = NN_FEEDBACK_SC_MAXONEHOT;[m
[32m	    //beamSize   = (int)scheduleSampPara;[m
[32m	    break;[m
	}[m


	[32m//Generation[m
	// [31mlayer above feedback[m[32mNormal generation (Greedy)[m
[32m	if (scheduleSampOpt != NN_FEEDBACK_BEAMSEARCH){[m
	    for (int timeStep = 0, cnt = 0; timeStep < curMaxSeqLength; timeStep ++, cnt = 0){
		[32mBOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m		    if (cnt >= m_firstFeedBackLayer){[m
[32m			// prepare the matrix (for rnn, lstm)[m
[32m			layer->prepareStepGeneration(timeStep);[m
[32m			// compute for 1 frame[m
[32m			layer->computeForwardPass(timeStep, m_trainingState);    [m
[32m		    }[m
[32m		    cnt++;[m
[32m		}[m
[32m		[m
[32m		// Generate the output from MDN[m
[32m		olm = outMDNLayer();[m
[32m		if (olm != NULL) olm->getOutput(timeStep, generationOpt);[m
[32m		[m
[32m		// Feedback the data[m
[32m		if (dist(*gen) < sampThreshold){[m
[32m		    this->postOutputLayer().retrieveFeedBackData(timeStep, 0);[m
[32m		}else{[m
[32m		    this->postOutputLayer().retrieveFeedBackData(timeStep, methodCode);[m
[32m		    printf("%d ", timeStep);[m
[32m		}[m
[32m	    }[m
[32m	    [m
[32m	// Beam search generation[m
[32m	}else{[m
[32m	    [m
[32m	    int stateNum;       // number of states per time step[m
[32m	    int layerCnt;       // counter of the hidden layers[m
[32m	    int beamSize   = (int)scheduleSampPara; // size of beam[m
[32m	    [m
[32m	    /* ----- pre-execution check  ----- */[m
[32m	    if (beamSize < 0)[m
[32m		throw std::runtime_error("beam size cannot be < 1");[m
[32m	    if (m_firstFeedBackLayer < 0)[m
[32m		throw std::runtime_error("No need to use beam size for network without feedback");[m
[32m	    olm = outMDNLayer();[m
[32m	    if (olm == NULL)[m
[32m		throw std::runtime_error("Beam size is used for non-MDN output layer");[m
[32m	    stateNum = olm->mdnParaDim();[m
[32m	    if (beamSize >= stateNum)[m
[32m		throw std::runtime_error("Beam size is larger than the number of state");[m

[32m	    /* ----- initialization ----- */[m
[32m	    // count the number of hidden elements in the network[m
[32m	    std::vector<int> netStateSize;[m
[32m	    int hidEleNum = 0;	// number of hidden elements in the network[m
[32m	    [m
[32m	    layerCnt  = 0;[m
	    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
		if [31m(cnt[m[32m(layerCnt[m >= m_firstFeedBackLayer){
		    [31mlayer->prepareStepGeneration(timeStep); // prepare the matrix (for rnn, lstm)[m
[31m		    layer->computeForwardPass(timeStep, m_trainingState);    // compute for 1 frame[m[32mnetStateSize.push_back(layer->hiddenStateSize());[m
[32m		    hidEleNum += layer->hiddenStateSize();[m
		}[m
		[31mcnt++;[m[32mlayerCnt++;[m
	    }[m
	    [32m// allocate memory spaces for searching [m
[32m	    beamsearch::searchState<TDevice>  bmState(netStateSize, curMaxSeqLength, stateNum);[m
[32m	    beamsearch::searchEngine<TDevice> bmEngine(beamSize);[m
[32m	    for (int i = 0; i < beamSize + beamSize * stateNum; i++)[m
[32m		bmEngine.addState(bmState);[m
[32m	    bmEngine.setValidBeamSize(1);[m
[32m	    std::vector<beamsearch::sortUnit> preSortVec(stateNum);[m
[32m	    // allocate memory spaces for hidden features of network[m
[32m	    Cpu::real_vector netStateTmpTmp(hidEleNum, 0.0);[m
[32m	    typename TDevice::real_vector netStateTmp = netStateTmpTmp;[m


	    [m
	    [31m// if the output is MDN, we need to do one step further to get the output[m
[31m	    olm[m[32m/* ----- Search loop ----- */[m
[32m	    for (int timeStep[m = [31moutMDNLayer();[m
[31m	    if (olm != NULL)[m
[31m		olm->getOutput(timeStep, generationOpt);[m[32m0; timeStep < curMaxSeqLength; timeStep++){[m

		// [31minfer[m[32mCount[m the [31moutput from MDN[m[32mextended number of states[m
[32m		int stateCnt = 0;[m
		[m
		[32m// loop over beam[m
[32m		for (int searchPT = 0; searchPT < bmEngine.getValidBeamSize(); searchPT ++){[m

[32m		    // get the state to be extended[m
[32m		    beamsearch::searchState<TDevice>& bmState2 = bmEngine.retrieveState(searchPT);[m
[32m		    [m
[32m		    // prepare states from bmState2[m
[32m		    bmState.liteCopy(bmState2);[m
[32m		    [m
[32m		    // set the network state[m
[32m		    // 1. set the feedback data[m
[32m		    if (timeStep > 0)[m
[32m			this->postOutputLayer().setFeedBackData(timeStep-1, bmState2.getStateID());[m
[32m		    [m
[32m		    // 2. set the hidde layers and compute[m
[32m		    layerCnt = 0;[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
			if [31m(dist(*gen) < sampThreshold){[m
[31m		this->postOutputLayer().retrieveFeedBackData(timeStep, 0);[m
[31m	    }else{[m
[31m		this->postOutputLayer().retrieveFeedBackData(timeStep, methodCode);[m
[31m		printf("%d ", timeStep);[m[32m(layerCnt >= m_firstFeedBackLayer){[m
[32m			    int layerID = layerCnt - m_firstFeedBackLayer;[m
[32m			    layer->prepareStepGeneration(timeStep);[m
[32m			    if (timeStep > 0)[m
[32m				layer->setHiddenState(timeStep-1, bmState2.getNetState(layerID));[m
[32m			    layer->computeForwardPass(timeStep, m_trainingState);[m

[32m			    // store the state of network in new states [m
[32m			    // this should be in step3. but this is more efficient[m
[32m			    layer->retrieveHiddenState(timeStep, netStateTmp);[m
[32m			    bmState.setNetState(layerID, netStateTmp);[m
[32m			}[m
[32m			layerCnt++;[m
[32m		    }[m
[32m		    // 3. pre-select the states to be explored[m
[32m		    for (int newStateID = 0; newStateID < stateNum; newStateID++){[m
[32m			preSortVec[newStateID].prob = olm->retrieveProb(timeStep, newStateID);[m
[32m			preSortVec[newStateID].idx  = newStateID;[m
[32m		    }[m
[32m		    std::sort(preSortVec.begin(), preSortVec.end(), beamsearch::compareFunc);[m
[32m		    [m
[32m		    // 4. add new search state[m
[32m		    //  probability before this step[m
[32m		    for (int i = 0; i < bmEngine.getBeamSize(); i++){[m
[32m			[m
[32m			bmState.setStateID(preSortVec[i].idx);[m
[32m			bmState.setStateTrace(timeStep, preSortVec[i].idx);[m
[32m			bmState.setTimeStep(timeStep);[m
[32m			if (preSortVec[i].prob < 1e-15f)[m
[32m			    continue; // trim the zero probability path[m
[32m			else[m
[32m			    bmState.setProb(bmState2.getProb() + std::log(preSortVec[i].prob));[m
[32m			bmState.setProbTrace(timeStep, preSortVec[i].prob);[m
[32m			bmEngine.setState(bmEngine.getBeamSize() + stateCnt, bmState);[m
[32m			bmEngine.setSortUnit(stateCnt, bmState);[m
[32m			stateCnt++;[m
[32m		    }[m
[32m		}	[m
[32m		bmEngine.sortSet(stateCnt);[m
[32m		bmEngine.printBeam();[m
	    }[m
	    
	    [32m// Finish the beam search, finally, generate[m
[32m	    bmEngine.printBeam();[m
[32m	    bmState.fullCopy(bmEngine.retrieveState(0));[m
[32m	    for (int timeStep = 0; timeStep < curMaxSeqLength; timeStep++){[m
[32m		if (timeStep > 0)[m
[32m		    this->postOutputLayer().setFeedBackData(timeStep-1,[m
[32m							    bmState.getStateID(timeStep-1));[m
[32m		[m
[32m		layerCnt = 0;[m
[32m		BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m		    if (layerCnt >= m_firstFeedBackLayer){[m
[32m			layer->prepareStepGeneration(timeStep);[m
[32m			layer->computeForwardPass(timeStep, m_trainingState);[m
		    }
		    [31m/*olm[m[32mlayerCnt++;[m
[32m		}[m
[32m		[m
[32m		olm[m = outMDNLayer();
		if (olm != [31mNULL){	[m
[31m	    olm->getOutput(generationOpt);[m
[31m	    }*/[m[32mNULL) olm->getOutput(timeStep, generationOpt);[m
[32m	    }[m
[32m	    [m
[32m	} // Beam search generation[m
	
    } [32m// Generation for network with feedback layers (structured prediction)[m
}[m

template <typename TDevice>[m
[36m@@ -1214,7 +1820,7 @@[m [mvoid NeuralNetwork<TDevice>::updateNNState(const int trainingEpoch, const int fr[m
template <typename TDevice>[m
void NeuralNetwork<TDevice>::updateNNStateForGeneration()[m
{[m
    m_trainingState = [31mNN_STATE_GAN_GENERATION_STAGE;[m[32mNN_STATE_GENERATION_STAGE;[m
}[m


[36m@@ -1228,16 +1834,20 @@[m [mvoid NeuralNetwork<TDevice>::reInitWeight()[m
}[m

template <typename TDevice>[m
void [31mNeuralNetwork<TDevice>::initOutputForMDN([m[32mNeuralNetwork<TDevice>::initOutputForMDN(const helpers::JsonDocument &jsonDoc,[m
					      const data_sets::DataSetMV &datamv)
{[m
    [32mif (jsonDoc->HasMember("weights")) {[m
[32m	printf("\nEscapre MDN initialization\n");[m
[32m	return;[m
[32m    }[m
    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer,[m
		   m_layers){[m
	layers::MDNLayer<TDevice>* mdnLayer = [m
	    dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
	if (mdnLayer){[m
	    mdnLayer->initPreOutput(datamv.outputM(), datamv.outputV());[m
	    [31mprintf("MDN initialization[m[32mprintf("\nRe-initialize the layer before MDN[m \t");
	    if (datamv.outputM().size()<1)[m
		printf("using global zero mean and uni variance");[m
	    else[m
[36m@@ -1247,8 +1857,7 @@[m [mvoid NeuralNetwork<TDevice>::initOutputForMDN([m
}[m

template <typename TDevice>[m
void [31mNeuralNetwork<TDevice>::readMVForOutput([m
[31m const[m[32mNeuralNetwork<TDevice>::readMVForOutput(const[m data_sets::DataSetMV &datamv)
{[m
    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer,[m
		   m_layers){[m
[36m@@ -1270,11 +1879,13 @@[m [mvoid NeuralNetwork<TDevice>::importWeights(const helpers::JsonDocument &jsonDoc,[m
					   const std::string &ctrStr)[m
{[m
    try{[m
	// [31mRead[m[32mStep1. read[m in the control vector, a sequence of 1 0
	Cpu::int_vector tempctrStr;[m
	tempctrStr.resize(m_layers.size(), 1);[m
	if (ctrStr.size() > 0 && ctrStr.size()!= m_layers.size()){[m
	    [31mthrow std::runtime_error("Length[m[32mprintf("\n\tLength[m of [31mtrainedParameterCtr[m[32mtrainedModelCtr is[m unequal [31m#layer.");[m[32mto the number of layers in ");[m
[32m	    printf("the network to be trained\n");[m
[32m	    throw std::runtime_error("Please check trainedModelCtr");[m
	}else if (ctrStr.size()>0){[m
	    for (int i=0; i<ctrStr.size(); i++)[m
		tempctrStr[i] = ctrStr[i]-'0';[m
[36m@@ -1282,7 +1893,7 @@[m [mvoid NeuralNetwork<TDevice>::importWeights(const helpers::JsonDocument &jsonDoc,[m
	    // nothing[m
	}[m
	[m
	// [31mRead[m[32mStep2. read[m in the [31mweight parameter as a whole[m[32mweights from the pre-trained network[m
	helpers::JsonValue weightsSection;[m
        if (jsonDoc->HasMember("weights")) {[m
            if (!(*jsonDoc)["weights"].IsObject())[m
[36m@@ -1292,7 +1903,12 @@[m [mvoid NeuralNetwork<TDevice>::importWeights(const helpers::JsonDocument &jsonDoc,[m
	    throw std::runtime_error("No weight section found");[m
	}[m

	// [31mAssign parameter to[m[32mStep3. for[m each layer [32min the network to be traiend[m
[32m	//        load the weights from the pre-trained network[m
[32m	//        if [m
[32m	//         1. control option is 1[m
[32m	//         2. the name matches (checked handles in Layer->reReadWeight())[m
	
	int cnt=0;[m
	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
	{[m
[36m@@ -1303,17 +1919,7 @@[m [mvoid NeuralNetwork<TDevice>::importWeights(const helpers::JsonDocument &jsonDoc,[m
	    if (Layer && tempctrStr[cnt] > 0){[m
		printf("\n\t(%d) ", cnt);[m
		Layer->reReadWeight(weightsSection, Layer->size(), tempctrStr[cnt]);[m
[31m/*[m
[31m		layers::LstmLayerCharW<TDevice>* LstmCharWLayer = [m
[31m		    dynamic_cast<layers::LstmLayerCharW<TDevice>*>(layer.get());[m
[31m		if (LstmCharWLayer){[m
[31m		    // Because LstmCharWLayer is special[m
[31m		    Layer->reReadWeight(weightsSection, LstmCharWLayer->lstmSize(), [m
[31m					tempctrStr[cnt]);[m
[31m		}else{[m
[31m		   Layer->reReadWeight(weightsSection, Layer->size(), tempctrStr[cnt]); [m
[31m		}*/[m				
	    // Read in the parameter for MDN layer with trainable link[m
	    }else if(tempctrStr[cnt] > 0){[m
		layers::MDNLayer<TDevice>* mdnlayer = [m
[36m@@ -1334,14 +1940,13 @@[m [mvoid NeuralNetwork<TDevice>::importWeights(const helpers::JsonDocument &jsonDoc,[m
	printf("\tdone\n\n");[m
	[m
    }catch (const std::exception &e){[m
	[31mprintf("\n\tTo read[m[32mprintf("\n\t%s\n", e.what());[m
[32m	printf("\n\tError in reading[m weight from [31manother trained network (refer to net2):\n");[m
[31m	printf("\n\t1. prepare network.jsn (net1). Set[m[32ma pre-trained network:");[m
[32m	printf("\n\tPlease check[m the [31mname[m[32mconfiguration[m of [31mthe layer[m[32mtrainedModel and trainedModelCtr");[m
[32m	printf("\n\t1. trainedModel points[m to[31mbe initialized\n");[m
[31m	printf("\n\t   as[m the [31msame name of the source layer in net2. \n");[m[32mcorrect pre-trained network.jsn ?");[m
	printf("\n\t2. [31mset the --trainedModelCtr as the[m[32mtrainedModelCtr,[m a [31mstring[m[32msequence[m of [31mnumber (0/1/2/3), whose \n");[m
[31m	printf("\n\t   length is[m[32m0/1, has[m the same [31mas[m[32mlength as");[m
[32m	printf("the[m number of layers in [31mthe net1.\n");[m
[31m	printf("\n\t   Please check currennt --help for the mearning of 0/1/2/3. \n");[m
[31m	printf("\n\t   If the number for one layer in net1 is 0, or its name can not[m[32mnetwork.jsn to[m be [31mfound \n");[m
[31m	printf("\n\t   in net2, that[m[32mtrained?");[m
[32m	printf("\n\t3. the[m layer [31mwill not[m[32mto[m be [31minitilized using[m[32minitialized has[m the [31mweights in net2.\n");[m[32msame name as the pre-trained layer?\n");[m
	throw std::runtime_error(std::string("Fail to read network weight")+e.what());[m
    }[m
}[m
[36m@@ -1481,6 +2086,13 @@[m [mbool NeuralNetwork<TDevice>::isMDNLayer(const int layerID)[m
	return ((dynamic_cast<layers::MDNLayer<TDevice>*>(m_layers[layerID].get())) != NULL);[m
}[m


[32mtemplate class beamsearch::searchEngine<Gpu>;[m
[32mtemplate class beamsearch::searchEngine<Cpu>;[m

[32mtemplate class beamsearch::searchState<Gpu>;[m
[32mtemplate class beamsearch::searchState<Cpu>;[m

// explicit template instantiations[m
template class NeuralNetwork<Cpu>;[m
template class NeuralNetwork<Gpu>;[m
[1mdiff --git a/NeuralNetwork.cpp~ b/NeuralNetwork.cpp~[m
[1mindex a00639a..509de73 100644[m
[1m--- a/NeuralNetwork.cpp~[m
[1m+++ b/NeuralNetwork.cpp~[m
[36m@@ -16,6 +16,7 @@[m
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the[m
 * GNU General Public License for more details.[m
 *[m
 [32m*[m
 * You should have received a copy of the GNU General Public License[m
 * along with CURRENNT.  If not, see <http://www.gnu.org/licenses/>.[m
 *****************************************************************************/[m
[36m@@ -23,58 +24,521 @@[m
#include "NeuralNetwork.hpp"[m
#include "Configuration.hpp"[m
#include "LayerFactory.hpp"[m
[32m#include "layers/Layer.hpp"[m
#include "layers/InputLayer.hpp"[m
#include "layers/PostOutputLayer.hpp"[m
[32m#include "layers/FeedBackLayer.hpp"[m
#include "helpers/JsonClasses.hpp"[m
[32m#include "MacroDefine.hpp"[m
[32m#include "helpers/misFuncs.hpp"[m
#include <vector>[m
#include <stdexcept>[m
[32m#include <algorithm>[m
#include <cassert>[m

#include <boost/foreach.hpp>[m
[32m#include <boost/random/normal_distribution.hpp>[m
[32m#include <boost/random/uniform_real_distribution.hpp>[m
[32m#include <boost/random/mersenne_twister.hpp>[m


[32m/* ----- Definition for beam-search generation ----- */[m
[32m/*   Internal class defined for NeuralNetwork only   */[m
[32m/* --------------------------------------------------*/[m
[32mnamespace beamsearch{[m

[32m    // Search state[m
[32m    template <typename TDevice>[m
[32m    class searchState[m
[32m    {[m
[32m	typedef typename TDevice::real_vector real_vector;[m
[32m	typedef typename Cpu::real_vector cpu_real_vector;[m
[32m	typedef typename TDevice::int_vector  int_vector;[m
[32m	typedef typename Cpu::int_vector      cpu_int_vector;[m

[32m    private:[m
[32m	int          m_stateID;    // ID of the current state[m
[32m	real_t       m_prob;       // probability[m
[32m	int          m_timeStep;[m
[32m	int_vector   m_stateTrace; // trace of the state ID[m
[32m	real_vector  m_probTrace;  // trace of the probability distribution[m

[32m	std::vector<int>         m_netStateSize;   // pointer in m_netState[m
[32m	std::vector<real_vector> m_netState;     // hidden variables of the network[m
[32m	[m
[32m	[m
[32m    public:[m
[32m	searchState();[m
[32m	searchState(std::vector<int> &netStateSize, const int maxSeqLength, const int stateNM);[m
[32m	~searchState();[m

[32m	const int      getStateID();[m
[32m	const real_t   getProb();[m
[32m	const int      getStateID(const int id);[m
[32m	const real_t   getProb(const int id);[m
[32m	const int      getTimeStep();[m
[32m	int_vector&    getStateTrace();[m
[32m	real_vector&   getProbTrace();[m
[32m	real_vector&   getNetState(const int id);[m
[32m	[m
[32m	void setStateID(const int stateID);[m
[32m	void setTimeStep(const int timeStep);[m
[32m	void setProb(const real_t prob);[m
[32m	void mulProb(const real_t prob);[m
[32m	void setStateTrace(const int time, const int stateID);[m
[32m	void setProbTrace(const int time, const real_t prob);[m
[32m	void setNetState(const int layerID, real_vector& state);[m
[32m	void liteCopy(searchState<TDevice>& sourceState);[m
[32m	void fullCopy(searchState<TDevice>& sourceState);[m
[32m	void print();[m
[32m    };[m

[32m    template <typename TDevice>[m
[32m    searchState<TDevice>::searchState()[m
[32m	: m_stateID(-1)[m
[32m	, m_prob(1.0)[m
[32m	, m_timeStep(-1)[m
[32m    {[m
[32m	m_netState.clear();[m
[32m	m_stateTrace.clear();[m
[32m	m_probTrace.clear();[m
[32m	m_netStateSize.clear();[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    searchState<TDevice>::searchState(std::vector<int> &netStateSize,[m
[32m				      const int maxSeqLength, const int stateNM)[m
[32m	: m_stateID(-1)[m
[32m	, m_prob(0.0)[m
[32m	, m_timeStep(-1)[m
[32m    {[m
[32m	m_netState.resize(netStateSize.size());[m
[32m	[m
[32m	cpu_real_vector tmp;[m
[32m	int tmpBuf = 0;[m
[32m	for (int i = 0; i < netStateSize.size(); i++) {[m
[32m	    tmpBuf += netStateSize[i];[m
[32m	    tmp.resize(netStateSize[i], 0.0);[m
[32m	    m_netState[i] = tmp;[m
[32m	}[m

[32m	m_netStateSize = netStateSize;[m

[32m	cpu_real_vector tmp2(maxSeqLength * stateNM, 0.0);[m
[32m	m_probTrace = tmp2;[m

[32m	cpu_int_vector tmp3(maxSeqLength, 0);[m
[32m	m_stateTrace = tmp3;[m
[32m	[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    searchState<TDevice>::~searchState()[m
[32m    {[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const int searchState<TDevice>::getStateID()[m
[32m    {[m
[32m	return m_stateID;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const real_t searchState<TDevice>::getProb()[m
[32m    {[m
[32m	return m_prob;[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    const int searchState<TDevice>::getTimeStep()[m
[32m    {[m
[32m	return m_timeStep;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const int searchState<TDevice>::getStateID(const int id)[m
[32m    {[m
[32m	if (id >= m_stateTrace.size())[m
[32m	    throw std::runtime_error("state ID is larger than expected");[m
[32m	return m_stateTrace[id];[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    typename searchState<TDevice>::int_vector& searchState<TDevice>::getStateTrace()[m
[32m    {[m
[32m	return m_stateTrace;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    typename searchState<TDevice>::real_vector& searchState<TDevice>::getProbTrace()[m
[32m    {[m
[32m	return m_probTrace;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::liteCopy(searchState<TDevice>& sourceState)[m
[32m    {[m
[32m	m_stateID    = sourceState.getStateID();[m
[32m	m_prob       = sourceState.getProb();[m
[32m	m_timeStep   = sourceState.getTimeStep();[m
[32m	thrust::copy(sourceState.getStateTrace().begin(),[m
[32m		     sourceState.getStateTrace().end(), m_stateTrace.begin());[m
[32m	thrust::copy(sourceState.getProbTrace().begin(),[m
[32m		     sourceState.getProbTrace().end(), m_probTrace.begin());[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::fullCopy(searchState<TDevice>& sourceState)[m
[32m    {[m
[32m	m_stateID    = sourceState.getStateID();[m
[32m	m_prob       = sourceState.getProb();[m
[32m	m_timeStep   = sourceState.getTimeStep();[m
[32m	thrust::copy(sourceState.getStateTrace().begin(),[m
[32m		     sourceState.getStateTrace().end(), m_stateTrace.begin());[m
[32m	thrust::copy(sourceState.getProbTrace().begin(),[m
[32m		     sourceState.getProbTrace().end(), m_probTrace.begin());[m
[32m	for (int i = 0; i < m_netStateSize.size(); i++){[m
[32m	    this->setNetState(i, sourceState.getNetState(i));[m
[32m	}[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    const real_t searchState<TDevice>::getProb(const int id)[m
[32m    {[m
[32m	if (id >= m_probTrace.size())[m
[32m	    throw std::runtime_error("prob ID is larger than expected");[m
[32m	return m_probTrace[id];[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    typename searchState<TDevice>::real_vector& searchState<TDevice>::getNetState(const int id)[m
[32m    {[m
[32m	if (id >= m_netStateSize.size())[m
[32m	    throw std::runtime_error("layer ID is larger than expected");[m
[32m	return m_netState[id];[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setStateID(const int stateID)[m
[32m    {[m
[32m	m_stateID = stateID;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setTimeStep(const int timeStep)[m
[32m    {[m
[32m	m_timeStep = timeStep;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setProb(const real_t prob)[m
[32m    {[m
[32m	m_prob = prob;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::mulProb(const real_t prob)[m
[32m    {[m
[32m	if (prob < 1.1754944e-038f)[m
[32m	    m_prob += (-1e30f);[m
[32m	else[m
[32m	    m_prob += std::log(prob);[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setStateTrace(const int time, const int stateID)[m
[32m    {[m
[32m	if (time >= m_stateTrace.size())[m
[32m	    throw std::runtime_error("setStateTrace, time is larger than expected");[m
[32m	m_stateTrace[time] = stateID;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setProbTrace(const int time, const real_t prob)[m
[32m    {[m
[32m	if (time >= m_probTrace.size())[m
[32m	    throw std::runtime_error("setProbTrace, time is larger than expected");[m
[32m	m_probTrace[time] = prob;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::setNetState(const int layerID, real_vector& state)[m
[32m    {[m
[32m	if (layerID >= m_netStateSize.size())[m
[32m	    throw std::runtime_error("setNetState, time is larger than expected");[m
[32m	if (m_netStateSize[layerID] > 0)[m
[32m	    thrust::copy(state.begin(), state.begin()+m_netStateSize[layerID],[m
[32m			 m_netState[layerID].begin());[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchState<TDevice>::print()[m
[32m    {[m
[32m	printf("%d:%d\t%f\t", m_timeStep, m_stateID, m_prob);[m
[32m	//printf("%d", m_stateTrace.size());[m
[32m	cpu_int_vector tmp = m_stateTrace;[m
[32m	for (int i = 0; i <= m_timeStep; i++)[m
[32m	    printf("%d ", tmp[i]);[m
[32m	printf("\n");[m
[32m    }[m
[32m    [m

[32m    //[m
[32m    struct sortUnit{[m
[32m	real_t prob;[m
[32m	int    idx;[m
[32m    };[m
[32m    [m
[32m    bool compareFunc(sortUnit& a, sortUnit& b){[m
[32m	return a.prob >= b.prob;[m
[32m    }[m

[32m    [m
[32m    // Macro search state[m
[32m    template <typename TDevice>[m
[32m    class searchEngine[m
[32m    {[m
[32m	typedef typename TDevice::real_vector real_vector;[m
[32m	typedef typename Cpu::real_vector cpu_real_vector;[m
[32m	typedef typename TDevice::int_vector  int_vector;[m
[32m	typedef typename Cpu::int_vector      cpu_int_vector;[m
[32m	[m
[32m    private:[m
[32m	std::vector<searchState<TDevice> > m_stateSeq;[m
[32m	std::vector<sortUnit> m_sortUnit;[m
[32m	[m
[32m	int m_beamSize;[m
[32m	int m_stateLength;[m
[32m	int m_validStateNum;[m
[32m	[m
[32m    public:[m
[32m	searchEngine(const int beamSize);	[m
[32m	~searchEngine();[m
[32m	[m

[32m	void setState(const int id, searchState<TDevice> &state);[m
[32m	void setSortUnit(const int id, searchState<TDevice> &state);[m
[32m	void setValidBeamSize(const int num);[m
[32m	[m
[32m	void addState(searchState<TDevice> &state);[m
[32m	void sortSet(const int size);[m
[32m	void printBeam();[m
[32m	[m
[32m	searchState<TDevice>& retrieveState(const int id);[m
[32m	int  getBeamSize();[m
[32m	int  getValidBeamSize();[m

[32m    };[m

[32m    [m
[32m    template <typename TDevice>[m
[32m    searchEngine<TDevice>::searchEngine(const int beamSize)[m
[32m	: m_beamSize(beamSize)[m
[32m	, m_stateLength(0)[m
[32m	, m_validStateNum(0)[m
[32m    {[m
[32m	m_stateSeq.clear();[m
[32m    }[m

    [32mtemplate <typename TDevice>[m
[32m    searchEngine<TDevice>::~searchEngine()[m
[32m    {[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::addState(searchState<TDevice> &state)[m
[32m    {[m
[32m	sortUnit tmp;[m
[32m	m_stateSeq.push_back(state);[m
[32m	m_sortUnit.push_back(tmp);[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::setState(const int id, searchState<TDevice> &state)[m
[32m    {[m
[32m	if (id > m_stateSeq.size())[m
[32m	    throw std::runtime_error("beam search state not found");[m
[32m	m_stateSeq[id].fullCopy(state);[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::setSortUnit(const int id, searchState<TDevice> &state)[m
[32m    {[m
[32m	if (id > m_sortUnit.size())[m
[32m	    throw std::runtime_error("beam search state not found");[m
[32m	m_sortUnit[id].prob = state.getProb();[m
[32m	m_sortUnit[id].idx  = id;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::setValidBeamSize(const int num)[m
[32m    {[m
[32m	m_validStateNum = num;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    int searchEngine<TDevice>::getBeamSize()[m
[32m    {[m
[32m	return m_beamSize;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    int searchEngine<TDevice>::getValidBeamSize()[m
[32m    {[m
[32m	return m_validStateNum;[m
[32m    }[m

    [32mtemplate <typename TDevice>[m
[32m    searchState<TDevice>& searchEngine<TDevice>::retrieveState(const int id)[m
[32m    {[m
[32m	if (id > m_stateSeq.size())[m
[32m	    throw std::runtime_error("beam search state not found");[m
[32m	return m_stateSeq[id];[m
[32m    }[m
[32m	[m
[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::sortSet(const int size)[m
[32m    {[m
[32m	m_validStateNum = (m_beamSize < size)?(m_beamSize):(size);[m
[32m	std::sort(m_sortUnit.begin(), m_sortUnit.begin() + size, compareFunc);[m
[32m	for (int i = 0; i < m_validStateNum; i++){[m
[32m	    if ((m_beamSize + m_sortUnit[i].idx) < m_stateSeq.size())[m
[32m		m_stateSeq[i] = m_stateSeq[m_beamSize + m_sortUnit[i].idx];[m
[32m	    else{[m
[32m		printf("beam search %d unit invalid", m_beamSize + m_sortUnit[i].idx);[m
[32m		throw std::runtime_error("beam search sort error");[m
[32m	    }[m
[32m	}[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void searchEngine<TDevice>::printBeam()[m
[32m    {[m
[32m	for (int i = 0; i < m_validStateNum; i++)[m
[32m	    m_stateSeq[i].print();[m
[32m    }[m
[32m}[m

[32mnamespace internal {[m
[32m    [m
[32m    bool invalidMiddleMDN(const std::string layerType){[m
[32m	// check whether the next layer is skip layer[m
[32m	return (layerType !="skipini" && layerType!="skipadd" && layerType!="skipcat" &&[m
[32m		layerType !="operator");[m
[32m    }[m
[32m}[m

[32m/* ----- Definition for NeuralNetwork  ----- */[m
[32m/* ------------------------------------------*/[m
template <typename TDevice>[m
[31mNeuralNetwork<TDevice>::NeuralNetwork(const[m[32mNeuralNetwork<TDevice>::NeuralNetwork([m
[32m const[m helpers::JsonDocument &jsonDoc,
 int parallelSequences, 
 int maxSeqLength,
 int [31minputSizeOverride = -1,[m[32minputSizeOverride,[m
 int outputSizeOverride
 [31m= -1)[m[32m)[m
{[m
    try {[m
	[m
	[32m/* ----- initialization ----- */[m
[32m	const Configuration &config = Configuration::instance();[m
	[m
        // check the layers and weight sections[m
        if (!jsonDoc->HasMember("layers"))[m
            throw std::runtime_error("Missing section 'layers'");[m
        rapidjson::Value &layersSection  = (*jsonDoc)["layers"];[m

        if (!layersSection.IsArray())[m
            throw std::runtime_error("Section 'layers' is not an array");[m

        helpers::JsonValue weightsSection;[m
        if (jsonDoc->HasMember("weights")) {[m
            if (!(*jsonDoc)["weights"].IsObject())[m
                throw std::runtime_error("Section 'weights' is not an object");[m

            weightsSection = helpers::JsonValue(&(*jsonDoc)["weights"]);[m
        }[m
	
	
	[32m// Add 1220, support to the FeedBackLayer[m
[32m	std::vector<int> feedBacklayerId; // Idx of all the feedBackLayers[m
[32m	feedBacklayerId.clear();[m
[32m	[m
[32m	m_firstFeedBackLayer    = -1;     // Idx of the first feedback layer[m
[32m	m_middlePostOutputLayer = -1;     // Idx of the PostOutputLayer inside the network[m
[32m	m_featMatchLayer        = -1;     // Idx of the featMatching layer (for GAN)[m
[32m	m_vaeLayer              = -1;     // Idx of the VAE interface layer[m
[32m	int tmp_wavNetCore      = -1;     // Idx of the first waveNet core module (for waveNet)[m
[32m	bool flagSaveMemWavNet  = false;  // Flag to save the mem usage of wavenet in generation[m
[32m	int outputLayerIdx      = -1;     // Idx of the output layer (before postoutput)           [m
[32m	[m
[32m	m_trainingEpoch         = -1;     // initialize the training epoch counter[m
[32m	m_trainingFrac          = -1;     // initialize the training data counter[m
[32m	m_trainingState         = -1;     // initialize the training state of the network[m

	[32m/* ----- processing loop ----- */[m
	// [31mextract the layers[m[32mpreloop to determine type o fnetwork[m
[32m	int counter = 0;[m
        for (rapidjson::Value::ValueIterator layerChild = layersSection.Begin(); 
	     layerChild != layersSection.End();
	     [31m++layerChild) {[m
[31m            [m
[31m	    // check the layer child type[m[32m++layerChild, counter++){[m
	    
            if (!layerChild->IsObject())[m
                throw std::runtime_error("A layer[31msection[m in the 'layers' array is not an object");[31m// extract the layer type and create the layer[m
            if (!layerChild->HasMember("type"))[m
                throw std::runtime_error("Missing value 'type' in layer description");[m
            [32mstd::string layerType = (*layerChild)["type"].GetString();[m

[32m	    // logging information[m
[32m	    if (layerType == "feedback"){[m
[32m		// feedback layer[m
[32m		feedBacklayerId.push_back(counter);[m
[32m		if (m_firstFeedBackLayer < 0) m_firstFeedBackLayer = counter;[m
[32m	    }else if (layerType == "middleoutput"){[m
[32m		// for GAN[m
[32m		m_middlePostOutputLayer = counter;[m
[32m	    }else if (layerType == "featmatch"){[m
[32m		// for GAN[m
[32m		m_featMatchLayer = counter;[m
[32m	    }else if (layerType == "vae"){[m
[32m		// for vae[m
[32m		m_vaeLayer       = counter;[m
[32m	    }else if (layerType == "wavnetc" && tmp_wavNetCore < 0){[m
[32m		// for the wavenet component[m
[32m		tmp_wavNetCore   = counter;[m
[32m		if (!config.trainingMode() && m_firstFeedBackLayer > 0)[m
[32m		    flagSaveMemWavNet = true;[m
[32m	    }[m
[32m	}[m
[32m	outputLayerIdx = counter - 2; // an output before the postoutput layer[m

	[32m// loop to build each layer[m
[32m	counter = 0;[m
[32m        for (rapidjson::Value::ValueIterator layerChild = layersSection.Begin(); [m
[32m	     layerChild != layersSection.End();[m
[32m	     ++layerChild, counter++){[m
[32m	    [m
[32m            printf("\nLayer (%d)", counter);[m
[32m	    [m
[32m	    // get layer name and type[m
[32m            std::string layerName = (*layerChild)["name"].GetString();[m
[32m	    printf(" [ %s ] ", layerName.c_str());[m
            std::string layerType = (*layerChild)["type"].GetString();[m
	    [32mprintf(" %s ", layerType.c_str());[m

            // [31moverride input/output sizes[m[32mupdate the input layer size[m
[32m	    // (I don't know why the original CURRENNT does this, by Xin)[m
[32m	    // this part is transparent to the user[m
            if (inputSizeOverride > 0 && layerType == [31m"input") {[m[32m"input"){[m
[32m		// for word embedding, input layer size can be changed here[m
[32m		if (config.weUpdate() && config.trainingMode())[m
[32m		    inputSizeOverride += (config.weDim() - 1);[m
[32m		// overwrite the layer size[m
		(*layerChild)["size"].SetInt(inputSizeOverride);
            }[m

	    
	    /*  [32mOriginal code of CURRENNT, I don't know why this is necessary[m
		Does not work yet, need another way to identify a) postoutput layer (last!) and 
                then the corresponging output layer and type!
		if (outputSizeOverride > 0 && (*layerChild)["name"].GetString() == "output") {[m
		(*layerChild)["size"].SetInt(outputSizeOverride);[m
		}[m
[36m@@ -82,70 +546,157 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork(const helpers::JsonDocument &jsonDoc, int[m
		(*layerChild)["size"].SetInt(outputSizeOverride);[m
		}[m
	    */[m

	    [32m// create a layer[m
            try {[m
[31m            	layers::Layer<TDevice> *layer;[m
		[m
            	[32mlayers::Layer<TDevice> *layer;[m
		[m
		/* [31mAdd 02-24 Wang for Residual Network*/[m
[31m		/*[m[32mOriginal code of CURRENNT[m
                if (m_layers.empty())[m
		   layer = LayerFactory<TDevice>::createLayer(layerType, 
		           &*layerChild, weightsSection, parallelSequences, maxSeqLength);
                else[m
		   layer = LayerFactory<TDevice>::createLayer(layerType, 
		           &*layerChild, weightsSection, parallelSequences, maxSeqLength, 
                           m_layers.back().get()); */
		
		
                if [31m(m_layers.empty()){[m[32m(m_layers.empty())[m
[32m		{[m
		    // [32mcreate the[m first layer
		    [32m// check the layer type[m
		    if (layerType == "skipadd"           || layerType == [32m"skipini"       ||[m
[32m			layerType == "skipcat"           ||[m
[32m			layerType ==[m "skippara_logistic" || layerType == "skippara_relu" || 
			layerType == "skippara_tanh"     ||
			layerType == "skippara_identity")
		    [31mthrow std::runtime_error("SkipAdd and[m[32m{[m
[32m			printf("SkipAdd,[m SkipPara[31mlayer[m can not be the first [32mhidden[m layer");
			[32mthrow std::runtime_error("Error in network.jsn: layer type error\n");[m
[32m		    }[m
[32m		    // create the layer[m
		    layer = [31mLayerFactory<TDevice>::createLayer(layerType,[m[32mLayerFactory<TDevice>::createLayer([m
[32m				layerType,[m     &*layerChild, 
				weightsSection, parallelSequences, 
				maxSeqLength);[31m}else if(layerType == "skipadd" || layerType == "skippara_logistic" || layerType == "skippara_relu" || layerType == "skippara_tanh" || layerType == "skippara_identity"){[m

		[32m}[m
[32m		else if(layerType == "skipadd"           || layerType == "skipini"       ||[m
[32m			layerType == "skipcat"           ||[m
[32m			layerType == "skippara_logistic" || layerType == "skippara_relu" || [m
[32m			layerType == "skippara_tanh"     || [m
[32m			layerType == "skippara_identity")[m
[32m		{[m
		    
		    // SkipLayers: all the layers that link to the current skip layer[m
		    //  here, it includes the last skip layer and the previous normal 
		    [32m//[m  layer connected to this skip layer
		    std::vector<layers::Layer<TDevice>*> SkipLayers;[m
		    
		    // for skipadd layer:[m
		    //   no need to check whether the last skiplayer is directly 
		    [32m//[m   connected to current skiplayer
		    //   in that case, F(x) + x = 2*x, the gradients will be multiplied by 2[m
		    // for skippara layer:[m
		    //   need to check, because H(x)*T(x)+x(1-T(x)) = x if H(x)=x[m
		    //   check it in SkipParaLayer.cu[m


		    if [31m(m_skipAddLayers.size()>0) {SkipLayers.push_back(m_skipAddLayers.back());}[m[32m(m_skipAddLayers.size() == 0)[m
[32m		    {[m
[32m			if (layerType == "skipini" || layerType == "skipadd" ||[m
[32m			    layerType == "skipcat")[m
[32m		        {[m
[32m			    // do nothing[m
[32m			}[m
[32m			else[m
[32m			{[m
[32m			    // skippara requires previous skip layers[m
[32m			    throw std::runtime_error("Error: no skipini layer been found");[m
[32m			}[m
[32m		    }[m
[32m		    else[m
[32m		    {[m
[32m			if (layerType == "skipini")[m
[32m			{[m
[32m			    // do nothing[m
[32m			}[m
[32m			else if (layerType == "skipadd" || layerType == "skipcat")[m
[32m			{[m
[32m			    // skipadd and skipcat can take multiple skiplayers[m
[32m			    BOOST_FOREACH (layers::Layer<TDevice>* skiplayer, m_skipAddLayers)[m
[32m			    {[m
[32m				SkipLayers.push_back(skiplayer);[m
[32m			    }[m
[32m			}[m
[32m			else{[m
[32m			    // skippara (highway block) only takes one skiplayer as input source[m
[32m			    SkipLayers.push_back(m_skipAddLayers.back());[m
[32m			}[m
[32m		    }[m

[32m		    // a skiplayer can additionally take the previous layer as input source.[m
[32m		    // note, this layer may be a normal layer or a skip layer.[m 
		    SkipLayers.push_back(m_layers.back().get());[m
		    [32m// I should check whether the previous layer is still a skip layer[m
		    [m
		    if (layerType == [31m"skipadd")[m[32m"skipadd" || layerType == "skipini" || layerType == "skipcat")[m
[32m		    {[m
			layer = [31mLayerFactory<TDevice>::createSkipAddLayer(layerType,[m[32mLayerFactory<TDevice>::createSkipAddLayer([m
[32m				  layerType,[m     &*layerChild,
				  weightsSection, parallelSequences, 
				  maxSeqLength,   SkipLayers);
		    [32m}[m
		    else[m
		    [32m{[m
			layer = [31mLayerFactory<TDevice>::createSkipParaLayer(layerType,[m[32mLayerFactory<TDevice>::createSkipParaLayer([m
[32m				  layerType,[m     &*layerChild,
				  weightsSection, parallelSequences, 
				  maxSeqLength,   SkipLayers);
		    [32m}[m
		    // add the skipadd layer to [31mNetwork[m[32mthe[m buffer [32mof the network[m
		    m_skipAddLayers.push_back(layer);[m
		[m
[31m		}else{[m
[31m		    // normal layers[m
[31m                    layer = LayerFactory<TDevice>::createLayer(layerType, &*layerChild, weightsSection, [m
[31m							       parallelSequences, maxSeqLength, m_layers.back().get());		[m
		}[m
		[32melse[m
[32m		{[m
[32m		    // other layers types[m
[32m                    layer = LayerFactory<TDevice>::createLayer([m
[32m			       layerType,      &*layerChild,[m
[32m			       weightsSection, parallelSequences, [m
[32m			       maxSeqLength, [m
[32m			       m_layers.back().get());[m
[32m		}[m
		

		[32m// post processing for the layer[m
[32m		{[m
[32m		    // for wavenet, reduce the memory in generation[m
[32m		    if (flagSaveMemWavNet){[m
[32m			// only save the memory for layers between the feedback and output layer[m
[32m			if (counter < outputLayerIdx && counter > m_firstFeedBackLayer){[m
[32m			    layer->reduceOutputBuffer();[m
[32m			}[m
[32m		    }[m
[32m		}[m

[32m		// save the layer[m
                m_layers.push_back(boost::shared_ptr<layers::Layer<TDevice> >(layer));[m
	       		
            }[m
            catch (const std::exception &e) {[m
                throw std::runtime_error(std::string("Could not create layer: ") + e.what());[m
            }[m
        } // [32mProcessing loop done[m
[32m	[m
[32m	[m
[32m	/* ----- post-processing[m check [31mif we have at least one input, one output and one post output layer[m[32m----- */[m
        if (m_layers.size() < 3)[m
            throw [31mstd::runtime_error("Not enough layers defined");[m[32mstd::runtime_error("Error in network.jsn: there must be a hidden layer\n");[m
        // check if only the first layer is an input layer[m
        if (!dynamic_cast<layers::InputLayer<TDevice>*>(m_layers.front().get()))[m
            throw std::runtime_error("The first layer is not an input layer");[m

        for (size_t i = 1; i < m_layers.size(); ++i) {[m
            if (dynamic_cast<layers::InputLayer<TDevice>*>(m_layers[i].get()))[m
                throw std::runtime_error("Multiple input layers defined");[m
[36m@@ -155,18 +706,90 @@[m [mNeuralNetwork<TDevice>::NeuralNetwork(const helpers::JsonDocument &jsonDoc, int[m
        if (!dynamic_cast<layers::PostOutputLayer<TDevice>*>(m_layers.back().get()))[m
            throw std::runtime_error("The last layer is not a post output layer");[m

	[32m// check the post output layer[m
[32m	{[m
[32m	    layers::PostOutputLayer<TDevice>* lastPOLayer;[m
[32m	    lastPOLayer = dynamic_cast<layers::PostOutputLayer<TDevice>*>(m_layers.back().get());[m
[32m	    [m
[32m	    layers::PostOutputLayer<TDevice>* midPOLayer;[m
	    for (size_t i = 0; i < m_layers.size()-1; ++i) {
		[32mmidPOLayer = dynamic_cast<layers::PostOutputLayer<TDevice>*>(m_layers[i].get());[m

[32m	[m
[32m		if (midPOLayer && midPOLayer->type() == "middleoutput"){[m
[32m		    // tell the last postoutput layer about the existence of middleoutput[m
[32m		    lastPOLayer->linkMiddleOutptu(midPOLayer);[m
[32m		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_MIDDLEOUTPUT);[m
[32m		    if (internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
[32m			throw std::runtime_error("No skipini/add/cat layer after middleoutput");[m
[32m		    [m
[32m		}else if (midPOLayer && midPOLayer->type() == "featmatch" && flagNetworkForGAN()){[m
[32m		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_FEATMATCH);[m
[32m		    if (internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
[32m			throw std::runtime_error("No skipini/add/cat layer after featmatch");[m
[32m		    [m
[32m		}else if (midPOLayer && midPOLayer->type() == "mdn" && flagNetworkForGAN()){[m
[32m		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_NOTLASTMDN);[m
[32m		    if (internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
[32m			throw std::runtime_error("No skipini/add/cat layer after MDN(GAN)");[m
[32m		    [m
[32m		}else if (midPOLayer && midPOLayer->type() == "vae"){[m
[32m		    midPOLayer->setPostLayerType(NN_POSTOUTPUTLAYER_VAEKL);[m
[32m		    if (flagNetworkForGAN())[m
[32m			throw std::runtime_error("GAN + VAE is not implemented");[m
[32m		    [m
[32m		    if (internal::invalidMiddleMDN(m_layers[i+1]->type()))[m
[32m			throw std::runtime_error("No skipini/add/cat layer after VAE");[m
[32m		    [m
[32m		}else[m if [31m(dynamic_cast<layers::PostOutputLayer<TDevice>*>(m_layers[i].get()))[m[32m(midPOLayer){[m
		    throw std::runtime_error("Multiple post output layers defined");[31m}[m

		[32m}else{[m
[32m		    // no other postoutput layer[m
[32m		}		    [m
[32m	    }[m
[32m	}[m
	
        // check if two layers have the same name[m
        for (size_t i = 0; i < m_layers.size(); ++i) {[m
            for (size_t j = 0; j < m_layers.size(); ++j) {[m
                if (i != j && m_layers[i]->name() == m_layers[j]->name())[m
                    throw [31mstd::runtime_error(std::string("Different[m[32mstd::runtime_error([m
[32m			std::string("Error in network.jsn: different[m layers have the[31msame[m name '") + 
			m_layers[i]->name() + "'");
            }[m
        }[m
	
	[32m// Link the target layer with the feedback layer[m
[32m	if (!feedBacklayerId.empty()){[m
[32m	    for (size_t i = 0; i<feedBacklayerId.size(); i++){[m
[32m		m_layers[feedBacklayerId[i]]->linkTargetLayer(*(m_layers.back().get()));[m
[32m	    }[m
[32m	    // check the bi-directional RNN after the feedback layer[m
[32m	    for (size_t i = m_firstFeedBackLayer; i < m_layers.size()-1; i++){[m
[32m		if (m_layers[i]->type()==std::string("brnn") ||[m
[32m		    m_layers[i]->type()==std::string("blstm")){[m
[32m		    throw std::runtime_error([m
[32m			 std::string("Error in network.jsn.") +[m
[32m			 std::string("brnn and blstm can't be used with feedback."));[m
[32m		}else if (m_layers[i]->type()==std::string("featmatch") ||[m
[32m			  m_layers[i]->type()==std::string("middleoutput")){[m
[32m		    throw std::runtime_error([m
[32m			 std::string("Error in network.jsn.") +[m
[32m			 std::string("Feedback was not implemented for GAN"));[m
[32m		}[m
[32m	    }[m
[32m	}[m

[32m	// Link the wavnet copmonents [m
[32m	if (tmp_wavNetCore > 0){[m
[32m	    for (size_t i = 0; i < m_layers.size(); ++i) {[m
[32m		if (m_layers[i]->type() == std::string("wavnetc")){[m
[32m		    m_layers[i]->linkTargetLayer(*(m_layers[tmp_wavNetCore].get()));[m
[32m		}[m
[32m	    }[m
[32m	}[m
	
    }[m
    catch (const std::exception &e) {[m
        throw std::runtime_error(std::string("Invalid network file: ") + e.what());[m
[36m@@ -178,6 +801,17 @@[m [mNeuralNetwork<TDevice>::~NeuralNetwork()[m
{[m
}[m


[32mtemplate <typename TDevice>[m
[32mbool NeuralNetwork<TDevice>::flagNetworkForGAN() const{[m
[32m    // Whether this network is for GAN[m
[32m    // output: [m
[32m    //    true: this network is for training GAN[m
[32m    //   false: a normal NN[m
[32m    return (m_middlePostOutputLayer > 0);[m
[32m}[m


template <typename TDevice>[m
const std::vector<boost::shared_ptr<layers::Layer<TDevice> > >& NeuralNetwork<TDevice>::layers() const[m
{[m
[36m@@ -190,10 +824,50 @@[m [mlayers::InputLayer<TDevice>& NeuralNetwork<TDevice>::inputLayer()[m
    return static_cast<layers::InputLayer<TDevice>&>(*m_layers.front());[m
}[m

[32m/* Modify 04-08 to tap in the output of arbitary layer */[m
[32m/*template <typename TDevice>[m
[32m  layers::TrainableLayer<TDevice>& NeuralNetwork<TDevice>::outputLayer()[m
[32m  {[m
[32m    return static_cast<layers::TrainableLayer<TDevice>&>(*m_layers[m_layers.size()-2]);[m
[32m  }[m
[32m*/[m

template <typename TDevice>[m
[31mlayers::TrainableLayer<TDevice>& NeuralNetwork<TDevice>::outputLayer()[m[32mlayers::Layer<TDevice>& NeuralNetwork<TDevice>::outputLayer(const int layerID)[m
{[m
    [32m// default case, the output layer[m
[32m    int tmpLayerID = layerID;[m
[32m    if (tmpLayerID < 0)[m
[32m	tmpLayerID = m_layers.size()-2;[m
[32m    [m
[32m    // check[m
[32m    if (tmpLayerID > (m_layers.size()-1))[m
[32m	throw std::runtime_error(std::string("Invalid output_tap ID (out of range)"));[m
    
    return [31mstatic_cast<layers::TrainableLayer<TDevice>&>(*m_layers[m_layers.size()-2]);[m[32m(*m_layers[tmpLayerID]);[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mlayers::SkipLayer<TDevice>* NeuralNetwork<TDevice>::outGateLayer(const int layerID)[m
[32m{[m
[32m    // default case, the output[m
[32m    int tmpLayerID = layerID;[m
[32m    [m
[32m    // check[m
[32m    if (tmpLayerID > (m_layers.size()-2) || tmpLayerID < 0)[m
[32m	throw std::runtime_error(std::string("Invalid gate_output_tap ID (out of range)"));[m
[32m    [m
[32m    return dynamic_cast<layers::SkipLayer<TDevice>*>(m_layers[tmpLayerID].get());[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mlayers::MDNLayer<TDevice>* NeuralNetwork<TDevice>::outMDNLayer(const int layerID)[m
[32m{[m
[32m    if (layerID < 0){[m
[32m	return dynamic_cast<layers::MDNLayer<TDevice>*>(m_layers[m_layers.size()-1].get());[m
[32m    }else{[m
[32m	return dynamic_cast<layers::MDNLayer<TDevice>*>(m_layers[layerID].get());[m
[32m    }[m
}[m

template <typename TDevice>[m
[36m@@ -206,31 +880,541 @@[m [mtemplate <typename TDevice>[m
void NeuralNetwork<TDevice>::loadSequences(const data_sets::DataSetFraction &fraction)[m
{[m
    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
    [31mlayer->loadSequences(fraction);[m[32m{[m
[32m        layer->loadSequences(fraction, m_trainingState);[m
[32m    }[m
}[m

template <typename TDevice>[m
void [31mNeuralNetwork<TDevice>::computeForwardPass()[m[32mNeuralNetwork<TDevice>::restoreTarget(const data_sets::DataSetFraction &fraction)[m
{[m
    [32mconst Configuration &config = Configuration::instance();[m

[32m    if (config.scheduleSampOpt() == NN_FEEDBACK_SC_SOFT ||[m
[32m	config.scheduleSampOpt() == NN_FEEDBACK_SC_MAXONEHOT ||[m
[32m	config.scheduleSampOpt() == NN_FEEDBACK_SC_RADONEHOT){[m
[32m        m_layers[m_layers.size()-1]->loadSequences(fraction, m_trainingState);[m
[32m    }[m
[32m}[m


[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::computeForwardPass(const int curMaxSeqLength,[m
[32m						const real_t uttCnt)[m
[32m{[m
[32m    // |[m
[32m    // |- No feedback, normal forward and recurrent computation[m
[32m    // |- Feedback layer exists[m
[32m    //    |- Case 0: use only ground truth as feedback data[m
[32m    //    |- Case 1: use schedule uniform initialization ( 1/N )[m
[32m    //    |- Case 2: use schedule back-off (set to zero)[m
[32m    //    |- Case 3: use schedule sampling, soft-vector feedback[m
[32m    //    |- Case 4: use schedule sampling, one-hot feedback[m
[32m    //[m
[32m    [m
[32m    const Configuration &config = Configuration::instance();[m

[32m    // No feedback, normal forward computation[m
[32m    if (m_firstFeedBackLayer <= 0){[m
	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)
	    [31mlayer->computeForwardPass();[m[32mlayer->computeForwardPass(m_trainingState);[m

[32m	// For GAN with featMatch, do additional propagation[m
[32m	if (m_trainingState == NN_STATE_GAN_GEN_FEATMAT && flagNetworkForGAN() && [m
[32m	    m_featMatchLayer > 0){[m
[32m	    m_trainingState = NN_STATE_GAN_GEN; // return to the normal state[m
[32m	    for (int i = m_middlePostOutputLayer; i < m_layers.size(); i++)[m
[32m		m_layers[i]->computeForwardPass(m_trainingState);[m
[32m	}[m

[32m    // Other cases, Feedback exists[m
[32m    }else {[m
[32m	[m
[32m	// prepare random numbers[m
[32m    	static boost::mt19937 *gen = NULL;[m
[32m	if (!gen) {[m
[32m	    gen = new boost::mt19937;[m
[32m	    gen->seed(config.randomSeed()+98); // any random number[m
[32m	}[m
[32m	boost::random::uniform_real_distribution<real_t> dist(0, 1);[m

[32m	// options for schedule sampling[m
[32m	int scheduleSampOpt = config.scheduleSampOpt();[m
[32m	int scheduleSampPara= config.scheduleSampPara();[m
[32m	[m
[32m	// Prepare the ground truth [m
[32m	/*layers::MDNLayer<TDevice> *olm;[m
[32m	olm = outMDNLayer();[m
[32m	if (olm != NULL){[m
[32m	    olm->retrieveFeedBackData();[m
[32m	}else if (scheduleSampOpt > 0){[m
[32m	    printf("\n\n Schedule sampling is not implemented for non-MDN network\n\n");[m
[32m	    throw std::runtime_error(std::string("To be implemented"));[m
[32m	}else{[m
[32m	    [m
[32m	}*/[m
[32m	[m
[32m	// Ask the postoutputLayer to retrieve the training data for feedback[m
[32m	// postOutputLayer loads the target training data during loadSequences()[m
[32m	this->postOutputLayer().retrieveFeedBackData();[m

[32m	// Depends on the training method, start the training[m
[32m	int methodCode;[m
[32m	switch (scheduleSampOpt){[m

[32m	 // Case0: use ground truth directly, without dropout or sampling[m
[32m	case NN_FEEDBACK_GROUND_TRUTH:[m
[32m	    {[m
[32m		BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
[32m		    layer->computeForwardPass(m_trainingState);[m
[32m		break;[m
[32m	    }[m

[32m	// Case 1 & 2: schedule dropout, set data to 1/N (case 1) or 0 (case 2)[m
[32m	case NN_FEEDBACK_DROPOUT_1N:[m
[32m	case NN_FEEDBACK_DROPOUT_ZERO:[m
[32m	    {[m
[32m		real_t threshold = ((real_t)scheduleSampPara)/100;[m

[32m		// Prepare the random vector[m
[32m		Cpu::real_vector randNum;[m
[32m		randNum.reserve(curMaxSeqLength);[m
[32m		for (size_t i = 0; i < curMaxSeqLength; ++i){[m
[32m		    if (dist(*gen) > threshold){[m
[32m			randNum.push_back(0);[m
[32m		    }else{[m
[32m			randNum.push_back(1);[m
[32m		    }[m
[32m		}[m

[32m		//[m
[32m		if (m_vaeLayer > 0){[m
[32m		    // If vae exists, don't dropout until after VAE layers[m
[32m		    // ComputeForward[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m			layer->computeForwardPass(m_trainingState);[m
[32m			if (layer->type() == "vae") break;[m
[32m		    }[m
[32m		    // Drop out the feedback data randomly[m
[32m		    typename TDevice::real_vector temp = randNum;[m
[32m		    this->postOutputLayer().retrieveFeedBackData(temp, scheduleSampOpt);[m
[32m		    // computeforward after VAE layers[m
[32m		    int cnt = 0;[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m			if (cnt > m_vaeLayer)[m
[32m			    layer->computeForwardPass(m_trainingState);[m
[32m			cnt++;[m
[32m		    }[m
[32m		    [m
[32m		}else{[m
[32m		    // Normal cases, dropout before feedback[m
[32m		    // Drop out the feedback data randomly[m
[32m		    typename TDevice::real_vector temp = randNum;[m
[32m		    this->postOutputLayer().retrieveFeedBackData(temp, scheduleSampOpt);[m

[32m		    // ComputeForward[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
[32m			layer->computeForwardPass(m_trainingState);[m
[32m		}[m
[32m		break;[m
[32m	    }[m

[32m	// Case 3,  4, 5: schedule sampling[m
[32m	// use soft vector as feedback (case 3) or one-hot (case 4), or random output as feedback[m
[32m	case NN_FEEDBACK_SC_SOFT:[m
[32m	case NN_FEEDBACK_SC_MAXONEHOT:[m
[32m	case NN_FEEDBACK_SC_RADONEHOT:[m
[32m	    {[m
[32m		[m
[32m		real_t sampThreshold;[m
[32m		methodCode = scheduleSampOpt;[m
[32m		[m
[32m		// Forward computation for layers before the Feedback layer[m
[32m		int cnt = 0;[m
[32m		BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
[32m		{[m
[32m		    if (cnt == m_firstFeedBackLayer) break; [m
[32m		    layer->computeForwardPass(m_trainingState);[m
[32m		    cnt++;[m
[32m		}[m
[32m		[m
[32m		// Determine the threshold [m
[32m		if (scheduleSampPara > 0){[m
[32m		    // randomly use the generated sample[m
[32m		    sampThreshold =[m
[32m			(1.0 / (1.0 + exp((uttCnt - NN_FEEDBACK_SCHEDULE_SIG) * 1.0 /[m
[32m					  scheduleSampPara)));[m
[32m		    // sampThreshold = 1.0 - ((real_t)uttCnt/scheduleSampPara);[m
[32m		    //sampThreshold = pow(scheduleSampPara/100.0, uttCnt);[m
[32m		    sampThreshold = ((sampThreshold  < NN_FEEDBACK_SCHEDULE_MIN) ?[m
[32m				     NN_FEEDBACK_SCHEDULE_MIN : sampThreshold);[m
[32m		}else{[m
[32m		    sampThreshold = (-1.0 * (real_t)scheduleSampPara / 100.0);[m
[32m		}[m

[32m		// printf("%f %f\n", uttCnt, sampThreshold);[m
[32m		// Forward computation for layer above feedback using schedule sampling[m
[32m		for (int timeStep = 0; timeStep < curMaxSeqLength; timeStep++){[m

[32m		    cnt = 0;[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
[32m		    {[m
[32m			if (cnt >= m_firstFeedBackLayer){[m
[32m			    // For Rnn and LSTM, prepareStepGeneration is necessary[m
[32m			    // to prepare the data matrix per frame[m
[32m			    layer->prepareStepGeneration(timeStep); [m
[32m			    layer->computeForwardPass(timeStep, m_trainingState);    [m
[32m			}[m
[32m			cnt++;[m
[32m		    }[m
[32m		    [m
[32m		    // [m
[32m		    if (dist(*gen) > sampThreshold){[m
[32m			//printf("\n %d HIT", timeStep);[m
[32m			layers::MDNLayer<TDevice> *olm;[m
[32m			olm = outMDNLayer();[m
[32m			if (olm != NULL){[m
[32m			    olm->getOutput(timeStep, 0.0001); [m
[32m			    olm->retrieveFeedBackData(timeStep, methodCode);[m
[32m			    /******** Fatal Error *******/[m
[32m			    // After getOutput, the targets will be overwritten by generated data.[m
[32m			    // But the target will be used by calculateError and computeBackWard.[m
[32m			    // Thus, targets of the natural data should be re-written[m
[32m			    // This is now implemented as this->restoreTarget(frac)	    [m
[32m			}    [m
[32m		    }else{[m
[32m			//printf("\n %d MISS", timeStep);[m
[32m		    }[m
[32m		}[m
[32m		break;[m
[32m	    }[m
[32m	}[m
[32m    }[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::computeForwardPassGen(const int curMaxSeqLength, [m
[32m						   const real_t generationOpt)[m
[32m{[m
[32m    layers::MDNLayer<TDevice> *olm;[m
[32m    const Configuration &config = Configuration::instance();[m
[32m    [m
[32m    // no feedback layer, normal computation[m
[32m    if (m_firstFeedBackLayer < 0){[m

[32m	this->computeForwardPass(curMaxSeqLength, -1);[m
[32m	// if MDN is available, infer the output, or copy the MDN parameter vector[m
[32m	olm = outMDNLayer();[m
[32m	if (olm != NULL) olm->getOutput(generationOpt);[m

[32m    // feedback layer exists[m
[32m    }else{[m

[32m	// Prepare the random seed[m
[32m	static boost::mt19937 *gen = NULL;[m
[32m	if (!gen) {[m
[32m	    gen = new boost::mt19937;[m
[32m	    gen->seed(config.randomSeed()+98); // any random number[m
[32m	}[m
[32m	boost::random::uniform_real_distribution<real_t> dist(0, 1);[m

[32m	[m
[32m	int scheduleSampOpt = config.scheduleSampOpt();[m
[32m	int scheduleSampPara= config.scheduleSampPara();[m
[32m	printf("SSAMPOpt: %d, SSAMPPara: %d\n", scheduleSampOpt, scheduleSampPara);[m
[32m	[m
[32m	int    methodCode    = 0;[m
[32m	real_t sampThreshold = 0.0;[m
[32m	int    cnt           = 0;[m
[32m	//int    beamSize      = 0;[m
[32m	[m
[32m	// Forward computation for layers before Feedback[m
[32m	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	    if (cnt == m_firstFeedBackLayer) break; [m
[32m	    layer->computeForwardPass(m_trainingState);[m
[32m	    cnt++;[m
[32m	}[m

[32m	// Parameter for genreation[m
[32m	switch (scheduleSampOpt){[m

[32m	// Case 0: use probability vector for feedback[m
[32m	//         1. native training approach[m
[32m	//         2. schedule sampling (soft-feedback training)[m
[32m	case NN_FEEDBACK_GROUND_TRUTH:[m
[32m	case NN_FEEDBACK_SC_SOFT:[m
[32m	    // always uses the soft vector (default option)[m
[32m	    sampThreshold  = 1;[m
[32m	    methodCode     = NN_FEEDBACK_GROUND_TRUTH;[m
[32m	    break;[m

[32m	// Case 1: use one hot vector[m
[32m	case NN_FEEDBACK_SC_MAXONEHOT:[m
[32m	    if (scheduleSampPara > 0){[m
[32m		sampThreshold = 1;[m
[32m		methodCode = NN_FEEDBACK_GROUND_TRUTH;[m
[32m	    }else{[m
[32m		sampThreshold = (-1.0 * (real_t)scheduleSampPara / 100.0);[m
[32m		methodCode = NN_FEEDBACK_SC_MAXONEHOT;[m
[32m	    }[m
[32m	    // use the one-hot best[m
[32m	    break;[m
[32m	    [m
[32m	// Case 2: dropout[m
[32m	case NN_FEEDBACK_DROPOUT_1N:[m
[32m	    methodCode = NN_FEEDBACK_DROPOUT_1N;[m
[32m	    sampThreshold = ((real_t)scheduleSampPara)/100;[m
[32m	    break;					    [m
[32m	case NN_FEEDBACK_DROPOUT_ZERO:[m
[32m	    methodCode = NN_FEEDBACK_DROPOUT_ZERO;[m
[32m	    sampThreshold = ((real_t)scheduleSampPara)/100;[m
[32m	    break;[m
[32m	    [m
[32m	// Case 3: beam search[m
[32m	case NN_FEEDBACK_BEAMSEARCH:[m
[32m	    methodCode = NN_FEEDBACK_SC_MAXONEHOT;[m
[32m	    //beamSize   = (int)scheduleSampPara;[m
[32m	    break;[m
[32m	}[m


[32m	//Generation[m
[32m	// Normal generation (Greedy)[m
[32m	if (scheduleSampOpt != NN_FEEDBACK_BEAMSEARCH){[m
[32m	    for (int timeStep = 0, cnt = 0; timeStep < curMaxSeqLength; timeStep ++, cnt = 0){[m
[32m		BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m		    if (cnt >= m_firstFeedBackLayer){[m
[32m			// prepare the matrix (for rnn, lstm)[m
[32m			layer->prepareStepGeneration(timeStep);[m
[32m			// compute for 1 frame[m
[32m			layer->computeForwardPass(timeStep, m_trainingState);    [m
[32m		    }[m
[32m		    cnt++;[m
[32m		}[m
[32m		[m
[32m		// Generate the output from MDN[m
[32m		olm = outMDNLayer();[m
[32m		if (olm != NULL) olm->getOutput(timeStep, generationOpt);[m
[32m		[m
[32m		// Feedback the data[m
[32m		if (dist(*gen) < sampThreshold){[m
[32m		    this->postOutputLayer().retrieveFeedBackData(timeStep, 0);[m
[32m		}else{[m
[32m		    this->postOutputLayer().retrieveFeedBackData(timeStep, methodCode);[m
[32m		    printf("%d ", timeStep);[m
[32m		}[m
[32m	    }[m
[32m	    [m
[32m	// Beam search generation[m
[32m	}else{[m
[32m	    [m
[32m	    int stateNum;       // number of states per time step[m
[32m	    int layerCnt;       // counter of the hidden layers[m
[32m	    int beamSize   = (int)scheduleSampPara; // size of beam[m
[32m	    [m
[32m	    /* ----- pre-execution check  ----- */[m
[32m	    if (beamSize < 0)[m
[32m		throw std::runtime_error("beam size cannot be < 1");[m
[32m	    if (m_firstFeedBackLayer < 0)[m
[32m		throw std::runtime_error("No need to use beam size for network without feedback");[m
[32m	    olm = outMDNLayer();[m
[32m	    if (olm == NULL)[m
[32m		throw std::runtime_error("Beam size is used for non-MDN output layer");[m
[32m	    stateNum = olm->mdnParaDim();[m
[32m	    if (beamSize >= stateNum)[m
[32m		throw std::runtime_error("Beam size is larger than the number of state");[m

[32m	    /* ----- initialization ----- */[m
[32m	    // count the number of hidden elements in the network[m
[32m	    std::vector<int> netStateSize;[m
[32m	    int hidEleNum = 0;	// number of hidden elements in the network[m
[32m	    [m
[32m	    layerCnt  = 0;[m
[32m	    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m		if (layerCnt >= m_firstFeedBackLayer){[m
[32m		    netStateSize.push_back(layer->hiddenStateSize());[m
[32m		    hidEleNum += layer->hiddenStateSize();[m
[32m		}[m
[32m		layerCnt++;[m
[32m	    }[m
[32m	    // allocate memory spaces for searching [m
[32m	    beamsearch::searchState<TDevice>  bmState(netStateSize, curMaxSeqLength, stateNum);[m
[32m	    beamsearch::searchEngine<TDevice> bmEngine(beamSize);[m
[32m	    for (int i = 0; i < beamSize + beamSize * stateNum; i++)[m
[32m		bmEngine.addState(bmState);[m
[32m	    bmEngine.setValidBeamSize(1);[m
[32m	    std::vector<beamsearch::sortUnit> preSortVec(stateNum);[m
[32m	    // allocate memory spaces for hidden features of network[m
[32m	    Cpu::real_vector netStateTmpTmp(hidEleNum, 0.0);[m
[32m	    typename TDevice::real_vector netStateTmp = netStateTmpTmp;[m


[32m	    [m
[32m	    /* ----- Search loop ----- */[m
[32m	    for (int timeStep = 0; timeStep < curMaxSeqLength; timeStep++){[m

[32m		// Count the extended number of states[m
[32m		int stateCnt = 0;[m
[32m		[m
[32m		// loop over beam[m
[32m		for (int searchPT = 0; searchPT < bmEngine.getValidBeamSize(); searchPT ++){[m

[32m		    // get the state to be extended[m
[32m		    beamsearch::searchState<TDevice>& bmState2 = bmEngine.retrieveState(searchPT);[m
[32m		    [m
[32m		    // prepare states from bmState2[m
[32m		    bmState.liteCopy(bmState2);[m
[32m		    [m
[32m		    // set the network state[m
[32m		    // 1. set the feedback data[m
[32m		    if (timeStep > 0)[m
[32m			this->postOutputLayer().setFeedBackData(timeStep-1, bmState2.getStateID());[m
[32m		    [m
[32m		    // 2. set the hidde layers and compute[m
[32m		    layerCnt = 0;[m
[32m		    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m			if (layerCnt >= m_firstFeedBackLayer){[m
[32m			    int layerID = layerCnt - m_firstFeedBackLayer;[m
[32m			    layer->prepareStepGeneration(timeStep);[m
[32m			    if (timeStep > 0)[m
[32m				layer->setHiddenState(timeStep-1, bmState2.getNetState(layerID));[m
[32m			    layer->computeForwardPass(timeStep, m_trainingState);[m

[32m			    // store the state of network in new states [m
[32m			    // this should be in step3. but this is more efficient[m
[32m			    layer->retrieveHiddenState(timeStep, netStateTmp);[m
[32m			    bmState.setNetState(layerID, netStateTmp);[m
[32m			}[m
[32m			layerCnt++;[m
[32m		    }[m
[32m		    // 3. pre-select the states to be explored[m
[32m		    for (int newStateID = 0; newStateID < stateNum; newStateID++){[m
[32m			preSortVec[newStateID].prob = olm->retrieveProb(timeStep, newStateID);[m
[32m			preSortVec[newStateID].idx  = newStateID;[m
[32m		    }[m
[32m		    std::sort(preSortVec.begin(), preSortVec.end(), beamsearch::compareFunc);[m
[32m		    [m
[32m		    // 4. add new search state[m
[32m		    //  probability before this step[m
[32m		    for (int i = 0; i < bmEngine.getBeamSize(); i++){[m
[32m			[m
[32m			bmState.setStateID(preSortVec[i].idx);[m
[32m			bmState.setStateTrace(timeStep, preSortVec[i].idx);[m
[32m			bmState.setTimeStep(timeStep);[m
[32m			if (preSortVec[i].prob < 1e-15f)[m
[32m			    continue; // trim the zero probability path[m
[32m			else[m
[32m			    bmState.setProb(bmState2.getProb() + std::log(preSortVec[i].prob));[m
[32m			bmState.setProbTrace(timeStep, preSortVec[i].prob);[m
[32m			bmEngine.setState(bmEngine.getBeamSize() + stateCnt, bmState);[m
[32m			bmEngine.setSortUnit(stateCnt, bmState);[m
[32m			stateCnt++;[m
[32m		    }[m
[32m		}	[m
[32m		bmEngine.sortSet(stateCnt);[m
[32m		bmEngine.printBeam();[m
[32m	    }[m
[32m	    [m
[32m	    // Finish the beam search, finally, generate[m
[32m	    bmEngine.printBeam();[m
[32m	    bmState.fullCopy(bmEngine.retrieveState(0));[m
[32m	    for (int timeStep = 0; timeStep < curMaxSeqLength; timeStep++){[m
[32m		if (timeStep > 0)[m
[32m		    this->postOutputLayer().setFeedBackData(timeStep-1,[m
[32m							    bmState.getStateID(timeStep-1));[m
[32m		[m
[32m		layerCnt = 0;[m
[32m		BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m		    if (layerCnt >= m_firstFeedBackLayer){[m
[32m			layer->prepareStepGeneration(timeStep);[m
[32m			layer->computeForwardPass(timeStep, m_trainingState);[m
[32m		    }[m
[32m		    layerCnt++;[m
[32m		}[m
[32m		[m
[32m		olm = outMDNLayer();[m
[32m		if (olm != NULL) olm->getOutput(timeStep, generationOpt);[m
[32m	    }[m
[32m	    [m
[32m	} // Beam search generation[m
[32m	[m
[32m    } // Generation for network with feedback layers (structured prediction)[m
}[m

template <typename TDevice>[m
void NeuralNetwork<TDevice>::computeBackwardPass()[m
{[m
    BOOST_REVERSE_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers) {[m

	[31mlayer->computeBackwardPass();[m[32m// runningMode[m
[32m	if (Configuration::instance().runningMode() > 0){[m

[32m	    // Stop the backpropagation when the layer's learning rate is specified as 0[m
[32m	    layers::TrainableLayer<TDevice> *trainableLayer = [m
[32m		dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());[m
[32m	    if (trainableLayer && closeToZero(trainableLayer->learningRate()))[m
[32m		break;[m

[32m	    // Or, stop if it is a mdn output layer in acoustic model[m
[32m	    // Note, this is specific for GAN[m
[32m	    /* To be revised for non-gan network */[m
[32m	    layers::PostOutputLayer<TDevice> *mdnlayer = [m
[32m		dynamic_cast<layers::PostOutputLayer<TDevice>*>(layer.get());[m
[32m	    if (mdnlayer && mdnlayer->postLayerType() == NN_POSTOUTPUTLAYER_NOTLASTMDN)[m
[32m		break;[m
[32m	    [m
[32m	}[m
[32m        layer->computeBackwardPass(m_trainingState);[m
[32m	[m
[32m	// For debugging[m
	//std::cout << "output errors " << layer->name() << std::endl;
	//thrust::copy(layer->outputErrors().begin(), layer->outputErrors().end(), 
	[32m//[m std::ostream_iterator<real_t>(std::cout, ";"));
	//std::cout << std::endl;
    }[m
}[m

template <typename TDevice>[m
[31mreal_t NeuralNetwork<TDevice>::calculateError() const[m[32mvoid NeuralNetwork<TDevice>::cleanGradientsForDiscriminator()[m
{[m
    [32m// For GAN[m
[32m    if (flagNetworkForGAN() && m_trainingState == NN_STATE_GAN_GEN){[m
[32m	// clean the discrminator gradients when only generator is trained[m
[32m	int cnt = 0;[m
[32m	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers) {[m
[32m	    if (cnt > m_middlePostOutputLayer)[m
[32m		layer->cleanGradidents();[m
[32m	    cnt++;[m
[32m	}[m
[32m    }[m

[32m    // For general usage[m
[32m    [m
[32m    [m
[32m}[m


[32mtemplate <typename TDevice>[m
[32mreal_t NeuralNetwork<TDevice>::calculateError(const bool flagGenerateMainError) const[m
[32m{[m
[32m    if (flagNetworkForGAN()){[m
[32m	if (flagGenerateMainError)[m
[32m	    // there is middle post output[m
[32m	    return static_cast<layers::PostOutputLayer<TDevice>&>(*m_layers[m_middlePostOutputLayer]).calculateError();[m
[32m	else[m
[32m	    return static_cast<layers::PostOutputLayer<TDevice>&>(*m_layers.back()).calculateError();[m
[32m	[m
[32m    }else if (m_vaeLayer > 0){[m
[32m	if (flagGenerateMainError)[m
	    return static_cast<layers::PostOutputLayer<TDevice>&>(*m_layers.back()).calculateError();
	[32melse[m
[32m	    return static_cast<layers::PostOutputLayer<TDevice>&>(*m_layers[m_vaeLayer]).calculateError();[m
[32m    }else{[m
[32m	if(flagGenerateMainError)[m
[32m	    return static_cast<layers::PostOutputLayer<TDevice>&>(*m_layers.back()).calculateError();[m
[32m	else[m
[32m	    return 0;[m
[32m    }[m
}[m

template <typename TDevice>[m
[36m@@ -265,9 +1449,17 @@[m [mvoid NeuralNetwork<TDevice>::exportWeights(const helpers::JsonDocument& jsonDoc)[m

    // create the weight objects[m
    BOOST_FOREACH (const boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers) {[m
    	layers::TrainableLayer<TDevice> *trainableLayer = 
	    dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());
        if [31m(trainableLayer)[m[32m(trainableLayer){[m
            trainableLayer->exportWeights(&weightsObject, &jsonDoc->GetAllocator());[m
	[32m}else{[m
[32m	    // Modify 0507 Wang: for mdn PostProcess Layer[m
[32m	    layers::MDNLayer<TDevice> *mdnlayer = [m
[32m		dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
[32m	    if (mdnlayer)[m
[32m		mdnlayer->exportConfig(&weightsObject, &jsonDoc->GetAllocator());[m
[32m	}[m
    }[m

    // if the section already exists, we delete it first[m
[36m@@ -279,27 +1471,139 @@[m [mvoid NeuralNetwork<TDevice>::exportWeights(const helpers::JsonDocument& jsonDoc)[m
}[m

template <typename TDevice>[m
std::vector<std::vector<std::vector<real_t> > > [31mNeuralNetwork<TDevice>::getOutputs()[m[32mNeuralNetwork<TDevice>::getOutputs([m
[32m    const int layerID, const bool getGateOutput, const real_t mdnoutput)[m
{[m
[31m    layers::TrainableLayer<TDevice> &ol = outputLayer();[m
[31m    [m
    std::vector<std::vector<std::vector<real_t> > > outputs;[m
    [32mlayers::SkipLayer<TDevice> *olg;[m
[32m    layers::MDNLayer<TDevice> *olm;[m
[32m    unsigned char genMethod;[m
[32m    int tempLayerID;[m
[32m    enum genMethod {ERROR = 0, GATEOUTPUT, MDNSAMPLING, MDNPARAMETER, MDNEMGEN, NORMAL};[m

[32m    /*[m
[32m      specify old, olm, tempLayerId[m
[32m       -3.0 is chosen for convience.[m
[32m       [m
[32m       < -3.0: no MDN generation[m
[32m       > -3.0 && < -1.5: generating EM-style[m
[32m       > -1.5 && < 0.0: generate MDN parameters (mdnoutput = -1.0)[m
[32m       > 0.0 : generate samples from MDN with the variance = variance * mdnoutput [m
[32m    if (mdnoutput >= -3.0 && getGateOutput){[m
[32m	genMethod = ERROR;[m
[32m	throw std::runtime_error("MDN output and gate output can not be generated together");[m

[32m    }else if (mdnoutput < -3.0 && getGateOutput){[m
[32m	olg = outGateLayer(layerID);[m
[32m	olm = NULL;[m
[32m	tempLayerId = layerID;[m
[32m	if (olg == NULL)[m
[32m	    throw std::runtime_error("Gate output tap ID invalid\n");[m
[32m	genMethod = GATEOUTPUT;[m

[32m    }else if (mdnoutput >= -3.0 && !getGateOutput){[m
[32m	olg = NULL;[m
[32m	olm = outMDNLayer();[m
[32m	if (olm == NULL)[m
[32m	    throw std::runtime_error("No MDN layer in the current network");[m
[32m	//olm->getOutput(mdnoutput); // Move to computeForward(curMaxSeqLength, generationOpt)[m
[32m	tempLayerId = m_layers.size()-1;[m
[32m	genMethod = (mdnoutput < 0.0) ? ((mdnoutput < -1.5) ? MDNEMGEN:MDNPARAMETER):MDNSAMPLING;[m
[32m	[m
[32m    }else{[m
[32m	olg = NULL;[m
[32m	olm = NULL;[m
[32m	tempLayerId = layerID;[m
[32m	genMethod = NORMAL;[m
[32m    }*/[m

[32m    /* Since we move the olm->getOutput(mdnoutput) to computeForwardPassGen, mdnoutput is not [m
[32m       necessay here[m
[32m     */[m

[32m    // Determine the output layer[m
[32m    if (layerID < 0){[m
[32m	// If layerID is not specified, generate from the last output/postoutput layer[m
[32m	olg = NULL;[m
[32m	olm = outMDNLayer(-1);[m
[32m	if (olm == NULL)[m
[32m	    tempLayerID = this->m_layers.size()-2; // postouput MDN[m
[32m	else[m
[32m	    tempLayerID = this->m_layers.size()-1; // output[m
[32m    }else{[m
[32m	// If layerID is specified, generate from that layer[m
[32m	if (getGateOutput){[m
[32m	    // generate from Highway gate[m
[32m	    olg = outGateLayer(layerID);[m
[32m	    olm = NULL;[m
[32m	    if (olg == NULL) throw std::runtime_error("Gate output tap ID invalid\n");[m
[32m	}else{[m
[32m	    // generate from specified layerID[m
[32m	    olg = NULL;[m
[32m	    olm = outMDNLayer(layerID);[m
[32m	}[m
[32m	tempLayerID = layerID;[m
[32m    }[m

[32m    // Determine the generation method[m
[32m    if (olg == NULL){[m
[32m	if (olm == NULL)[m
[32m	    // output from the layer output[m
[32m	    genMethod = NORMAL;[m
[32m	else[m
[32m	    // output from the MDN layer[m
[32m	    genMethod = (mdnoutput<0.0) ? ((mdnoutput < -1.5) ? MDNEMGEN:MDNPARAMETER):MDNSAMPLING;[m
[32m    }else{[m
[32m	// output from the highway gate[m
[32m	genMethod = GATEOUTPUT;[m
[32m    }[m

[32m    // retrieve the output[m
[32m    layers::Layer<TDevice> &ol  = outputLayer(tempLayerID);[m
    
    for (int patIdx = 0; patIdx < (int)ol.patTypes().size(); ++patIdx) {[m
	switch (ol.patTypes()[patIdx]) {
	case PATTYPE_FIRST:
	    outputs.resize(outputs.size() + 1);
	    
	case PATTYPE_NORMAL:
	case PATTYPE_LAST: [31m{{[m[32m{[m
[32m	    switch (genMethod){[m
[32m	    case MDNEMGEN:[m
[32m	    case MDNSAMPLING:[m
[32m	    case NORMAL:[m
[32m		{[m
		    Cpu::real_vector pattern(ol.outputs().begin() + patIdx * ol.size(), 
					     ol.outputs().begin() + (patIdx+1) * ol.size());
		    int psIdx = patIdx % ol.parallelSequences();
		    outputs[psIdx].push_back(std::vector<real_t>(pattern.begin(), pattern.end()));
		    break;
		[31m}}[m[32m}[m
[32m	    case MDNPARAMETER:[m
[32m		{[m
[32m		    [m
[32m		    Cpu::real_vector pattern([m
[32m				olm->mdnParaVec().begin()+patIdx*olm->mdnParaDim(), [m
[32m				olm->mdnParaVec().begin()+(patIdx+1)*olm->mdnParaDim());[m
[32m		    int psIdx = patIdx % ol.parallelSequences();[m
[32m		    outputs[psIdx].push_back(std::vector<real_t>(pattern.begin(), pattern.end()));[m
[32m		    break;[m
[32m		}[m
[32m	    case GATEOUTPUT:[m
[32m		{[m
[32m		    Cpu::real_vector pattern(olg->outputFromGate().begin() + patIdx * ol.size(),[m
[32m					     olg->outputFromGate().begin()+(patIdx+1) * ol.size());[m
[32m		    int psIdx = patIdx % ol.parallelSequences();[m
[32m		    outputs[psIdx].push_back(std::vector<real_t>(pattern.begin(), pattern.end()));[m
[32m		    break;[m
[32m		}[m
[32m	    default:[m
[32m		break;   [m
[32m	    }[m
[32m	}[m
	default:
	    break;
	}
    }[m

    return outputs;[m
[36m@@ -307,13 +1611,15 @@[m [mstd::vector<std::vector<std::vector<real_t> > > NeuralNetwork<TDevice>::getOutpu[m


/* Add 16-02-22 Wang: for WE updating */[m
// Initialization for using external [31mwe[m[32mWE[m bank
[32m// (read in the word embeddings and save them in a matrix)[m
template <typename TDevice>[m
bool NeuralNetwork<TDevice>::initWeUpdate(const std::string weBankPath, const unsigned weDim, [m
					  const unsigned weIDDim, const unsigned maxLength)[m
{[m
    // check if only the first layer is an input layer[m
    layers::InputLayer<TDevice>* inputLayer = 
	dynamic_cast<layers::InputLayer<TDevice>*>(m_layers.front().get());
    if (!inputLayer)[m
	throw std::runtime_error("The first layer is not an input layer");[m
    else if (!inputLayer->readWeBank(weBankPath, weDim, weIDDim, maxLength)){[m
[36m@@ -321,11 +1627,29 @@[m [mbool NeuralNetwork<TDevice>::initWeUpdate(const std::string weBankPath, const un[m
    }[m
}[m

[32mtemplate <typename TDevice>[m
[32mbool NeuralNetwork<TDevice>::initWeNoiseOpt(const int weNoiseStartDim, const int weNoiseEndDim,[m
[32m					    const real_t weNoiseDev)[m
[32m{[m
[32m    // check if only the first layer is an input layer[m
[32m    layers::InputLayer<TDevice>* inputLayer = [m
[32m	dynamic_cast<layers::InputLayer<TDevice>*>(m_layers.front().get());[m
[32m    if (!inputLayer)[m
[32m	throw std::runtime_error("The first layer is not an input layer");[m
[32m    else if (!inputLayer->initWeNoiseOpt(weNoiseStartDim, weNoiseEndDim, weNoiseDev)){[m
[32m	throw std::runtime_error("Fail to initialize for we updating");[m
[32m    }[m
[32m    return true;[m
[32m}[m



// check whether the input layer uses external we bank[m
template <typename TDevice>[m
bool NeuralNetwork<TDevice>::flagInputWeUpdate() const[m
{[m
    layers::InputLayer<TDevice>* inputLayer = 
	dynamic_cast<layers::InputLayer<TDevice>*>(m_layers.front().get());
    if (!inputLayer){[m
	throw std::runtime_error("The first layer is not an input layer");[m
	return false;[m
[36m@@ -338,7 +1662,8 @@[m [mbool NeuralNetwork<TDevice>::flagInputWeUpdate() const[m
template <typename TDevice>[m
bool NeuralNetwork<TDevice>::saveWe(const std::string weFile) const[m
{[m
    layers::InputLayer<TDevice>* inputLayer = 
	dynamic_cast<layers::InputLayer<TDevice>*>(m_layers.front().get());
    if (!inputLayer){[m
	throw std::runtime_error("The first layer is not an input layer");[m
	return false;[m
[36m@@ -352,7 +1677,8 @@[m [mtemplate <typename TDevice>[m
bool NeuralNetwork<TDevice>::initMseWeight(const std::string mseWeightPath)[m
{[m
    [m
    layers::PostOutputLayer<TDevice>* outputLayer = 
	[31mdynamic_cast<layers::PostOutputLayer<TDevice>&>(*m_layers.back())[m[32mdynamic_cast<layers::PostOutputLayer<TDevice>*>(m_layers.back().get());[m
    if (!outputLayer){[m
	throw std::runtime_error("The output layer is not a postoutput layer");[m
	return false;[m
[36m@@ -362,6 +1688,398 @@[m [mbool NeuralNetwork<TDevice>::initMseWeight(const std::string mseWeightPath)[m
   [m
}[m

[32m/* Add 0413 Wang: for weight mask */[m
[32mtemplate <typename TDevice>[m
[32mbool NeuralNetwork<TDevice>::initWeightMask(const std::string weightMaskPath,[m
[32m					    const int         weightMaskOpt)[m
[32m{[m
[32m    std::ifstream ifs(weightMaskPath.c_str(), std::ifstream::binary | std::ifstream::in);[m
[32m    if (!ifs.good())[m
[32m	throw std::runtime_error(std::string("Fail to open") + weightMaskPath);[m
[32m    [m
[32m    // get the number of we data[m
[32m    std::streampos numEleS, numEleE;[m
[32m    long int numEle;[m
[32m    numEleS = ifs.tellg();[m
[32m    ifs.seekg(0, std::ios::end);[m
[32m    numEleE = ifs.tellg();[m
[32m    numEle  = (numEleE-numEleS)/sizeof(real_t);[m
[32m    ifs.seekg(0, std::ios::beg);[m

[32m    real_t tempVal;[m
[32m    std::vector<real_t> tempVec;[m
[32m    for (unsigned int i = 0; i<numEle; i++){[m
[32m	ifs.read ((char *)&tempVal, sizeof(real_t));[m
[32m	tempVec.push_back(tempVal);[m
[32m    }[m
[32m    [m
[32m    printf("Initialize weight mask: %d mask elements in total, ", (int)numEle);[m
[32m    printf("under the mode %d", weightMaskOpt);[m
[32m    [m
[32m    int pos = 0;[m
[32m    if (weightMaskOpt > 0){[m
[32m	printf("\n\tRead mask for embedded vectors ");[m
[32m	layers::InputLayer<TDevice>* inputLayer = [m
[32m	    dynamic_cast<layers::InputLayer<TDevice>*>((m_layers[0]).get());[m
[32m	pos = inputLayer->readWeMask(tempVec.begin());[m
[32m	printf("(%d elements)", pos);[m
[32m    }[m

[32m    if (weightMaskOpt == 0 || weightMaskOpt==2){[m
[32m	printf("\n\tRead mask for NN weights (");[m
[32m	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	    layers::TrainableLayer<TDevice>* weightLayer = [m
[32m		dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());[m
[32m	    if (weightLayer){[m
[32m		if (weightLayer->weightNum()+pos > numEle){[m
[32m		    throw std::runtime_error(std::string("Weight mask input is not long enough"));[m
[32m		}else{[m
[32m		    weightLayer->readWeightMask(tempVec.begin()+pos, [m
[32m						tempVec.begin()+pos+weightLayer->weightNum());[m
[32m		    pos = pos+weightLayer->weightNum();[m
[32m		}[m
[32m		printf("%d ", weightLayer->weightNum());[m
[32m	    }[m
[32m	}[m
[32m	printf("elements)");[m
[32m    }[m
[32m    printf("\n");[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::maskWeight()[m
[32m{[m
[32m    // mask the embedded vectors (if applicable)[m
[32m    layers::InputLayer<TDevice>* inputLayer = [m
[32m	dynamic_cast<layers::InputLayer<TDevice>*>((m_layers[0]).get());[m
[32m    inputLayer->maskWe();[m

[32m    // mask the weight (always do, as the default mask value is 1.0)[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layers::TrainableLayer<TDevice>* weightLayer = [m
[32m	    dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());[m
[32m	if (weightLayer){[m
[32m	    weightLayer->maskWeight();[m
[32m	}[m
[32m    }[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::notifyCurrentEpoch(const int trainingEpoch)[m
[32m{[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layer->setCurrTrainingEpoch(trainingEpoch);[m
[32m    }[m
[32m    m_trainingEpoch = trainingEpoch;[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::notifyCurrentFrac(const int fracNum)[m
[32m{[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layer->setCurrTrainingFrac(fracNum);[m
[32m    }[m
[32m    m_trainingFrac = fracNum;[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::updateNNState(const int trainingEpoch, const int fracNum)[m
[32m{[m

[32m    if (flagNetworkForGAN()){[m
[32m	// Rule:[m
[32m	//  temp == 1: train discriminator using natural data[m
[32m	//  temp == 2: train discriminator using generated data[m
[32m	//  temp == 0:[m
[32m	//         if featMatch is used, train the generator using feature matching[m
[32m	//         else the normal way[m
[32m	[m
[32m	int temp = ((fracNum + 1) % 3);[m
[32m	if (temp == 1)[m
[32m	    m_trainingState = NN_STATE_GAN_DIS_NATDATA;[m
[32m	else if (temp == 2)[m
[32m	    m_trainingState = NN_STATE_GAN_DIS_GENDATA;[m
[32m	else if (temp == 0)[m
[32m	    m_trainingState = (m_featMatchLayer > 0)?(NN_STATE_GAN_GEN_FEATMAT):(NN_STATE_GAN_GEN);[m
[32m	else[m
[32m	    throw std::runtime_error("Undefined nnstate");[m
[32m    }else{[m
[32m	m_trainingState = NN_STATE_GAN_NOGAN;[m
[32m    }[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::updateNNStateForGeneration()[m
[32m{[m
[32m    m_trainingState = NN_STATE_GENERATION_STAGE;[m
[32m}[m


[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::reInitWeight()[m
[32m{[m
[32m    printf("Reinitialize the weight\n");[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layer->reInitWeight();[m
[32m    }[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::initOutputForMDN([m
[32m const data_sets::DataSetMV &datamv)[m
[32m{[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer,[m
[32m		   m_layers){[m
[32m	layers::MDNLayer<TDevice>* mdnLayer = [m
[32m	    dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
[32m	if (mdnLayer){[m
[32m	    mdnLayer->initPreOutput(datamv.outputM(), datamv.outputV());[m
[32m	    printf("\nRe-initialize the layer before MDN \t");[m
[32m	    if (datamv.outputM().size()<1)[m
[32m		printf("using global zero mean and uni variance");[m
[32m	    else[m
[32m		printf("using data mean and variance");[m
[32m	}[m
[32m    }[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::readMVForOutput([m
[32m const data_sets::DataSetMV &datamv)[m
[32m{[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer,[m
[32m		   m_layers){[m
[32m	layers::PostOutputLayer<TDevice>* outputLayer = [m
[32m	    dynamic_cast<layers::PostOutputLayer<TDevice>*>(layer.get());[m
[32m	if (outputLayer){[m
[32m	    outputLayer->readMV(datamv.outputM(), datamv.outputV());[m
[32m	    printf("Read mean and variance into output layer \t");[m
[32m	}[m
[32m    }[m
[32m}[m


[32m/* importWeights[m
[32m * import weights from pre-trained model[m
[32m */[m
[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::importWeights(const helpers::JsonDocument &jsonDoc, [m
[32m					   const std::string &ctrStr)[m
[32m{[m
[32m    try{[m
[32m	// Step1. read in the control vector, a sequence of 1 0[m
[32m	Cpu::int_vector tempctrStr;[m
[32m	tempctrStr.resize(m_layers.size(), 1);[m
[32m	if (ctrStr.size() > 0 && ctrStr.size()!= m_layers.size()){[m
[32m	    printf("\n\tLength of trainedModelCtr is unequal to the number of layers in ");[m
[32m	    printf("the network to be trained\n");[m
[32m	    throw std::runtime_error("Please check trainedModelCtr");[m
[32m	}else if (ctrStr.size()>0){[m
[32m	    for (int i=0; i<ctrStr.size(); i++)[m
[32m		tempctrStr[i] = ctrStr[i]-'0';[m
[32m	}else{[m
[32m	    // nothing[m
[32m	}[m
[32m	[m
[32m	// Step2. read in the weights from the pre-trained network[m
[32m	helpers::JsonValue weightsSection;[m
[32m        if (jsonDoc->HasMember("weights")) {[m
[32m            if (!(*jsonDoc)["weights"].IsObject())[m
[32m                throw std::runtime_error("Section 'weights' is not an object");[m
[32m            weightsSection = helpers::JsonValue(&(*jsonDoc)["weights"]);[m
[32m        }else{[m
[32m	    throw std::runtime_error("No weight section found");[m
[32m	}[m

[32m	// Step3. for each layer in the network to be traiend[m
[32m	//        load the weights from the pre-trained network[m
[32m	//        if [m
[32m	//         1. control option is 1[m
[32m	//         2. the name matches (checked handles in Layer->reReadWeight())[m
[32m	[m
[32m	int cnt=0;[m
[32m	BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers)[m
[32m	{[m
[32m	    layers::TrainableLayer<TDevice>* Layer = [m
[32m		dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());[m
[32m	    [m
[32m	    // Read in the parameter for a hidden layer[m
[32m	    if (Layer && tempctrStr[cnt] > 0){[m
[32m		printf("\n\t(%d) ", cnt);[m
[32m		Layer->reReadWeight(weightsSection, Layer->size(), tempctrStr[cnt]);[m
[32m				[m
[32m	    // Read in the parameter for MDN layer with trainable link[m
[32m	    }else if(tempctrStr[cnt] > 0){[m
[32m		layers::MDNLayer<TDevice>* mdnlayer = [m
[32m		    dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
[32m		if (mdnlayer && mdnlayer->flagTrainable()){[m
[32m		    printf("\n\t(%d) ", cnt);[m
[32m		    mdnlayer->reReadWeight(weightsSection, tempctrStr[cnt]);[m
[32m		}[m
[32m		[m
[32m	    // This layer is skipped[m
[32m	    }else if(Layer){[m
[32m		printf("\n\t(%d) not read weight for layer %s", cnt, Layer->name().c_str());[m
[32m	    }else{[m
[32m		// other cases[m
[32m	    }[m
[32m	    cnt++;[m
[32m	}[m
[32m	printf("\tdone\n\n");[m
[32m	[m
[32m    }catch (const std::exception &e){[m
[32m	printf("\n\t%s\n", e.what());[m
[32m	printf("\n\tError in reading weight from a pre-trained network:");[m
[32m	printf("\n\tPlease check the configuration of trainedModel and trainedModelCtr");[m
[32m	printf("\n\t1. trainedModel points to the correct pre-trained network.jsn ?");[m
[32m	printf("\n\t2. trainedModelCtr, a sequence of 0/1, has the same length as");[m
[32m	printf("the number of layers in network.jsn to be trained?");[m
[32m	printf("\n\t3. the layer to be initialized has the same name as the pre-trained layer?\n");[m
[32m	throw std::runtime_error(std::string("Fail to read network weight")+e.what());[m
[32m    }[m
[32m}[m


[32mtemplate <typename TDevice>[m
[32mCpu::real_vector NeuralNetwork<TDevice>::getMdnConfigVec()[m
[32m{[m
[32m    Cpu::real_vector temp;[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layers::MDNLayer<TDevice>* mdnLayer = [m
[32m	    dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
[32m	if (mdnLayer)[m
[32m	    temp = mdnLayer->getMdnConfigVec();[m
[32m    }    [m
[32m    return temp;[m
[32m}[m

[32m// PrintWeightMatrix[m
[32m// print the weight of a network to a binary data[m
[32m// use ReadCURRENNTWeight(filename,format,swap) matlab function to read the data[m
[32mtemplate <typename TDevice>[m
[32mvoid NeuralNetwork<TDevice>::printWeightMatrix(const std::string weightPath, const int opt)[m
[32m{[m
[32m    std::fstream ifs(weightPath.c_str(),[m
[32m		      std::ifstream::binary | std::ifstream::out);[m
[32m    if (!ifs.good()){[m
[32m	throw std::runtime_error(std::string("Fail to open output weight path: "+weightPath));[m
[32m    }[m

[32m    // format of the output binary weight[m
[32m    std::vector<int> weightSize;[m
[32m    weightSize.clear();[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layers::TrainableLayer<TDevice>* Layer = [m
[32m	    dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());[m
[32m	[m
[32m	if (Layer){[m
[32m	    weightSize.push_back(Layer->weights().size());[m
[32m	    weightSize.push_back(Layer->size());[m
[32m	    weightSize.push_back(Layer->precedingLayer().size());[m
[32m	    weightSize.push_back(Layer->inputWeightsPerBlock());[m
[32m	    weightSize.push_back(Layer->internalWeightsPerBlock());[m
[32m	    if (opt==1){[m
[32m		if (Layer->type()=="feedforward_tanh")[m
[32m		    weightSize.push_back(0);[m
[32m		else if (Layer->type()=="feedforward_logistic")[m
[32m		    weightSize.push_back(1);[m
[32m		else if (Layer->type()=="feedforward_identity")[m
[32m		    weightSize.push_back(2);[m
[32m		else if (Layer->type()=="feedforward_relu")[m
[32m		    weightSize.push_back(3);		[m
[32m		else if (Layer->type()=="lstm")[m
[32m		    weightSize.push_back(4);		[m
[32m		else if (Layer->type()=="blstm")[m
[32m		    weightSize.push_back(5);[m
[32m		else[m
[32m		    printf("other weight type not implemented\n");[m
[32m	    }[m
[32m	}else{[m
[32m	    layers::MDNLayer<TDevice>* mdnlayer = [m
[32m		dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
[32m	    if (mdnlayer && mdnlayer -> flagTrainable()){[m
[32m		weightSize.push_back(mdnlayer->weights().size());[m
[32m		weightSize.push_back(mdnlayer->weights().size());[m
[32m		weightSize.push_back(0);  // previous size = 0[m
[32m		weightSize.push_back(1);  // internal block = 1[m
[32m		weightSize.push_back(0);  // internal weight = 0[m
[32m	    }[m
[32m	}[m
[32m    }[m

[32m    printf("Writing network to binary format: \n");[m
[32m    // macro information[m
[32m    // Number of layers[m
[32m    // weight size, layer size, preceding layer size, inputWeightsPerBlock, internalWeightsPerBlock[m
[32m    real_t tmpPtr;[m
[32m    tmpPtr = (real_t)weightSize.size()/((opt==1)?6:5);[m
[32m    ifs.write((char *)&tmpPtr, sizeof(real_t));[m
[32m    for (int i = 0 ; i<weightSize.size(); i++){[m
[32m	tmpPtr = (real_t)weightSize[i];[m
[32m	ifs.write((char *)&tmpPtr, sizeof(real_t));[m
[32m    }[m

[32m    // weights[m
[32m    int cnt = 0;[m
[32m    real_t *tmpPtr2;[m
[32m    Cpu::real_vector weightVec;[m
[32m    BOOST_FOREACH (boost::shared_ptr<layers::Layer<TDevice> > &layer, m_layers){[m
[32m	layers::TrainableLayer<TDevice>* Layer = [m
[32m	    dynamic_cast<layers::TrainableLayer<TDevice>*>(layer.get());[m
[32m	if (Layer){[m
[32m	    weightVec = Layer->weights();[m
[32m	    tmpPtr2 = weightVec.data();[m
[32m	    if (weightVec.size()>0 && tmpPtr2)[m
[32m		ifs.write((char *)tmpPtr2, sizeof(real_t)*Layer->weights().size());	[m
[32m	    printf("Layer (%2d) %s with %lu weights\n", cnt, Layer->type().c_str(), weightVec.size());[m
[32m	}else{[m
[32m	    layers::MDNLayer<TDevice>* mdnlayer = [m
[32m		dynamic_cast<layers::MDNLayer<TDevice>*>(layer.get());[m
[32m	    if (mdnlayer && mdnlayer -> flagTrainable()){[m
[32m		weightVec = mdnlayer->weights();[m
[32m		tmpPtr2 = weightVec.data();[m
[32m		if (weightVec.size()>0 && tmpPtr2){[m
[32m		    ifs.write((char *)tmpPtr2, sizeof(real_t)*mdnlayer->weights().size());[m
[32m		}else{[m
[32m		    throw std::runtime_error("Fail to output weight. Void pointer");[m
[32m		}[m
[32m		printf("Layer (%2d) MDN with %lu weights\n", cnt, weightVec.size());[m
[32m	    }[m
[32m	}[m
[32m	cnt++;[m
[32m    }[m
[32m    ifs.close();[m
[32m    printf("Writing done\n");[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mint NeuralNetwork<TDevice>::layerSize(const int layerID)[m
[32m{[m
[32m    if (layerID < 0)[m
[32m	return m_layers.back()->size();[m
[32m    else if (layerID > (m_layers.size()-1))[m
[32m	throw std::runtime_error(std::string("Invalid layer ID. In NN.layerSize"));[m
[32m    else[m
[32m	return m_layers[layerID]->size();[m
[32m}[m

[32mtemplate <typename TDevice>[m
[32mbool NeuralNetwork<TDevice>::isMDNLayer(const int layerID)[m
[32m{[m
[32m    if (layerID < 0)[m
[32m	return ((dynamic_cast<layers::MDNLayer<TDevice>*>(m_layers.back().get())) != NULL);[m
[32m    else if (layerID > (m_layers.size()-1))[m
[32m	throw std::runtime_error(std::string("Invalid layer ID. In NN.isMDNLayer"));[m
[32m    else[m
[32m	return ((dynamic_cast<layers::MDNLayer<TDevice>*>(m_layers[layerID].get())) != NULL);[m
[32m}[m


[32mtemplate class beamsearch::searchEngine<Gpu>;[m
[32mtemplate class beamsearch::searchEngine<Cpu>;[m

[32mtemplate class beamsearch::searchState<Gpu>;[m
[32mtemplate class beamsearch::searchState<Cpu>;[m

// explicit template instantiations[m
template class NeuralNetwork<Cpu>;[m
[1mdiff --git a/NeuralNetwork.hpp b/NeuralNetwork.hpp[m
[1mindex 896719e..4bd2dd0 100644[m
[1m--- a/NeuralNetwork.hpp[m
[1m+++ b/NeuralNetwork.hpp[m
[36m@@ -71,8 +71,7 @@[m [mpublic:[m
     * @param maxSeqLength      The maximum length of a sequence[m
     */[m
    NeuralNetwork(const helpers::JsonDocument &jsonDoc, int parallelSequences, [m
		  int maxSeqLength, int[31mchaDim, int maxTxtLength,[m
[31m                  int[m inputSizeOverride=-1, int outputSizeOverride=-1);
[m
    /**[m
     * Destructs the neural network[m
[36m@@ -204,7 +203,8 @@[m [mpublic:[m
    void reInitWeight();[m
[m
    /* Add 0514 Wang: initialize the output layer for MDN */[m
    void initOutputForMDN(const [32mhelpers::JsonDocument &jsonDoc,[m
[32m			  const[m data_sets::DataSetMV &datamv);
[m
    /* Add 1012 Read the mean and variance to the output layer*/[m
    void readMVForOutput(const data_sets::DataSetMV &datamv);[m
[1mdiff --git a/NeuralNetwork.hpp~ b/NeuralNetwork.hpp~[m
[1mindex 680cd3f..4b287b2 100644[m
[1m--- a/NeuralNetwork.hpp~[m
[1m+++ b/NeuralNetwork.hpp~[m
[36m@@ -26,6 +26,8 @@[m
#include "layers/InputLayer.hpp"[m
#include "layers/TrainableLayer.hpp"[m
#include "layers/PostOutputLayer.hpp"[m
[32m#include "layers/MDNLayer.hpp"[m
[32m#include "layers/SkipParaLayer.hpp"[m
#include "data_sets/DataSet.hpp"[m
#include "helpers/JsonClassesForward.hpp"[m
[m
[36m@@ -52,6 +54,14 @@[m [mprivate:[m
    /* Add 02-24 Wang Residual Network*/[m
    std::vector<layers::Layer<TDevice>*> m_skipAddLayers;[m
[m
    [32mint m_firstFeedBackLayer;                                  // ID of the first feedback Layer[m
[32m    int m_middlePostOutputLayer;[m
[32m    int m_featMatchLayer;[m
[32m    int m_vaeLayer;[m
[32m    int m_trainingEpoch;[m
[32m    int m_trainingFrac;[m
[32m    int m_trainingState;[m
    
public:[m
    /**[m
     * Creates the neural network from the process configuration[m
[36m@@ -60,8 +70,8 @@[m [mpublic:[m
     * @param parallelSequences The maximum number of sequences that shall be computed in parallel[m
     * @param maxSeqLength      The maximum length of a sequence[m
     */[m
    NeuralNetwork(const helpers::JsonDocument &jsonDoc, int parallelSequences, 
		  int maxSeqLength, int [31minputSizeOverride,[m[32minputSizeOverride=-1,[m int [31moutputSizeOverride);[m[32moutputSizeOverride=-1);[m
[m
    /**[m
     * Destructs the neural network[m
[36m@@ -87,7 +97,13 @@[m [mpublic:[m
     *[m
     * @return The output layer[m
     */[m
    [31mlayers::TrainableLayer<TDevice>&[m[32m/* Modify 04-08 Wang: to tap in the output of arbitary layer */[m
[32m    //layers::TrainableLayer<TDevice>&[m outputLayer();
    [32mlayers::Layer<TDevice>& outputLayer(const int layerID=-1);[m
[32m[m
[32m    layers::SkipLayer<TDevice>* outGateLayer(const int layerID);[m
[32m    [m
[32m    layers::MDNLayer<TDevice>* outMDNLayer(const int layerID=-1);[m
[m
    /**[m
     * Returns the post output layer[m
[36m@@ -103,12 +119,19 @@[m [mpublic:[m
     */[m
    void loadSequences(const data_sets::DataSetFraction &fraction);[m
[m
    [32mvoid restoreTarget(const data_sets::DataSetFraction &fraction);[m
    
    /**[m
     * Computes the forward pass[m
     */[m
    void [31mcomputeForwardPass();[m[32mcomputeForwardPass(const int curMaxSeqLength, const real_t uttCnt);[m
[m
    /**[m
     [32m* Computes the forward pass[m
[32m     */[m
[32m    void computeForwardPassGen(const int curMaxSeqLength, const real_t generationOpt);[m
[32m    [m
[32m    /**[m
     * Computes the backward pass, including the weight updates[m
     *[m
     * The forward pass must be computed first![m
[36m@@ -122,7 +145,7 @@[m [mpublic:[m
     *[m
     * @return The computed error[m
     */[m
    real_t [31mcalculateError()[m[32mcalculateError(const bool flagGenerateMainError)[m const;
[m
    /**[m
     * Stores the description of the layers in a JSON tree[m
[36m@@ -146,15 +169,71 @@[m [mpublic:[m
     *[m
     * @return Outputs of the processed fraction[m
     */[m
    std::vector<std::vector<std::vector<real_t> > > [31mgetOutputs();[m[32mgetOutputs(const int  layerID        = -1, [m
[32m							       const bool gateFromOutput = false,[m
[32m							       const real_t  mdnoutput   = -4.0);[m
[32m    [m
[32m    /**[m
[32m     * Read in the weight from trained_network.jsn or .autosave[m
[32m     * [m
[32m     */[m
[32m    void importWeights(const helpers::JsonDocument &jsonDoc, const std::string &ctrStr);[m
    
    /* Add 16-02-22 Wang: for WE updating */[m
    // repare for we updateing[m
    bool initWeUpdate(const std::string weBankPath, const unsigned weDim, [m
		      const unsigned weIDDim, const unsigned maxLength);[m
    [m
    [32mbool initWeNoiseOpt(const int weNoiseStartDim, const int weNoiseEndDim,[m
[32m			const real_t weNoiseDev);[m
    
    bool flagInputWeUpdate() const;[m

    bool saveWe(const std::string weFile) const;[m
    
    [32m/* Add 04-01 Wang: for RMSE output mask */[m
[32m    bool initMseWeight(const std::string mseWeightPath);[m
[32m[m
[32m    /* Add 0413 Wang: for weight mask */[m
[32m    bool initWeightMask(const std::string weightMaskPath, const int weightMaskOpt);[m
[32m[m
[32m    void maskWeight();[m
[32m    [m
[32m    /* Add 0511 Wang: re-initialize the weight*/[m
[32m    void reInitWeight();[m
[32m[m
[32m    /* Add 0514 Wang: initialize the output layer for MDN */[m
[32m    void initOutputForMDN(const data_sets::DataSetMV &datamv);[m
[32m[m
[32m    /* Add 1012 Read the mean and variance to the output layer*/[m
[32m    void readMVForOutput(const data_sets::DataSetMV &datamv);[m
[32m    [m
[32m    /* Add 0531 Wang: get the mdn config*/[m
[32m    Cpu::real_vector getMdnConfigVec();[m
[32m[m
[32m    /* Add 0630 Wang: print the binary weight matrix */[m
[32m    void printWeightMatrix(const std::string weightPath, const int opt);[m
[32m    [m
[32m    /* Add 0928 Wang: notify the current training epoch to each layer*/[m
[32m    void notifyCurrentEpoch(const int trainingEpoch);[m
[32m[m
[32m    /* Add 0928 Wang: notify the current training epoch to each layer*/[m
[32m    void notifyCurrentFrac(const int fracNum);[m
[32m[m
[32m    /* Add 170515 update the current state*/[m
[32m    void updateNNState(const int trainingEpoch, const int fracNum);[m
[32m[m
[32m    void updateNNStateForGeneration();[m
[32m    [m
[32m    int  layerSize(const int layerID);[m
[32m[m
[32m    bool isMDNLayer(const int layerID);[m
[32m    [m
[32m    /* Add 17-05-02: support for the GAN training */[m
[32m    void cleanGradientsForDiscriminator();[m
[32m    [m
[32m    //[m
[32m    bool flagNetworkForGAN() const;[m
};[m
[m
[m
[1mdiff --git a/activation_functions/Relu.cuh b/activation_functions/Relu.cuh[m
[1mindex e24ec3a..3591e5a 100644[m
[1m--- a/activation_functions/Relu.cuh[m
[1m+++ b/activation_functions/Relu.cuh[m
[36m@@ -25,6 +25,7 @@[m
[m
#include "../Types.hpp"[m
[m
[32m#define ACTIVIATION_FUNCTION_RELU_LeakyFactor 0.01[m
[m
namespace activation_functions {[m
[m
[36m@@ -32,12 +33,13 @@[m [mnamespace activation_functions {[m
    {[m
        static __host__ __device__ real_t fn(real_t x)[m
        {[m
            [31mreturn[m[32m//return[m (x+fabsf(x))*0.5;
	    [32mreturn (x>0)?(x):(exp(x)-1);[m
        }[m
[m
        static __host__ __device__ real_t deriv(real_t y)[m
        {[m
            return [31my>0;[m[32m(y>0)?(1.0):(y+1.0);[m
        }[m
    };[m
[m
[1mdiff --git a/data_sets/DataSet.cpp b/data_sets/DataSet.cpp[m
[1mindex 36a67f4..a6430c5 100644[m
[1m--- a/data_sets/DataSet.cpp[m
[1m+++ b/data_sets/DataSet.cpp[m
[36m@@ -27,6 +27,10 @@[m
#include <boost/thread.hpp>[m
#include <boost/function.hpp>[m
[m
[32m#include <thrust/functional.h>[m
[32m#include <thrust/fill.h>[m
[32m#include <thrust/transform.h>[m

#include "DataSet.hpp"[m
#include "../Configuration.hpp"[m
[m
[36m@@ -39,7 +43,9 @@[m
#include <limits>[m
#include <cassert>[m
[m
[32m#define DATASET_EXINPUT_TYPE_0 0 // nothing[m
[32m#define DATASET_EXINPUT_TYPE_1 1 // input is the index in a increasing order ([1 1 1 2..2 3..3])[m
[32m                                 // other types may be implemented in the future[m
namespace {[m
namespace internal {[m
[m
[36m@@ -253,35 +259,85 @@[m [mnamespace internal {[m
	return numEle;[m
    }[m
    [m
    int readRealData(const std::string dataPath, Cpu::real_vector [31m&data)[m[32m&data, [m
[32m		     const int startPos, const int endPos)[m
    {[m
	// [m
	std::ifstream ifs(dataPath.c_str(), std::ifstream::binary | std::ifstream::in);[m
	if (!ifs.good())[m
	    throw std::runtime_error(std::string("Fail to open ")+dataPath);[m
	[m
	// get the number of[31mwe[m data [32melements[m
	std::streampos numEleS, numEleE;[m
	long int numEle;[m
	[32mlong int stPos, etPos;[m
[32m	real_t   tempVal;[m
[32m[m
[32m	stPos = startPos; etPos = endPos;[m

	numEleS = ifs.tellg();[m
	ifs.seekg(0, std::ios::end);[m
	numEleE = ifs.tellg();[m
	numEle  = (numEleE-numEleS)/sizeof(real_t);[m
	ifs.seekg(0, std::ios::beg);[m
	[m
	[32mif (etPos == -1) etPos = numEle;[m
[32m	if (stPos >= etPos || stPos < 0 || etPos > numEle)[m
[32m	    throw std::runtime_error(std::string("Fail to read ")+dataPath);[m

	// read in the data[m
	data = [31mCpu::real_vector(numEle,[m[32mCpu::real_vector(etPos - stPos,[m 0);
	[31mreal_t tempVal;[m
[31m	std::vector<real_t> tempVec;[m[32mifs.seekg(stPos * sizeof(real_t), std::ios::beg);[m
	for [31m(unsigned[m[32m(long[m int i = 0; [31mi<numEle;[m[32mi<(etPos-stPos);[m i++){
	    ifs.read ((char *)&tempVal, sizeof(real_t));[m
	    [31mtempVec.push_back(tempVal);[m[32mdata[i] = tempVal;[m
	}[m
	[31mthrust::copy(tempVec.begin(),[m[32m//thrust::copy(tempVec.begin(),[m tempVec.end(), data.begin());
	ifs.close();[m
	return [31mnumEle;[m[32m(etPos - stPos);[m
    }[m
[m
    [32mint readRealDataAndFill(const std::string dataPath, Cpu::real_vector &buff, [m
[32m			    const int startPos, const int endPos, const int bufDim,[m
[32m			    const int dataDim,  const int dataStartDim)[m
[32m    {[m
[32m	// [m
[32m	std::ifstream ifs(dataPath.c_str(), std::ifstream::binary | std::ifstream::in);[m
[32m	if (!ifs.good())[m
[32m	    throw std::runtime_error(std::string("Fail to open ")+dataPath);[m
[32m	[m
[32m	// get the number of data elements[m
[32m	std::streampos numEleS, numEleE;[m
[32m	long int numEle;[m
[32m	long int stPos, etPos;[m
[32m	real_t   tempVal;[m
[32m[m
[32m	stPos = startPos; etPos = endPos;[m
[32m[m
[32m	numEleS = ifs.tellg();[m
[32m	ifs.seekg(0, std::ios::end);[m
[32m	numEleE = ifs.tellg();[m
[32m	numEle  = (numEleE-numEleS)/sizeof(real_t);[m
[32m	ifs.seekg(0, std::ios::beg);[m
[32m	[m
[32m	if (etPos == -1) etPos = numEle;[m
[32m	if (stPos >= etPos || stPos < 0 || etPos > numEle)[m
[32m	    throw std::runtime_error(std::string("Fail to read in readReadlDataAndFill ")+dataPath);[m
[32m [m
[32m	// read in the data[m
[32m	long int timeIdx, dimIdx;[m
[32m	ifs.seekg(stPos * sizeof(real_t), std::ios::beg);[m
[32m	[m
[32m	for (long int i = 0; i < (etPos-stPos); i++){[m
[32m	    ifs.read ((char *)&tempVal, sizeof(real_t));[m
[32m	    timeIdx = i / dataDim;[m
[32m	    dimIdx  = i % dataDim;[m
[32m	    buff[ timeIdx * bufDim + dataStartDim + dimIdx ] = tempVal;[m
[32m	}[m
[32m	//thrust::copy(tempVec.begin(), tempVec.end(), data.begin());[m
[32m	ifs.close();[m
[32m	return (etPos - stPos);[m
[32m    }[m
    [m
    bool comp_seqs(const data_sets::DataSet::sequence_t &a, [m
		   const data_sets::DataSet::sequence_t &b)[m
[36m@@ -388,9 +444,8 @@[m [mnamespace data_sets {[m
[m
    Cpu::real_vector DataSet::_loadInputsFromCache(const sequence_t &seq)[m
    {[m
	[32m/*[m
	if [31m(m_exInputFlag){[m[32m(0 && m_exInputFlag){[m
	    // read in external data case[m
	    Cpu::real_vector v(seq.length * m_exInputDim[0]);[m
	    m_cacheFile.seekg(seq.exInputBegin);[m
[36m@@ -404,8 +459,12 @@[m [mnamespace data_sets {[m
	    m_cacheFile.read((char*)v.data(), sizeof(real_t) * v.size());[m
	    assert (m_cacheFile.tellg() - seq.inputsBegin == v.size() * sizeof(real_t));[m
	    return v;[m
	    [31m}[m[32m}*/[m
[32m	Cpu::real_vector v(seq.length * m_inputPatternSize);[m
[32m	m_cacheFile.seekg(seq.inputsBegin);[m
[32m	m_cacheFile.read((char*)v.data(), sizeof(real_t) * v.size());[m
[32m	assert (m_cacheFile.tellg() - seq.inputsBegin == v.size() * sizeof(real_t));[m
[32m	return v;[m
    }[m
[m
    Cpu::real_vector DataSet::_loadOutputsFromCache(const sequence_t &seq)[m
[36m@@ -419,6 +478,15 @@[m [mnamespace data_sets {[m
        return v;[m
    }[m
[m
    [32mCpu::real_vector DataSet::_loadExInputsFromCache(const sequence_t &seq)[m
[32m    {[m
[32m	Cpu::real_vector v(seq.exInputLength * seq.exInputDim);[m
[32m	m_cacheFile.seekg(seq.exInputBegin);[m
[32m	m_cacheFile.read((char*)v.data(), sizeof(real_t) * v.size());[m
[32m	assert (m_cacheFile.tellg() - seq.exInputBegin == v.size() * sizeof(real_t));[m
[32m	return v;[m
[32m    }[m

    Cpu::int_vector DataSet::_loadTargetClassesFromCache(const sequence_t &seq)[m
    {[m
        Cpu::int_vector v(seq.length);[m
[36m@@ -429,7 +497,8 @@[m [mnamespace data_sets {[m
[m
        return v;[m
    }[m

    [32m/*[m
    Cpu::real_vector DataSet::_loadTxtDataFromCache(const sequence_t &seq)[m
    {[m
        Cpu::real_vector v(seq.txtLength * m_txtDataPatternSize);[m
[36m@@ -439,7 +508,7 @@[m [mnamespace data_sets {[m
        assert (m_cacheFile.tellg() - seq.txtDataBegin == v.size() * sizeof(int));[m
[m
        return v;[m
	[31m}[m[32m}*/[m
[m
    Cpu::real_vector DataSet::_loadAuxRealDataFromCache(const sequence_t &seq)[m
    {[m
[36m@@ -476,17 +545,27 @@[m [mnamespace data_sets {[m
[m
        //printf("(%d) Making task firstSeqIdx=%d...\n", (int)m_sequences.size(), firstSeqIdx);[m
        boost::shared_ptr<DataSetFraction> frac(new DataSetFraction);[m
[31mif (m_exInputFlag)[m
[31m	    frac->m_inputPatternSize  = m_exInputDim[0] * context_length;[m
[31m	else[m
	frac->m_inputPatternSize  = m_inputPatternSize * context_length;
	[m
        frac->m_outputPatternSize = m_outputPatternSize;[m
        frac->m_maxSeqLength      = std::numeric_limits<int>::min();[m
        frac->m_minSeqLength      = std::numeric_limits<int>::max();[m

	// Add [32m0815: info of the external input data[m
[32m	if (m_exInputFlag){[m
[32m	    if (m_exInputDims.size())[m
[32m		frac->m_exInputDim        = SumCpuIntVec(m_exInputDims);[m
[32m	    else[m
[32m		frac->m_exInputDim        = m_exInputDim;[m
[32m	}[m
[32m	[m
[32m	frac->m_maxExInputLength  = std::numeric_limits<int>::min();[m
[32m	frac->m_minExInputLength  = std::numeric_limits<int>::max();[m
[32m	[m
[32m	// Add[m 0620:  
	[31mWang [m
[31m	frac->m_maxTxtLength[m[32m//frac->m_maxTxtLength[m      = std::numeric_limits<int>::min();
	[31mfrac->m_txtPatternSize[m[32m//frac->m_txtPatternSize[m    = m_hasTxtData ? m_txtDataPatternSize : 0;
	[m
[m
        // fill fraction sequence info[m
[36m@@ -494,16 +573,21 @@[m [mnamespace data_sets {[m
            if (seqIdx < (int)m_sequences.size()) {[m
                frac->m_maxSeqLength = std::max(frac->m_maxSeqLength, m_sequences[seqIdx].length);[m
                frac->m_minSeqLength = std::min(frac->m_minSeqLength, m_sequences[seqIdx].length);[m
		
		[32mfrac->m_maxExInputLength = std::max(frac->m_maxExInputLength,[m
[32m						    m_sequences[seqIdx].exInputLength);[m
[32m		frac->m_minExInputLength = std::min(frac->m_minExInputLength,[m
[32m						    m_sequences[seqIdx].exInputLength);[m
		
                DataSetFraction::seq_info_t seqInfo;[m
                seqInfo.originalSeqIdx = m_sequences[seqIdx].originalSeqIdx;[m
                seqInfo.length         = m_sequences[seqIdx].length;[m
                seqInfo.seqTag         = m_sequences[seqIdx].seqTag;[m
[m
		[31mfrac->m_maxTxtLength[m[32m/*frac->m_maxTxtLength[m   = m_hasTxtData ?
		    std::max(frac->m_maxTxtLength, m_sequences[seqIdx].txtLength) : [m
		    frac->m_maxTxtLength;    [m
		    seqInfo.txtLength      = m_hasTxtData ? (m_sequences[seqIdx].txtLength) : [31m0;[m[32m0;*/[m
		[m
                frac->m_seqInfo.push_back(seqInfo);[m
            }[m
[36m@@ -516,25 +600,33 @@[m [mnamespace data_sets {[m
				PATTYPE_NONE);[m
	frac->m_fracTotalLength = 0;[m
	[m
	[32mif (m_exInputFlag)[m
[32m	    frac->m_exInputData.resize(frac->m_maxExInputLength * m_parallelSequences *[m
[32m				       frac->m_exInputDim, 0);[m
[32m	else[m
[32m	    frac->m_exInputData.clear();[m
	// Add 0620 Wang[m
	[31mif[m[32m/*if[m (m_hasTxtData)
	    frac->m_txtData.resize((frac->m_maxTxtLength * m_parallelSequences* [m
				    frac->m_txtPatternSize), 0);[m
	else[m
	[31mfrac->m_txtData.clear();[m[32mfrac->m_txtData.clear();*/[m
	[m
	if (m_auxDirPath.size()>0){[m
	    if (m_auxDataTyp == AUXDATATYPE_CHAR){[m
		frac->m_auxPattypeData.resize(frac->m_maxSeqLength * [31mm_auxDataDim);[m[32mm_auxDataDim *[m
[32m					      m_parallelSequences, 0);[m
		frac->m_auxRealData.clear();[m
		frac->m_auxIntData.clear();[m
	    }else if(m_auxDataTyp == AUXDATATYPE_INT){[m
		frac->m_auxPattypeData.clear();[m
		frac->m_auxRealData.clear();[m
		frac->m_auxIntData.resize(frac->m_maxSeqLength * [31mm_auxDataDim);[m[32mm_auxDataDim *[m
[32m					  m_parallelSequences, 0);[m
	    }else if(m_auxDataTyp == AUXDATATYPE_FLOAT){[m
		frac->m_auxPattypeData.clear();[m
		frac->m_auxRealData.resize(frac->m_maxSeqLength * [31mm_auxDataDim);[m[32mm_auxDataDim *[m
[32m					   m_parallelSequences, 0.0);[m
		frac->m_auxIntData.clear();[m
	    }[m
	    frac->m_auxDataDim = m_auxDataDim;[m
[36m@@ -548,39 +640,39 @@[m [mnamespace data_sets {[m
        if (m_isClassificationData)[m
            frac->m_targetClasses.resize(frac->m_maxSeqLength * m_parallelSequences, -1);[m
        else[m
            frac->m_outputs.resize((frac->m_maxSeqLength * m_parallelSequences *
				    m_outputPatternSize));[m
[m

        // load sequences from the cache file and create the fraction vectors[m
        for (int i = 0; i < m_parallelSequences; ++i) {[m
	    
            if (firstSeqIdx + i >= (int)m_sequences.size())[m
                continue;[m
[m
            const sequence_t &seq = m_sequences[firstSeqIdx + i];[m
[m
            // [32mload[m inputs [32mdata[m
            Cpu::real_vector inputs = _loadInputsFromCache(seq);[m
            _addNoise(&inputs);[m
	    [31mint[m[32m//int[m tmpInputPatternSize = (m_exInputFlag)?(m_exInputDim[0]):(m_inputPatternSize);
            for (int timestep = 0; timestep < seq.length; ++timestep) {[m
                int srcStart = [31mtmpInputPatternSize[m[32mm_inputPatternSize[m * timestep;
                int offset_out = 0;[m
                for (int offset_in = -context_left; offset_in <= context_right; ++offset_in) {[m
                    int srcStart = [31mtmpInputPatternSize[m[32mm_inputPatternSize[m * (timestep + offset_in);
                    // duplicate first time step if needed[m
                    if (srcStart < 0) [m
                        srcStart = 0;[m
                    // duplicate last time step if needed[m
                    else if (srcStart > [31mtmpInputPatternSize[m[32mm_inputPatternSize[m * (seq.length - 1))
                        srcStart = [31mtmpInputPatternSize[m[32mm_inputPatternSize[m * (seq.length - 1);
                    int tgtStart = frac->m_inputPatternSize * [m
			(timestep * m_parallelSequences + i) +[m
			offset_out * [31mtmpInputPatternSize;[m[32mm_inputPatternSize;[m
                    //std::cout << "copy from " << srcStart << " to " << tgtStart [m
		    // << " size " << m_inputPatternSize << std::endl;[m
                    thrust::copy_n(inputs.begin() + srcStart, [31mtmpInputPatternSize,[m[32mm_inputPatternSize,[m 
				   frac->m_inputs.begin() + tgtStart);[m
                    ++offset_out;[m
                }[m
[36m@@ -609,8 +701,7 @@[m [mnamespace data_sets {[m
                        int srcStart = m_outputPatternSize * (timestep - output_lag);[m
                        thrust::copy_n(outputs.begin() + srcStart, m_outputPatternSize, [m
				       frac->m_outputs.begin() + tgtStart);[m
                    [31m}[m
[31m                    else[m[32m}else[m {
                        for (int oi = 0; oi < m_outputPatternSize; ++oi) {[m
                            frac->m_outputs[tgtStart + oi] = 1.0f; [m
			    // default value (make configurable?)[m
[36m@@ -620,7 +711,7 @@[m [mnamespace data_sets {[m
            }[m
	    [m
	    // Add Wang 0620: Wang read in the txtData into fraction[m
	    [31mif[m[32m/*if[m (m_hasTxtData){
		Cpu::int_vector txtData = _loadTxtDataFromCache(seq);[m
		for (int txtstep = 0; txtstep < seq.txtLength; txtstep++){[m
		     int txtStart = m_txtDataPatternSize * (txtstep * m_parallelSequences + i);[m
[36m@@ -628,21 +719,63 @@[m [mnamespace data_sets {[m
		    thrust::copy_n(txtData.begin() + srcStart, m_txtDataPatternSize,[m
				   frac->m_txtData.begin() + txtStart);[m
		}[m
		[32m}*/[m
[32m	    if (m_exInputFlag){[m
[32m		Cpu::real_vector exInput = _loadExInputsFromCache(seq);[m
[32m		for (int timestep = 0; timestep < seq.exInputLength; ++timestep) {[m
[32m		    int tgtStart  = seq.exInputDim * (timestep * m_parallelSequences + i);[m
[32m		    int srcStart  = seq.exInputDim * timestep;[m
[32m		    thrust::copy_n(exInput.begin() + srcStart, seq.exInputDim, [m
[32m				   frac->m_exInputData.begin() + tgtStart);[m
[32m		}[m
	    }[m
[m
	    if (m_auxDirPath.size()>0){[m
		if (m_auxDataTyp == AUXDATATYPE_CHAR){[m
		    Cpu::pattype_vector auxData = _loadAuxPattypeDataFromCache(seq);[m
		    [31mthrust::copy(auxData.begin(),[m[32mfor (int timestep = 0; timestep < seq.length; ++timestep) {[m
[32m			int tgtStart  = m_auxDataDim * (timestep * m_parallelSequences + i);[m
[32m			if (timestep >= output_lag) {[m
[32m			    int srcStart = m_auxDataDim * (timestep - output_lag);[m
[32m			    thrust::copy_n(auxData.begin() + srcStart, m_auxDataDim, [m
[32m					   frac->m_auxPattypeData.begin() + tgtStart);[m
[32m			}else {[m
[32m			    for (int oi = 0; oi < m_auxDataDim; ++oi) [m
[32m				frac->m_auxPattypeData[tgtStart + oi] = 0;[m
[32m			}[m
[32m		    }[m
[32m		    //thrust::copy(auxData.begin(),[m auxData.end(), frac->m_auxPattypeData.begin());
		}else if(m_auxDataTyp == AUXDATATYPE_INT){[m
		    Cpu::int_vector auxData = _loadAuxIntDataFromCache(seq);[m
		    [31mthrust::copy(auxData.begin(),[m[32mfor (int timestep = 0; timestep < seq.length; ++timestep) {[m
[32m			int tgtStart  = m_auxDataDim * (timestep * m_parallelSequences + i);[m
[32m			if (timestep >= output_lag) {[m
[32m			    int srcStart = m_auxDataDim * (timestep - output_lag);[m
[32m			    thrust::copy_n(auxData.begin() + srcStart, m_auxDataDim, [m
[32m					   frac->m_auxIntData.begin() + tgtStart);[m
[32m			}else {[m
[32m			    for (int oi = 0; oi < m_auxDataDim; ++oi) [m
[32m				frac->m_auxIntData[tgtStart + oi] = 0;[m
[32m			}[m
[32m		    }[m
[32m		    //thrust::copy(auxData.begin(),[m auxData.end(), frac->m_auxIntData.begin());
		}else if(m_auxDataTyp == AUXDATATYPE_FLOAT){[m
		    Cpu::real_vector auxData = _loadAuxRealDataFromCache(seq);[m
		    [31mthrust::copy(auxData.begin(),[m[32mfor (int timestep = 0; timestep < seq.length; ++timestep) {[m
[32m			int tgtStart  = m_auxDataDim * (timestep * m_parallelSequences + i);[m
[32m			if (timestep >= output_lag) {[m
[32m			    int srcStart = m_auxDataDim * (timestep - output_lag);[m
[32m			    thrust::copy_n(auxData.begin() + srcStart, m_auxDataDim, [m
[32m					   frac->m_auxRealData.begin() + tgtStart);[m
[32m			}else {[m
[32m			    for (int oi = 0; oi < m_auxDataDim; ++oi) [m
[32m				frac->m_auxRealData[tgtStart + oi] = 1.0f;[m
[32m			}[m
[32m		    }[m
[32m		    //thrust::copy(auxData.begin(),[m auxData.end(), frac->m_auxRealData.begin());
		}[m
	    }	[m

            // pattern types[m
            for (int timestep = 0; timestep < seq.length; ++timestep) {[m
                Cpu::pattype_vector::value_type patType;[m
[36m@@ -656,14 +789,13 @@[m [mnamespace data_sets {[m
                frac->m_patTypes[timestep * m_parallelSequences + i] = patType;[m
		frac->m_fracTotalLength = frac->m_fracTotalLength+1;[m
            }[m
[31m	    [m
        }[m
[m
        [31m/* std::cout[m[32m/*std::cout[m << "inputs for data fraction: ";
        thrust::copy(frac->m_inputs.begin(), frac->m_inputs.end(), [m
	std::ostream_iterator<real_t>(std::cout, ";"));[m
        std::cout << std::endl; */[m
	
        return frac;[m
    }[m
[m
[36m@@ -691,9 +823,6 @@[m [mnamespace data_sets {[m
        , m_inputPatternSize (0)[m
        , m_outputPatternSize(0)[m
        , m_curFirstSeqIdx   (-1)[m
[31m	, m_totalTxtLength   (0)[m
[31m	, m_maxTxtLength     (0)[m
[31m	, m_txtDataPatternSize(0)[m
	, m_exInputFlag      (false)[m
	, m_auxDirPath       ("")[m
    {[m
[36m@@ -713,8 +842,6 @@[m [mnamespace data_sets {[m
        , m_minSeqLength     (std::numeric_limits<int>::max())[m
        , m_maxSeqLength     (std::numeric_limits<int>::min())[m
        , m_curFirstSeqIdx   (-1)[m
[31m	, m_totalTxtLength   (0)[m
[31m	, m_maxTxtLength     (0)[m
    {[m
        int ret;[m
        int ncid;[m
[36m@@ -722,8 +849,7 @@[m [mnamespace data_sets {[m
        if (fraction <= 0 || fraction > 1)[m
            throw std::runtime_error("Invalid fraction");[m
[m
	[31m//[m[32m/* ---[m Preparation [32m--- */[m
	// Add 1111: Prepare the auxillary data [m
	static Configuration config = Configuration::instance();[m
	m_auxDirPath         = config.auxillaryDataDir();[m
[36m@@ -732,16 +858,41 @@[m [mnamespace data_sets {[m
	m_auxDataTyp         = config.auxillaryDataTyp();[m
	[m
	// Add 170327: Prepare the external input data[m
	if [31m(config.exInputDir().size()){[m
[31m	    ParseStrOpt(config.exInputDir(), m_exInputDir);[m
[31m	    ParseStrOpt(config.exInputExt(), m_exInputExt);[m
[31m	    ParseIntOpt(config.exInputDim(), m_exInputDim);[m[32m(config.exInputDir().size() || config.exInputDirs().size()){[m
[32m	    if (config.exInputDim() > 0){[m
[32m		// only single external input file[m
[32m		m_exInputDir  = config.exInputDir();[m
[32m		m_exInputExt  = config.exInputExt();[m
[32m		m_exInputDim  = config.exInputDim();[m
[32m		m_exInputFlag = true;[m
[32m		m_exInputType = DATASET_EXINPUT_TYPE_1;[m
[32m		m_exInputDirs.clear();[m
[32m		m_exInputExts.clear();[m
[32m		m_exInputDims.clear();[m
[32m	    }else if (config.exInputDims().size() > 0){[m
[32m		ParseStrOpt(config.exInputDirs(), m_exInputDirs, ",");[m
[32m		ParseStrOpt(config.exInputExts(), m_exInputExts, ",");[m
[32m		ParseIntOpt(config.exInputDims(), m_exInputDims);[m
[32m		if (m_exInputDirs.size() != m_exInputExts.size() ||[m
[32m		    m_exInputDirs.size() != m_exInputDims.size())[m
[32m		    throw std::runtime_error("ExtInput options unequal length");[m
		m_exInputFlag = true;
		[32mm_exInputType = DATASET_EXINPUT_TYPE_1;[m
[32m		m_exInputDir  = "";[m
[32m		m_exInputExt  = "";[m
[32m		m_exInputDim  = 0;[m
[32m	    }else{[m
[32m		throw std::runtime_error("ExtInputDim(s) is not configured");[m
[32m	    }[m
	}else{[m
	    [31mm_exInputDir.clear();[m
[31m	    m_exInputExt.clear();[m
[31m	    m_exInputDim.clear();[m[32mm_exInputDir  = "";[m
[32m	    m_exInputExt  = "";[m
[32m	    m_exInputDim  = 0;[m
	    m_exInputFlag = false;[m
	    [32mm_exInputType = DATASET_EXINPUT_TYPE_0;[m
[32m	    m_exInputDirs.clear();[m
[32m	    m_exInputExts.clear();[m
[32m	    m_exInputDims.clear();[m
	}[m
	[m
        // Preparation: cache data[m
[36m@@ -760,6 +911,8 @@[m [mnamespace data_sets {[m
            throw std::runtime_error(std::string("Cannot open temporary file '") + [m
				     tmpFileName + "'");[m
[m
	[32m/* --- Read in the data --- */[m
	
	// Read *.nc files[m
        bool first_file = true;[m
        for (std::vector<std::string>::const_iterator nc_itr = ncfiles.begin();[m
[36m@@ -771,6 +924,8 @@[m [mnamespace data_sets {[m
					 *nc_itr + "': " + nc_strerror(ret));[m
            try {[m
                int maxSeqTagLength = internal::readNcDimension(ncid, "maxSeqTagLength");[m

		[32m// Check input and output size[m
                if (first_file) {[m
                    m_isClassificationData = internal::hasNcDimension (ncid, "numLabels");[m
                    m_inputPatternSize     = internal::readNcDimension(ncid, "inputPattSize");[m
[36m@@ -778,13 +933,11 @@[m [mnamespace data_sets {[m
                    if (m_isClassificationData) {[m
                        int numLabels       = internal::readNcDimension(ncid, "numLabels");[m
                        m_outputPatternSize = (numLabels == 2 ? 1 : numLabels);[m
                    [31m}[m
[31m                    else {[m[32m}else{[m
                        m_outputPatternSize = internal::readNcDimension(ncid, "targetPattSize");[m
                    }[m
[31m[m
		    // Add Wang 0620: check the txt data[m
		    [31mm_hasTxtData[m[32m/*m_hasTxtData[m            = internal::hasNcDimension (ncid, "txtLength");
		    if (m_hasTxtData){[m
			m_txtDataPatternSize = internal::readNcDimension(ncid, "txtPattSize");[m
			if (m_txtDataPatternSize != 1)[m
[36m@@ -792,85 +945,98 @@[m [mnamespace data_sets {[m
			if (truncSeqLength > 0){[m
			    printf("WARNING:truncSeqLength will be set to -1 in LstmCharW mode");[m
			    truncSeqLength = -1;[m
			[31m}[m
[31m		    }[m[32m}}*/[m
                }else{[m
		    if (m_isClassificationData) {
                        if (!internal::hasNcDimension(ncid, "numLabels")) [m
                            throw std::runtime_error("Cannot classification with regression NC");[m
                        int numLabels = internal::readNcDimension(ncid, "numLabels");[m
                        if (m_outputPatternSize != (numLabels == 2 ? 1 : numLabels))[m
                            throw std::runtime_error("Number of classes mismatch in NC files");[m
                    [31m}[m
[31m                    else {[m[32m}else{[m
                        if (m_outputPatternSize!= internal::readNcDimension(ncid,"targetPattSize"))[m
                            throw std::runtime_error("Number of targets mismatch in NC files");[m
                    }[m
                    if (m_inputPatternSize != internal::readNcDimension(ncid, "inputPattSize"))[m
                        throw std::runtime_error("Number of inputs mismatch in NC files");[m
		    
		    // Add Wang 0620: check the txt data[m
		    [31mif[m[32m/*if[m (m_hasTxtData &&
			m_txtDataPatternSize != internal::readNcDimension(ncid, "txtPattSize")){[m
			throw std::runtime_error("txtPattSize mismatch");[m
			[31m}[m[32m}*/[m
                }[m
[m
[31m[m
		// Read in sequence macro information[m
                int nSeq = internal::readNcDimension(ncid, "numSeqs");[m
                nSeq     = (int)((real_t)nSeq * fraction);[m
                nSeq     = std::max(nSeq, 1);[m
                int inputsBegin  = 0;[m
                int targetsBegin = 0;[m
		[32m//[m int txtDataBegin = 0;
                for (int i = 0; i < nSeq; ++i) {[m
                    int seqLength      = internal::readNcIntArray(ncid, "seqLengths", i);[m
                    m_totalTimesteps  += seqLength;[m
		    [m
		    // Add Wang 0620: check the txt data[m
		    [31mint[m[32m/*int[m txtLength      = (m_hasTxtData ?
					 (internal::readNcIntArray(ncid,"txtLengths",i)) : 0);[m
					 m_totalTxtLength  += [31mtxtLength;[m[32mtxtLength;*/[m
		    
                    std::string seqTag = internal::readNcStringArray(ncid, "seqTags", i, [m
								     maxSeqTagLength);[m
                    int k = 0;[m
		    int rePosInUtt = 0;[m
                    while (seqLength > 0) {[m
                        sequence_t seq;[m
			// Fill in the information for seq
                        seq.originalSeqIdx = k;[m
                        if (truncSeqLength > 0 && seqLength > 1.5 * truncSeqLength) [m
                            seq.length         = std::min(truncSeqLength, seqLength);[m
                        else[m
                            seq.length = seqLength;[m
                        seq.seqTag         = seqTag;
			[31mseq.txtLength[m[32m//seq.txtLength[m      = txtLength;[31m// If this utterance is cut into several pieces, beginInUtt[m
[31m			// logs the relative position of this piece in the utterance[m
			seq.beginInUtt     = rePosInUtt; 
                        sequences.push_back(seq);[m
                        seqLength         -= seq.length;[m
			rePosInUtt        += seq.length;[m
                        ++k;[m
			[32m// Note: if this utterance is cut into several pieces, beginInUtt[m
[32m			// logs the relative position of this piece in the utterance[m
                    }[m
                }[m
[m
		// Read in sequence data[m
                for (std::vector<sequence_t>::iterator seq = sequences.begin(); [m
		     seq != sequences.end(); ++seq) {
		    
                    m_minSeqLength = std::min(m_minSeqLength, seq->length);[m
                    m_maxSeqLength = std::max(m_maxSeqLength, seq->length);
		    [31mm_maxTxtLength[m[32m//m_maxTxtLength[m = std::max(m_maxTxtLength, [31mseq->txtLength); 	//[m[32mseq->txtLength);//[m Add 0620
[m
                    // read input patterns and store them in the cache file[m
                    seq->inputsBegin = m_cacheFile.tellp();[m
                    Cpu::real_vector inputs =
			[31minternal::readNcPatternArray([m
[31m							ncid,[m[32minternal::readNcPatternArray(ncid,[m "inputs", inputsBegin, seq->length,
						     m_inputPatternSize);
		    [32mif (m_exInputType == DATASET_EXINPUT_TYPE_1){[m
[32m			// When the input index is in increasing order[m
[32m			// exInputStartPos and EndPos are used to load data from external files[m
[32m			seq->exInputStartPos = inputs[0];   [m
[32m			seq->exInputEndPos   = inputs[seq->length-1] + 1;[m
[32m[m
[32m			// index is used to load the data in neural network[m
[32m			// thus, the index should be shifted and starts from 0[m
[32m			// Shift the index[m
[32m			Cpu::real_vector tempVec(inputs.size(), inputs[0]);[m
[32m			thrust::transform(inputs.begin(), inputs.end(), tempVec.begin(), [m
[32m					  inputs.begin(), thrust::minus<float>());[m
[32m		    }else{[m
[32m			seq->exInputStartPos = -1;[m
[32m			seq->exInputEndPos   = -1;[m
[32m		    }[m

                    m_cacheFile.write((const char*)inputs.data(), sizeof(real_t) * inputs.size());[m
                    assert (m_cacheFile.tellp() - seq->inputsBegin == [m
			    seq->length * m_inputPatternSize * sizeof(real_t));[m
[36m@@ -894,7 +1060,7 @@[m [mnamespace data_sets {[m
                    }[m
		    [m
		    // Add 0620 Wang: read and store the txt data[m
		    [31mif[m[32m/*if[m (m_hasTxtData){
			seq->txtDataBegin       = m_cacheFile.tellp();[m
			Cpu::int_vector txtData = [m
			    internal::readNcPatternArrayInt(ncid, "txtData", txtDataBegin, [m
[36m@@ -907,7 +1073,7 @@[m [mnamespace data_sets {[m
				seq->txtLength * m_txtDataPatternSize * sizeof(int));[m
		    }else{[m
			seq->txtDataBegin = 0;[m
		    [31m}[m[32m}*/[m
		    [m
		    // Add 1111: to read auxillary data from external binary data files[m
		    if (m_auxDirPath.size()>0){[m
[36m@@ -941,7 +1107,7 @@[m [mnamespace data_sets {[m
			    assert(m_cacheFile.tellp()-seq->auxDataBegin==dataSize*sizeof(int));[m
			}else if (m_auxDataTyp = AUXDATATYPE_FLOAT){[m
			    Cpu::real_vector temp;[m
			    int tempLength = internal::readRealData(fileName, [31mtemp);[m[32mtemp, 0, -1);[m
			    if (tempLength < (dataShift + dataSize)){[m
				printf("To short, auxillary data %s", fileName.c_str());[m
				throw std::runtime_error("Please check auxDataOption and data");[m
[36m@@ -954,33 +1120,78 @@[m [mnamespace data_sets {[m
			}[m
		    }else{[m
			seq->auxDataBegin = 0;[m
			[32mseq->auxDataDim   = 0;[m
[32m			seq->auxDataTyp   = 0;[m
		    }[m
[m
		    // Add 170327: to read external input data[m
		    if [31m(m_exInputFlag && inputs.size()>0){[m[32m(m_exInputFlag){[m
[32m			if (config.exInputDim() > 0){[m
			    // [31mCurrenntly only[m[32mOnly[m read one file [31massert(inputs.size() == seq->length);[m[32mamong external input files[m
			    seq->exInputBegin    = m_cacheFile.tellp();
			    seq->exInputDim      = [31mm_exInputDim[0];[m[32mm_exInputDim;[m
			[m
			    std::string fileName = [31mm_exInputDir[0]+"/"+seq->seqTag+m_exInputExt[0];[m[32mm_exInputDir+"/"+seq->seqTag+m_exInputExt;[m 
			    Cpu::real_vector temp;
			    int [32mstPos, etPos;[m
[32m			    if (m_exInputType == DATASET_EXINPUT_TYPE_1){[m
[32m				stPos = seq->exInputStartPos * seq->exInputDim;[m
[32m				etPos = seq->exInputEndPos   * seq->exInputDim;[m
[32m			    }else{[m
[32m				stPos = 0; etPos = -1;[m
[32m			    }[m
[32m			    int[m tempLength = internal::readRealData(fileName, [31mtemp);[m[32mtemp, stPos, etPos);[m
[32m			    seq->exInputLength   = tempLength / seq->exInputDim;[m
[32m			    assert(seq->exInputLength * seq->exInputDim == tempLength);[m
[32m			[m
[32m			    m_cacheFile.write((const char *)(temp.data()),[m
[32m					      sizeof(real_t) * tempLength);[m
[32m			    assert((m_cacheFile.tellp()-seq->exInputBegin)==[m
[32m				   (seq->exInputDim * seq->exInputLength * sizeof(real_t)));[m
[32m			}else if (config.exInputDims().size() > 0){[m
			    // [31mread the data for every frame[m[32mload multiple files[m
[32m			    seq->exInputDim    = SumCpuIntVec(m_exInputDims);[m
[32m			    seq->exInputLength = seq->exInputEndPos - seq->exInputStartPos;[m
[32m			    Cpu::real_vector exDataBuf(seq->exInputDim *[m
[32m						       (seq->exInputEndPos - seq->exInputStartPos),[m
[32m						       0.0);[m
[32m			    int cnt = 0;[m
			    int [31mdataShift[m[32mdimCnt[m = 0;
			    for (int [31midx[m[32mi[m = 0; [31midx[m[32mi[m < [31mseq->length; idx++){[m
[31m			    dataShift[m[32mm_exInputDirs.size(); i++){[m
[32m				std::string fileName[m = [31minputs[idx] * seq->exInputDim;[m
[31m			    assert((dataShfit[m[32m(m_exInputDirs[i] + "/"[m + [31mseq->exInputDim) <= temp.size());[m[32mseq->seqTag +[m
[32m							m_exInputExts[i]); [m
[32m[m
[32m				int stPos, etPos;[m
[32m				if (m_exInputType == DATASET_EXINPUT_TYPE_1){[m
[32m				    stPos = seq->exInputStartPos * m_exInputDims[i];[m
[32m				    etPos = seq->exInputEndPos   * m_exInputDims[i];[m
[32m				}else{[m
[32m				    stPos = 0; etPos = -1;[m
[32m				}[m
[32m				cnt += internal::readRealDataAndFill([m
[32m					fileName, exDataBuf, stPos, etPos,[m
[32m					seq->exInputDim, m_exInputDims[i], dimCnt);[m
[32m				dimCnt += m_exInputDims[i];[m
[32m				[m
[32m			    }[m
[32m			    assert(seq->exInputLength * seq->exInputDim == cnt);[m
[32m			    seq->exInputBegin    = m_cacheFile.tellp();[m
			    m_cacheFile.write((const char [31m*)(temp.data() + dataShift),[m[32m*)(exDataBuf.data()),[m
					      sizeof(real_t) * [31mseq->exInputDim);[m[32mcnt);[m
[32m			    assert((m_cacheFile.tellp()-seq->exInputBegin)==[m
[32m				   (seq->exInputDim * seq->exInputLength * sizeof(real_t)));[m
[32m			}else{[m
[32m			    throw std::runtime_error("Impossible bug");[m
			}[m
		    [31massert((m_cacheFile.tellp()-seq->exInputBegin)==[m
[31m			       (seq->exInputDim*seq->length*sizeof(real_t)));[m[32m}else{[m
[32m			seq->exInputBegin  = 0;[m
[32m			seq->exInputLength = 0;[m
[32m			seq->exInputDim    = 0;[m
		    }[m
		    [m
                    inputsBegin  += seq->length; [32m// position in data.nc[m
                    targetsBegin += seq->length; [31mtxtDataBegin[m[32m// position in data.nc[m
[32m		    //txtDataBegin[m += seq->txtLength;
                }[m
[m
                if (first_file) {[m
[36m@@ -1109,15 +1320,15 @@[m [mnamespace data_sets {[m
[m
    int DataSet::maxTxtLength() const[m
    {[m
	return [31mm_maxTxtLength;[m[32m0;//m_maxTxtLength;[m
    }[m
[m
    int DataSet::inputPatternSize() const[m
    {[m
	[31mif[m[32m//if[m (m_exInputFlag)
	[32m//[m    return m_exInputDim[0];
	[31melse[m[32m//else[m
	return m_inputPatternSize;
    }[m
[m
    int DataSet::outputPatternSize() const[m
[1mdiff --git a/data_sets/DataSet.hpp b/data_sets/DataSet.hpp[m
[1mindex f3c463f..cef0f8e 100644[m
[1m--- a/data_sets/DataSet.hpp[m
[1m+++ b/data_sets/DataSet.hpp[m
[36m@@ -56,8 +56,8 @@[m [mnamespace data_sets {[m
            std::streampos targetsBegin;[m
	    [m
	    // Add 0620, Wang: support to the txt int data[m
	    [31mint[m[32m//int[m            txtLength;        // length of the txt data for this sequence
	    [31mstd::streampos[m[32m//std::streampos[m txtDataBegin;     //  
[m
	    // Add 1111, support to the auxillary data[m
	    int            auxDataDim;[m
[36m@@ -67,8 +67,11 @@[m [mnamespace data_sets {[m
	    int            beginInUtt;       // the relative position of the start of the seq[m
	                                     // in the utterance[m
	    // Add 170327, support to the external input data[m
	    int            exInputDim;       [32m// [m
[32m	    int            exInputLength;    //[m
[32m	    int            exInputStartPos;  //[m
[32m	    int            exInputEndPos;    //[m 
	    std::streampos exInputBegin;     [32m//[m
        };[m
[m
    private:[m
[36m@@ -78,6 +81,7 @@[m [mnamespace data_sets {[m
        void _addNoise(Cpu::real_vector *v);[m
        Cpu::real_vector    _loadInputsFromCache(const sequence_t &seq);[m
        Cpu::real_vector    _loadOutputsFromCache(const sequence_t &seq);[m
	[32mCpu::real_vector    _loadExInputsFromCache(const sequence_t &seq);[m
        Cpu::int_vector     _loadTargetClassesFromCache(const sequence_t &seq);[m
        boost::shared_ptr<DataSetFraction> _makeFractionTask(int firstSeqIdx);[m
        boost::shared_ptr<DataSetFraction> _makeFirstFractionTask();[m
[36m@@ -116,10 +120,10 @@[m [mnamespace data_sets {[m
	[m
	// Add 0620: Wang support to the txt input data[m
	// (Support for the txt data should be merged with the auxillary data)[m
	[31mint[m[32m//int[m    m_maxTxtLength;                 // the maximum length of txt over corpus
	[31mint[m[32m//int[m    m_txtDataPatternSize;           // dimension of the txt data 
	[31mint[m[32m//int[m    m_totalTxtLength;               // the total length of txt data for this fraction
	[31mbool[m[32m//bool[m   m_hasTxtData;                   // whether contains the txt data?
	[m
	// Add 1111: Support to the auxillary data (external data not in .nc format)[m
	std::string m_auxDirPath;              // path to the directory where auxillary data exist[m
[36m@@ -128,11 +132,17 @@[m [mnamespace data_sets {[m
	int         m_auxDataDim;              // dimension of the auxillary data[m
[m
	// Add 170327: external input file[m
	[31mstd::vector<std::string>[m[32mstd::string[m m_exInputDir;
	[31mstd::vector<std::string>[m[32mstd::string[m m_exInputExt;
	[31mCpu::int_vector[m[32mint[m         m_exInputDim;
	[32mint         m_exInputType;             // reserved[m
	bool        m_exInputFlag;
[m
	[32mstd::vector<std::string> m_exInputDirs;[m
[32m	std::vector<std::string> m_exInputExts;[m
[32m	Cpu::int_vector          m_exInputDims;[m

	
    public:[m
        /**[m
         * Creates an empty data set[m
[1mdiff --git a/data_sets/DataSetFraction.cpp b/data_sets/DataSetFraction.cpp[m
[1mindex 8b1c62b..c7ec00f 100644[m
[1m--- a/data_sets/DataSetFraction.cpp[m
[1m+++ b/data_sets/DataSetFraction.cpp[m
[36m@@ -43,6 +43,11 @@[m [mnamespace data_sets {[m
        return m_outputPatternSize;[m
    }[m
[m
    [32mint DataSetFraction::externalInputSize() const[m
[32m    {[m
[32m        return m_exInputDim;[m
[32m    }[m

    int DataSetFraction::maxSeqLength() const[m
    {[m
        return m_maxSeqLength;[m
[36m@@ -53,6 +58,16 @@[m [mnamespace data_sets {[m
        return m_minSeqLength;[m
    }[m
[m
    [32mint DataSetFraction::maxExInputLength() const[m
[32m    {[m
[32m        return m_maxExInputLength;[m
[32m    }[m
[32m[m
[32m    int DataSetFraction::minExInputLength() const[m
[32m    {[m
[32m        return m_minExInputLength;[m
[32m    }[m
    
    int DataSetFraction::numSequences() const[m
    {[m
        return (int)m_seqInfo.size();[m
[36m@@ -103,15 +118,20 @@[m [mnamespace data_sets {[m
	return m_auxDataDim;[m
    }[m
[m
    [32mconst Cpu::real_vector& DataSetFraction::exInputData()    const[m
[32m    {[m
[32m	return m_exInputData;[m
[32m    }[m
[32m    [m
[32m    /*[m
    const Cpu::int_vector& DataSetFraction::txtData() const[m
    {[m
        return m_txtData;[m
    }[m
[31m[m
    int DataSetFraction::maxTxtLength() const[m
    {[m
        return m_maxTxtLength;[m
	[31m}[m[32m}*/[m
  [m
    int DataSetFraction::fracTimeLength() const[m
    {[m
[1mdiff --git a/data_sets/DataSetFraction.hpp b/data_sets/DataSetFraction.hpp[m
[1mindex 55a04d9..b70f5b0 100644[m
[1m--- a/data_sets/DataSetFraction.hpp[m
[1m+++ b/data_sets/DataSetFraction.hpp[m
[36m@@ -41,10 +41,12 @@[m [mnamespace data_sets {[m
[m
    public:[m
        struct seq_info_t {[m
            int         originalSeqIdx;  [32m//[m
            int         length;          [32m//[m
[32m	    int         exInputLength;   //[m
            std::string seqTag;          [31mint[m[32m//[m
[32m	    [m
[32m	    //int[m         txtLength;
        };[m
[m
    private:[m
[36m@@ -52,7 +54,7 @@[m [mnamespace data_sets {[m
        int m_outputPatternSize;[m
        int m_maxSeqLength;[m
        int m_minSeqLength;[m
	
        std::vector<seq_info_t> m_seqInfo;[m
[m
        Cpu::real_vector    m_inputs;[m
[36m@@ -61,18 +63,25 @@[m [mnamespace data_sets {[m
        Cpu::int_vector     m_targetClasses;[m
	[m
	// Add 0620 Wang: [m
	[31mint[m[32m//int[m m_txtPatternSize;
	[31mint[m[32m//int[m m_maxTxtLength;                 // the maximum length of txt of this faction
	[31mCpu::int_vector[m[32m//Cpu::int_vector[m   m_txtData;       // the txt input data of this fraction
	[m
	// Add 1024 [m
	int m_fracTotalLength;[m
[m
	// Add 1111[m
	int                 m_auxDataDim;
	Cpu::pattype_vector m_auxPattypeData;[m
	Cpu::real_vector    m_auxRealData;[m
	Cpu::int_vector     m_auxIntData;[m

	[32m// Add 0815[m
[32m	int                 m_exInputDim;[m
[32m	int                 m_maxExInputLength;[m
[32m	int                 m_minExInputLength;[m
[32m	Cpu::real_vector    m_exInputData;[m

	[m
    private:[m
        /**[m
[36m@@ -101,6 +110,11 @@[m [mnamespace data_sets {[m
        int outputPatternSize() const;[m
[m
        /**[m
         [32m* Returns the size of external input data[m
[32m         */[m
[32m        int externalInputSize() const;[m
[32m       	[m
[32m        /**[m
         * Returns the length of the longest sequence[m
         *[m
         * @return The length of the longest sequence[m
[36m@@ -114,6 +128,13 @@[m [mnamespace data_sets {[m
         */[m
        int minSeqLength() const;[m
[m
	[32m/**[m
[32m	 * Return the length of the external input data[m
[32m	 */[m
[32m	int maxExInputLength() const;[m
[32m	[m
[32m	int minExInputLength() const;[m
	
        /**[m
         * Returns the number of sequences in the fraction[m
         *[m
[36m@@ -149,12 +170,18 @@[m [mnamespace data_sets {[m
         */[m
        const Cpu::real_vector& outputs() const;[m
[m
	[31mconst Cpu::real_vector&    auxRealData() const;[m
[31m	const Cpu::pattype_vector& auxPattypeData() const;[m
[31m	const Cpu::int_vector&     auxIntData() const;[m[32m/**[m
[32m	 * Returns the auxilary data[m
[32m	 */[m
	const int& auxDataDim() const;[m
	[32mconst Cpu::real_vector&    auxRealData()    const;[m
[32m	const Cpu::pattype_vector& auxPattypeData() const;[m
[32m	const Cpu::int_vector&     auxIntData()     const;[m
[m
	[32m/**[m
[32m	 * Return the external Input data[m
[32m	 */[m
[32m	const Cpu::real_vector&    exInputData()    const;[m
        /**[m
         * Returns the target classes vector[m
         *[m
[36m@@ -168,9 +195,8 @@[m [mnamespace data_sets {[m
         *[m
         * @return the txt data[m
         */[m
	[31mconst[m[32m//const[m Cpu::int_vector& txtData() const;
	[31mint[m[32m//int[m maxTxtLength() const;
[m
	/*[m
	 * Return the number of valid frames for this current fraction[m
[1mdiff --git a/diff.log b/diff.log[m
[1mindex 2906125..f502a61 100644[m
[1m--- a/diff.log[m
[1m+++ b/diff.log[m
[36m@@ -1,676 +0,0 @@[m
[31mdiff --git a/LayerFactory.cu b/LayerFactory.cu[m
[31mindex 7ee2f55..d128e6c 100644[m
[31m--- a/LayerFactory.cu[m
[31m+++ b/LayerFactory.cu[m
[31m@@ -35,6 +35,7 @@[m
[31m #include "layers/BinaryClassificationLayer.hpp"[m
[31m #include "layers/MulticlassClassificationLayer.hpp"[m
[31m #include "layers/Amalgamate.hpp"[m
[31m+#include "layers/BatchNorm.hpp"[m
[31m #include "activation_functions/Tanh.cuh"[m
[31m #include "activation_functions/Logistic.cuh"[m
[31m #include "activation_functions/Identity.cuh"[m
[31m@@ -91,6 +92,8 @@ layers::Layer<TDevice>* LayerFactory<TDevice>::createLayer([m
[31m     	return new FeedBackLayer<TDevice>(layerChild, weightsSection, *precedingLayer);[m
[31m     else if (layerType == "amalgamate")[m
[31m     	return new AmalgamateLayer<TDevice>(layerChild, weightsSection, *precedingLayer);[m
[31m+    else if (layerType == "batchnorm")[m
[31m+    	return new BatchNormLayer<TDevice>(layerChild, weightsSection, *precedingLayer);[m
[31m     [m
[31m     /*[m
[31m     // not implemented yet[m
[31mdiff --git a/layers/FeedForwardLayer.cu b/layers/FeedForwardLayer.cu[m
[31mindex a28af7e..fa2b595 100644[m
[31m--- a/layers/FeedForwardLayer.cu[m
[31m+++ b/layers/FeedForwardLayer.cu[m
[31m@@ -32,6 +32,9 @@[m
[31m #include "../activation_functions/Identity.cuh"[m
[31m #include "../activation_functions/Relu.cuh"[m
[31m [m
[31m+#include "../helpers/JsonClasses.hpp"[m
[31m+#include "../Configuration.hpp"[m
[31m+[m
[31m #include <thrust/transform.h>[m
[31m #include <thrust/transform_reduce.h>[m
[31m #include <thrust/for_each.h>[m
[31m@@ -113,20 +116,219 @@ namespace {[m
[31m         }[m
[31m 	};*/[m
[31m [m
[31m+    // [m
[31m+    struct BatchSize[m
[31m+    {[m
[31m+	// over time t * parallel sentence[m
[31m+	const char *patTypes;[m
[31m+	[m
[31m+	__host__ __device__ real_t operator() (const thrust::tuple<const real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int timeIdx = t.get<1>();[m
[31m+	    if (patTypes[timeIdx] == PATTYPE_NONE)[m
[31m+		return 0.0;// skip dummy node[m
[31m+	    else[m
[31m+		return 1.0;[m
[31m+	}[m
[31m+    };[m
[31m+    [m
[31m+    struct PrepareForMeanStd[m
[31m+    {[m
[31m+	int layerSize;[m
[31m+	bool   meanNotVar;[m
[31m [m
[31m+	[m
[31m+	const char *patTypes;   [m
[31m+	real_t     *data;[m
[31m+	real_t     *outdata;[m
[31m+	real_t     *mean;[m
[31m+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int dataIdx = t.get<1>();[m
[31m+	    int timeIdx = dataIdx / layerSize;[m
[31m+	    int dimIdx  = dataIdx % layerSize;[m
[31m+	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[31m+		// skip dummy node[m
[31m+		outdata[dataIdx] = 0.0; //[m
[31m+	    }else{[m
[31m+		if (meanNotVar)[m
[31m+		    outdata[dataIdx] = data[dataIdx]; //[m
[31m+		else[m
[31m+		    outdata[dataIdx] = (data[dataIdx]-mean[dimIdx]) * (data[dataIdx]-mean[dimIdx]);[m
[31m+	    }[m
[31m+	}[m
[31m+    };[m
[31m+    struct PrepareGrad[m
[31m+    {[m
[31m+	int    layerSize;[m
[31m+	bool   alphaNotBeta;[m
[31m+	const char *patTypes;   [m
[31m+	real_t     *grad;[m
[31m+	real_t     *data;[m
[31m+	[m
[31m+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int dataIdx = t.get<1>();[m
[31m+	    int timeIdx = dataIdx / layerSize;[m
[31m+	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[31m+		t.get<0>() = 0.0; // skip dummy node[m
[31m+	    }else{[m
[31m+		if (alphaNotBeta)[m
[31m+		    t.get<0>() = grad[dataIdx] * data[dataIdx];[m
[31m+		else[m
[31m+		    t.get<0>() = grad[dataIdx];[m
[31m+	    }[m
[31m+	}[m
[31m+    };[m
[31m+[m
[31m+    struct GetStd[m
[31m+    {[m
[31m+	real_t  stdConst;[m
[31m+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int dimIdx = t.get<1>();[m
[31m+	    t.get<0>() = sqrt(t.get<0>() +stdConst);[m
[31m+	}[m
[31m+    };[m
[31m+[m
[31m+    struct AveMeanStd[m
[31m+    {[m
[31m+	real_t *meanStdBuf;[m
[31m+	real_t  cnt;[m
[31m+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int dimIdx = t.get<1>();[m
[31m+	    meanStdBuf[dimIdx] += (t.get<0>() - meanStdBuf[dimIdx]) / cnt;[m
[31m+	}[m
[31m+    };[m
[31m+    [m
[31m+    [m
[31m+[m
[31m+    template <typename TActFn>[m
[31m+    struct ComputeBatchNorm_Transform[m
[31m+    {[m
[31m+	int layerSize;[m
[31m+[m
[31m+	const char *patTypes;   [m
[31m+	real_t *data;[m
[31m+	real_t *outdata;[m
[31m+	real_t *meanStd;[m
[31m+	real_t *meanStdBuf;[m
[31m+	real_t *scale;[m
[31m+	bool    trainFlag;[m
[31m+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int dataIdx = t.get<1>();[m
[31m+	    int dimIdx  = dataIdx % layerSize;[m
[31m+	    int timeIdx = dataIdx / layerSize;[m
[31m+	    int varIdx  = dimIdx  + layerSize;[m
[31m+	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[31m+		// skip dummy node[m
[31m+	    }else{[m
[31m+		// \hat{x} = (x - \mu) / \sigma[m
[31m+		if (trainFlag)[m
[31m+		    data[dataIdx] = (data[dataIdx]-meanStd[dimIdx])/meanStd[varIdx];[m
[31m+		else[m
[31m+		    data[dataIdx] = (data[dataIdx]-meanStdBuf[dimIdx])/meanStdBuf[varIdx];[m
[31m+[m
[31m+		// y =f(\alpha \hat{x} + \beta)[m
[31m+		outdata[dataIdx]   = TActFn::fn(data[dataIdx] * scale[dimIdx] + scale[varIdx]);[m
[31m+	    }[m
[31m+	}[m
[31m+    };[m
[31m+    [m
[31m+    struct ComputeBatchGradient_output[m
[31m+    {[m
[31m+	[m
[31m+	int layerSize;[m
[31m+[m
[31m+	const char *patTypes;   [m
[31m+	real_t *errors;[m
[31m+	real_t *outNormed;[m
[31m+	real_t *meanStd;[m
[31m+	real_t *scale;[m
[31m+	real_t *scaleGrad;	[m
[31m+	real_t  batchSize;[m
[31m+	[m
[31m+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m+	{[m
[31m+	    int dataIdx      = t.get<1>();[m
[31m+	    int dimIdx       = dataIdx % layerSize;[m
[31m+	    int timeIdx      = dataIdx / layerSize;[m
[31m+	    [m
[31m+	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[31m+		errors[dataIdx] = 0.0;[m
[31m+	    }else{[m
[31m+		// gradient =[m
[31m+		// alpha / std * (\deltaE/\delta{y} - \deltaE/\deltaBeta / batchSize -[m
[31m+		//                \deltaE/\deltaAlpha * dataNormed / batchSize)[m
[31m+		errors[dataIdx] = ((errors[dataIdx] -[m
[31m+				    scaleGrad[dimIdx] * outNormed[dataIdx]/ batchSize -[m
[31m+				    scaleGrad[dimIdx + layerSize] / batchSize ) *[m
[31m+				   scale[dimIdx] / meanStd[dimIdx + layerSize]);[m
[31m+	    }[m
[31m+	}[m
[31m+    };[m
[31m+    [m
[31m } // anonymous namespace[m
[31m } // namespace internal[m
[31m [m
[31m [m
[31m namespace layers {[m
[31m [m
[31m+    // Additional weight due to batch normalization[m
[31m+    int wNum(const helpers::JsonValue &layerChild){[m
[31m+	if (layerChild->HasMember("batchnorm") && ((*layerChild)["batchnorm"].GetInt()))[m
[31m+	    return 3; // alpha, mean, std[m
[31m+	else[m
[31m+	    return 0;[m
[31m+    }[m
[31m+    [m
[31m     template <typename TDevice, typename TActFn>[m
[31m     FeedForwardLayer<TDevice, TActFn>::FeedForwardLayer(const helpers::JsonValue &layerChild, [m
[31m 							const helpers::JsonValue &weightsSection, [m
[31m 							Layer<TDevice> &precedingLayer)[m
[31m-        : TrainableLayer<TDevice>(layerChild, weightsSection, 1, 0, precedingLayer)[m
[31m+        : TrainableLayer<TDevice>(layerChild, weightsSection, 1, wNum(layerChild), precedingLayer)[m
[31m     {[m
[31m+	m_batchNorm = (wNum(layerChild)>0)? true : false;[m
[31m 	[m
[31m+	if (m_batchNorm){[m
[31m+	    // initialization[m
[31m+	    m_stdConst  = 0.001;[m
[31m+	    m_batchCnt  = 0.0;[m
[31m+	    m_preEpoch  = 1;[m
[31m+	    [m
[31m+	    // mean, std[m
[31m+	    Cpu::real_vector tmp;[m
[31m+	    tmp.resize(this->size() * 2, 0.0); [m
[31m+	    m_stats     = tmp;[m
[31m+[m
[31m+	    // all-one vector for vector summation[m
[31m+	    tmp.resize(this->outputs().size()/this->size(), 1.0);[m
[31m+	    m_oneVector = tmp;[m
[31m+[m
[31m+	    // a tempopary buff[m
[31m+	    m_buff      = this->outputs();[m
[31m+	    m_outNormed = this->outputs();[m
[31m+		    [m
[31m+	    if (weightsSection.isValid() && weightsSection->HasMember(this->name().c_str())) {[m
[31m+		// read [m
[31m+	    [m
[31m+	    }else{[m
[31m+		// initialize [m
[31m+		int transMatrixWeightNum = this->size() * this->precedingLayer().size();[m
[31m+		[m
[31m+		// alpha = 1.0[m
[31m+		thrust::fill(this->weights().begin() + transMatrixWeightNum,[m
[31m+			     this->weights().begin() + transMatrixWeightNum + this->size(), 1.0);[m
[31m+		// beta, mean, std[m
[31m+		thrust::fill(this->weights().begin() + transMatrixWeightNum + this->size(),[m
[31m+			     this->weights().end(), 0.0);[m
[31m+	    }[m
[31m+[m
[31m+	    const Configuration &config = Configuration::instance();[m
[31m+	    m_trainFlag = config.trainingMode();[m
[31m+	}[m
[31m     }[m
[31m [m
[31m     template <typename TDevice, typename TActFn>[m
[31m@@ -156,9 +358,13 @@ namespace layers {[m
[31m     template <typename TDevice, typename TActFn>[m
[31m     void FeedForwardLayer<TDevice, TActFn>::computeForwardPass()[m
[31m     {[m
[31m-		[m
[31m-        // collect outputs from preceding layer[m
[31m-        {{[m
[31m+[m
[31m+	// Fine, I am lazy to merge the code[m
[31m+	if (!m_batchNorm){[m
[31m+	    [m
[31m+	    // The conventional feedforward part[m
[31m+	    // collect outputs from preceding layer[m
[31m+	    {{[m
[31m             helpers::Matrix<TDevice> weightsMatrix  (&this->weights(),                  [m
[31m 						     this->precedingLayer().size(), this->size());[m
[31m 	    [m
[31m@@ -174,10 +380,10 @@ namespace layers {[m
[31m 						     this->parallelSequences());[m
[31m [m
[31m             outputsMatrix.assignProduct(weightsMatrix, true, plOutputsMatrix, false);[m
[31m-        }}[m
[31m-[m
[31m-        // calculate the outputs of the layer[m
[31m-        {{[m
[31m+	    }}[m
[31m+	[m
[31m+	    // calculate the outputs of the layer[m
[31m+	    {{[m
[31m             internal::ComputeOutputFn<TActFn> fn;[m
[31m             fn.layerSize        = this->size();[m
[31m             fn.bias             = this->bias();[m
[31m@@ -192,13 +398,160 @@ namespace layers {[m
[31m                 this->_outputs().begin(),[m
[31m                 fn[m
[31m                 );[m
[31m-        }}[m
[31m+	   }}[m
[31m+	    [m
[31m+	}else{[m
[31m+	    // if batch normalization is used[m
[31m+	    int transMatrixWeightNum = this->size() * this->precedingLayer().size();[m
[31m+	    [m
[31m+	    // Re-initialize the batch mean and variance[m
[31m+	    if (m_trainFlag && m_preEpoch > 0 && m_preEpoch != this->getCurrTrainingEpoch()){[m
[31m+		// always update the mean, std for each epoch[m
[31m+		m_batchCnt = 0;[m
[31m+		thrust::fill(this->weights().begin() + transMatrixWeightNum + 2 * this->size(),[m
[31m+			     this->weights().end(),  0.0);[m
[31m+		m_preEpoch = this->getCurrTrainingEpoch();[m
[31m+	    }[m
[31m+[m
[31m+	    // Wx[m
[31m+	    {{[m
[31m+		helpers::Matrix<TDevice> weightsMatrix  (&this->weights(),                  [m
[31m+							 this->precedingLayer().size(),[m
[31m+							 this->size());[m
[31m+		helpers::Matrix<TDevice> plOutputsMatrix(&this->precedingLayer().outputs(), [m
[31m+							 this->precedingLayer().size(), [m
[31m+							 this->curMaxSeqLength() * [m
[31m+							 this->parallelSequences());[m
[31m+		helpers::Matrix<TDevice> outputsMatrix  (&this->m_outNormed,                 [m
[31m+							 this->size(),                  [m
[31m+							 this->curMaxSeqLength() * [m
[31m+							 this->parallelSequences());[m
[31m+		outputsMatrix.assignProduct(weightsMatrix, true, plOutputsMatrix, false);[m
[31m+	    }}	    [m
[31m+[m
[31m+	    // normalize the data[m
[31m+	    m_batchCnt++;[m
[31m+	    {{[m
[31m+	       int maxFrameNum = this->curMaxSeqLength() * this->parallelSequences();[m
[31m+	       int maxDataNum  = maxFrameNum * this->size();[m
[31m+	       [m
[31m+	       // Step1. calculate the batch size[m
[31m+	       //        For parallel sentences, there is dummy node. BatchSize should not count it.[m
[31m+	       internal::BatchSize fn0;[m
[31m+	       fn0.patTypes = helpers::getRawPointer(this->patTypes());[m
[31m+	       m_batchSize  = thrust::transform_reduce([m
[31m+				thrust::make_zip_iterator([m
[31m+				    thrust::make_tuple([m
[31m+					this->m_buff.begin(), [m
[31m+					thrust::counting_iterator<int>(0))),[m
[31m+				thrust::make_zip_iterator([m
[31m+				    thrust::make_tuple([m
[31m+					this->m_buff.begin()              + maxFrameNum, [m
[31m+					thrust::counting_iterator<int>(0) + maxFrameNum)),[m
[31m+				fn0, (real_t)0.0, thrust::plus<real_t>());[m
[31m+	       [m
[31m+	       thrust::fill(this->m_oneVector.begin(), this->m_oneVector.end(), 1.0/m_batchSize);[m
[31m+	       [m
[31m+	       // Step2. accumulate the mean[m
[31m+	       internal::PrepareForMeanStd fn1;[m
[31m+	       fn1.layerSize  = this->size();[m
[31m+	       fn1.meanNotVar = true;[m
[31m+	       fn1.mean       = NULL;[m
[31m+	       fn1.patTypes   = helpers::getRawPointer(this->patTypes());[m
[31m+	       fn1.data       = helpers::getRawPointer(this->m_outNormed);[m
[31m+	       fn1.outdata    = helpers::getRawPointer(this->m_buff);	   [m
[31m+	       thrust::for_each([m
[31m+		 thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->outputs().begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		 thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->outputs().begin()           + maxDataNum, [m
[31m+					   thrust::counting_iterator<int>(0) + maxDataNum)),[m
[31m+		 fn1);[m
[31m+	   [m
[31m+	       helpers::Matrix<TDevice> onevec  (&this->m_oneVector, maxFrameNum,  1);[m
[31m+	       helpers::Matrix<TDevice> data    (&this->m_buff,      this->size(), maxFrameNum);[m
[31m+	       helpers::Matrix<TDevice> meanVec (&this->m_stats,     this->size(), 1);[m
[31m+	       meanVec.assignProduct(data, false, onevec, false);[m
[31m+[m
[31m+	       // Step3. accumulate the var[m
[31m+	       fn1.meanNotVar = false;[m
[31m+	       fn1.mean       = helpers::getRawPointer(this->m_stats);; [m
[31m+	       thrust::for_each([m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->outputs().begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->outputs().begin()           + maxDataNum, [m
[31m+					   thrust::counting_iterator<int>(0) + maxDataNum)),[m
[31m+		fn1);[m
[31m+	       [m
[31m+	       helpers::Matrix<TDevice> data2   (&this->m_buff,  this->size(), maxFrameNum);[m
[31m+	       helpers::Matrix<TDevice> stdVec  (&this->m_stats, this->size(), 1, this->size());[m
[31m+	       stdVec.assignProduct(data2, false, onevec, false);[m
[31m+	       [m
[31m+	       internal::GetStd fn3;[m
[31m+	       fn3.stdConst = m_stdConst;[m
[31m+	       thrust::for_each([m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(m_stats.begin() + this->size(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(m_stats.begin() + this->size() * 2, [m
[31m+					   thrust::counting_iterator<int>(0) + this->size())),[m
[31m+		fn3);[m
[31m+[m
[31m+	       // Step4. accumulate the mean and std, for generation stage[m
[31m+	       if (m_trainFlag){[m
[31m+		   internal::AveMeanStd fn5;[m
[31m+		   fn5.meanStdBuf = (helpers::getRawPointer(this->weights()) +[m
[31m+				     transMatrixWeightNum + this->size() * 2);[m
[31m+		   fn5.cnt        = m_batchCnt;[m
[31m+		   thrust::for_each([m
[31m+		     thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(m_stats.begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		     thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(m_stats.begin() + this->size() * 2, [m
[31m+					   thrust::counting_iterator<int>(0) + this->size() * 2)),[m
[31m+		     fn5);[m
[31m+	       }[m
[31m+	   [m
[31m+	       // Step5: normalize and scale the data[m
[31m+	       internal::ComputeBatchNorm_Transform<TActFn> fn2;[m
[31m+	       fn2.layerSize = this->size();[m
[31m+	       fn2.patTypes  = helpers::getRawPointer(this->patTypes());[m
[31m+	       fn2.data      = helpers::getRawPointer(this->m_outNormed);[m
[31m+	       fn2.outdata   = helpers::getRawPointer(this->outputs());[m
[31m+	       fn2.scale     = helpers::getRawPointer(this->weights()) + transMatrixWeightNum;[m
[31m+	       fn2.meanStd   = helpers::getRawPointer(this->m_stats);[m
[31m+	       fn2.meanStdBuf= (helpers::getRawPointer(this->weights()) +[m
[31m+				transMatrixWeightNum + this->size() * 2);[m
[31m+	       fn2.trainFlag = m_trainFlag;[m
[31m+	   [m
[31m+	       thrust::for_each([m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->outputs().begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->outputs().begin()           + maxDataNum, [m
[31m+					   thrust::counting_iterator<int>(0) + maxDataNum)),[m
[31m+		fn2);[m
[31m+[m
[31m+	    }}[m
[31m+	}[m
[31m+	[m
[31m+	// done[m
[31m     }[m
[31m [m
[31m [m
[31m     template <typename TDevice, typename TActFn>[m
[31m     void FeedForwardLayer<TDevice, TActFn>::computeForwardPass(const int timeStep)[m
[31m     {[m
[31m+	if (m_batchNorm){[m
[31m+	    throw std::runtime_error("Error: batchnorm not available for online processing");[m
[31m+	}[m
[31m+	[m
[31m 	int effTimeStep = timeStep * this->parallelSequences();[m
[31m 	// collect outputs from preceding layer[m
[31m         {{[m
[31m@@ -239,23 +592,95 @@ namespace layers {[m
[31m     template <typename TDevice, typename TActFn>[m
[31m     void FeedForwardLayer<TDevice, TActFn>::computeBackwardPass()[m
[31m     {[m
[31m-        // compute deltas[m
[31m-        {{[m
[31m+	// compute deltas[m
[31m+	{{[m
[31m             internal::ComputeDeltaFn<TActFn> fn;[m
[31m [m
[31m             int n = this->curMaxSeqLength() * this->parallelSequences() * this->size();[m
[31m [m
[31m             thrust::for_each([m
[31m-                thrust::make_zip_iterator(thrust::make_tuple(this->outputErrors().begin(),   [m
[31m-							     this->outputs().begin())),[m
[31m-                thrust::make_zip_iterator(thrust::make_tuple(this->outputErrors().begin()+n, [m
[31m-							     this->outputs().begin()+n)),[m
[31m-                fn[m
[31m-                );[m
[31m-        }}[m
[31m+               thrust::make_zip_iterator([m
[31m+		  thrust::make_tuple(this->outputErrors().begin(),   this->outputs().begin())),[m
[31m+	       thrust::make_zip_iterator([m
[31m+		  thrust::make_tuple(this->outputErrors().begin()+n, this->outputs().begin()+n)),[m
[31m+                fn);[m
[31m+	}}[m
[31m+[m
[31m+	if (m_batchNorm) {[m
[31m+	    // for batch normalization[m
[31m+	    int maxFrameNum          = this->curMaxSeqLength() * this->parallelSequences();[m
[31m+	    int maxDataNum           = maxFrameNum * this->size();[m
[31m+	    int transMatrixWeightNum = this->size() * this->precedingLayer().size();[m
[31m+[m
[31m+	    thrust::fill(m_oneVector.begin(),            m_oneVector.end(),            1.0);[m
[31m+	    thrust::fill(m_buff.begin(),                 m_buff.end(),                 0.0);[m
[31m+	    thrust::fill(this->_weightUpdates().begin(), this->_weightUpdates().end(), 0.0);[m
[31m+	    [m
[31m+	    [m
[31m+	    // Step1. Calculate \deltaE/\delta{\alpha}[m
[31m+	    internal::PrepareGrad fn1;[m
[31m+	    fn1.layerSize    = this->size();[m
[31m+	    fn1.alphaNotBeta = true;[m
[31m+	    fn1.patTypes     = helpers::getRawPointer(this->patTypes());[m
[31m+	    fn1.grad         = helpers::getRawPointer(this->outputErrors());[m
[31m+	    fn1.data         = helpers::getRawPointer(this->m_outNormed);[m
[31m+	    [m
[31m+	    thrust::for_each([m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->m_buff.begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->m_buff.begin() + maxDataNum, [m
[31m+					   thrust::counting_iterator<int>(0) + maxDataNum)),[m
[31m+		fn1);[m
[31m+	   [m
[31m+	    helpers::Matrix<TDevice> onevec    (&this->m_oneVector, maxFrameNum, 1);[m
[31m+	    helpers::Matrix<TDevice> data      (&this->m_buff,      this->size(), maxFrameNum);[m
[31m+	    helpers::Matrix<TDevice> gradAlpha (&this->_weightUpdates(), this->size(), 1,[m
[31m+						transMatrixWeightNum);[m
[31m+	   gradAlpha.assignProduct(data, false, onevec, false);[m
[31m+[m
[31m+	   // Step2. Calculate \deltaE/\delta{\beta}[m
[31m+	   fn1.alphaNotBeta = false;	   [m
[31m+	   thrust::for_each([m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->m_buff.begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(this->m_buff.begin() + maxDataNum, [m
[31m+					   thrust::counting_iterator<int>(0) + maxDataNum)),[m
[31m+		fn1);[m
[31m+	   [m
[31m+	   helpers::Matrix<TDevice> gradBeta (&this->_weightUpdates(), this->size(),1,[m
[31m+					      transMatrixWeightNum + this->size());[m
[31m+	   gradBeta.assignProduct(data, false, onevec, false);[m
[31m+	   [m
[31m+[m
[31m+	   // Step3. Calculate \deltaE/\delta{x}[m
[31m+	   internal::ComputeBatchGradient_output fn2;[m
[31m+	   fn2.layerSize = this->size();[m
[31m+	   fn2.patTypes  = helpers::getRawPointer(this->patTypes());[m
[31m+	   fn2.errors    = helpers::getRawPointer(this->outputErrors());[m
[31m+	   fn2.outNormed = helpers::getRawPointer(m_outNormed);[m
[31m+	   fn2.meanStd   = helpers::getRawPointer(m_stats);[m
[31m+	   fn2.scale     = helpers::getRawPointer(this->weights())        + transMatrixWeightNum;[m
[31m+	   fn2.scaleGrad = helpers::getRawPointer(this->_weightUpdates()) + transMatrixWeightNum;[m
[31m+	   fn2.batchSize = m_batchSize;[m
[31m+	   [m
[31m+	   thrust::for_each([m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(m_outNormed.begin(), [m
[31m+					   thrust::counting_iterator<int>(0))),[m
[31m+		thrust::make_zip_iterator([m
[31m+			thrust::make_tuple(m_outNormed.begin() + maxDataNum, [m
[31m+					   thrust::counting_iterator<int>(0) + maxDataNum)),[m
[31m+		fn2);[m
[31m+[m
[31m+	}[m
[31m [m
[31m-        // back-propagate the error to the preceding layer[m
[31m-        {{[m
[31m+	[m
[31m+	// back-propagate the error to the preceding layer[m
[31m+	{{[m
[31m             TrainableLayer<TDevice> *pl = [m
[31m 		dynamic_cast<TrainableLayer<TDevice>*>(&this->precedingLayer());[m
[31m 	    [m
[31m@@ -295,10 +720,10 @@ namespace layers {[m
[31m 		    plErrorsMatrix.assignProduct(weightsMatrix, false, deltasMatrix, false);[m
[31m 		}[m
[31m 	    }[m
[31m-        }}[m
[31m+	}}[m
[31m [m
[31m-        // compute the input weight updates[m
[31m-        {{[m
[31m+	// compute the input weight updates[m
[31m+	{{[m
[31m             helpers::Matrix<TDevice> weightUpdatesMatrix(&this->_weightUpdates(),           [m
[31m 							 this->precedingLayer().size(), [m
[31m 							 this->size());[m
[31m@@ -314,10 +739,11 @@ namespace layers {[m
[31m 							 this->parallelSequences());[m
[31m [m
[31m             weightUpdatesMatrix.assignProduct(plOutputsMatrix, false, deltasMatrix, true);[m
[31m-        }}[m
[31m+	}}[m
[31m [m
[31m-        // compute the bias weight updates[m
[31m-        {{[m
[31m+	if (!m_batchNorm){[m
[31m+	    // compute the bias weight updates[m
[31m+	    {{[m
[31m             internal::ComputeBiasWeightUpdateFn fn;[m
[31m             fn.layerSize     = this->size();[m
[31m             fn.patternsCount = this->curMaxSeqLength() * this->parallelSequences();[m
[31m@@ -330,24 +756,29 @@ namespace layers {[m
[31m                 this->_weightUpdates().begin() + this->precedingLayer().size() * this->size(),[m
[31m                 fn[m
[31m                 );[m
[31m-        }}[m
[31m-	[m
[31m-	/*[m
[31m-	if (this->_optOpt()){[m
[31m-	    [m
[31m-	    {{[m
[31m-		internal::GradientAverage fn;[m
[31m-		fn.timeStep = (real_t)(this->curMaxSeqLength() * this->parallelSequences());[m
[31m-		fn.gradients= helpers::getRawPointer(this->_weightUpdates());		[m
[31m-		thrust::for_each([m
[31m-				 thrust::counting_iterator<int>(0),[m
[31m-				 thrust::counting_iterator<int>(0) + this->_weightUpdates().size(),[m
[31m-				 fn[m
[31m-                );[m
[31m-[m
[31m 	    }}[m
[31m-	    [m
[31m-	    }*/[m
[31m+	}[m
[31m+	/* Gradient averaging ?[m
[31m+	      if (this->_optOpt()){[m
[31m+	      {{[m
[31m+	      internal::GradientAverage fn;[m
[31m+	      fn.timeStep = (real_t)(this->curMaxSeqLength() * this->parallelSequences());[m
[31m+	      fn.gradients= helpers::getRawPointer(this->_weightUpdates());		[m
[31m+	      thrust::for_each([m
[31m+	      thrust::counting_iterator<int>(0),[m
[31m+	      thrust::counting_iterator<int>(0) + this->_weightUpdates().size(),[m
[31m+	      fn);[m
[31m+	      }}[m
[31m+	      }*/[m
[31m+    }[m
[31m+[m
[31m+    template <typename TDevice, typename TActFn>[m
[31m+    void FeedForwardLayer<TDevice, TActFn>::exportLayer([m
[31m+	const helpers::JsonValue     &layersArray, [m
[31m+	const helpers::JsonAllocator &allocator) const[m
[31m+    {[m
[31m+        TrainableLayer<TDevice>::exportLayer(layersArray, allocator);[m
[31m+        (*layersArray)[layersArray->Size() - 1].AddMember("batchnorm", (int)m_batchNorm, allocator);[m
[31m     }[m
[31m [m
[31m [m
[31mdiff --git a/layers/FeedForwardLayer.hpp b/layers/FeedForwardLayer.hpp[m
[31mindex c794243..20bd8e4 100644[m
[31m--- a/layers/FeedForwardLayer.hpp[m
[31m+++ b/layers/FeedForwardLayer.hpp[m
[31m@@ -37,6 +37,24 @@ namespace layers {[m
[31m     template <typename TDevice, typename TActFn>[m
[31m     class FeedForwardLayer : public TrainableLayer<TDevice>[m
[31m     {[m
[31m+	typedef typename TDevice::real_vector real_vector;[m
[31m+	typedef typename TDevice::int_vector  int_vector;[m
[31m+	typedef typename TDevice::bool_vector bool_vector;[m
[31m+	typedef typename TDevice::pattype_vector pattype_vector;[m
[31m+[m
[31m+	bool m_batchNorm;            // whether to use batch normalization[m
[31m+	real_vector m_stats;         // mean and variance of each batch[m
[31m+	real_vector m_outNormed;     // normed data output without being scaled[m
[31m+	[m
[31m+	real_t      m_stdConst;      // const floor for the var[m
[31m+	real_t      m_batchCnt;[m
[31m+	bool        m_trainFlag;[m
[31m+	int         m_preEpoch;[m
[31m+	real_t      m_batchSize;     //[m
[31m+[m
[31m+	real_vector m_oneVector;     // all-one vector[m
[31m+	real_vector m_buff;[m
[31m+[m
[31m     public:[m
[31m         /**[m
[31m          * Constructs the Layer[m
[31m@@ -76,7 +94,12 @@ namespace layers {[m
[31m 	 * [m
[31m 	 */[m
[31m 	virtual void computeForwardPass(const int timeStep);[m
[31m-	[m
[31m+[m
[31m+[m
[31m+	// export[m
[31m+	virtual void exportLayer(const helpers::JsonValue &layersArray, [m
[31m+				 const helpers::JsonAllocator &allocator) const;[m
[31m+[m
[31m     };[m
[31m [m
[31m } // namespace layers[m
[1mdiff --git a/layers/BatchNorm.cu b/layers/BatchNorm.cu[m
[1mindex 2174bed..a7095c1 100644[m
[1m--- a/layers/BatchNorm.cu[m
[1m+++ b/layers/BatchNorm.cu[m
[36m@@ -357,8 +357,12 @@[m [mnamespace layers{[m
	    thrust::fill(this->weights().begin() + this->size(), this->weights().end(), 0.0);[m
	}[m

	[31mconst[m[32m//const[m Configuration &config = Configuration::instance();
	[31mm_trainFlag[m[32m//m_trainFlag[m = config.trainingMode();
	
	[32mif (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("layer before batchnorm is reduced in mem");[m  

    }[m

    template <typename TDevice>[m
[36m@@ -377,7 +381,8 @@[m [mnamespace layers{[m
    template <typename TDevice>[m
    void BatchNormLayer<TDevice>::computeForwardPass(const int nnState)[m
    {[m
	if [31m(m_trainFlag[m[32m(this->flagTrainingMode()[m && m_preEpoch > 0 &&
	    m_preEpoch != this->getCurrTrainingEpoch()){
	    // always update the mean, std for each epoch[m
	    m_batchCnt = 0;[m
	    thrust::fill(this->weights().begin() + 2 * this->size(), this->weights().end(), 0.0);[m
[36m@@ -465,7 +470,7 @@[m [mnamespace layers{[m
		fn3);[m

	   // Step4. accumulate the mean and std, for generation stage[m
	   if [31m(m_trainFlag){[m[32m(this->flagTrainingMode()){[m
	       internal::AveMeanStd fn5;[m
	       fn5.meanStd    = helpers::getRawPointer(m_stats);[m
	       fn5.meanStdBuf = helpers::getRawPointer(this->weights()) + this->size() * 2;[m
[36m@@ -515,7 +520,7 @@[m [mnamespace layers{[m
	   fn2.scale     = helpers::getRawPointer(this->weights());[m
	   fn2.meanStd   = helpers::getRawPointer(m_stats);[m
	   fn2.meanStdBuf= helpers::getRawPointer(this->weights()) + this->size() * 2;[m
	   fn2.trainFlag = [31mm_trainFlag;[m[32mthis->flagTrainingMode();[m
	   [m
	   tmp       = this->size() * this->curMaxSeqLength() * this->parallelSequences();[m
	   thrust::for_each([m
[1mdiff --git a/layers/BatchNorm.hpp b/layers/BatchNorm.hpp[m
[1mindex b99540b..0f322c4 100644[m
[1m--- a/layers/BatchNorm.hpp[m
[1m+++ b/layers/BatchNorm.hpp[m
[36m@@ -45,7 +45,7 @@[m [mnamespace layers{[m
	[m
	real_t      m_stdConst;  // const floor for the var[m
	real_t      m_batchCnt;[m
	[32m//[m bool        m_trainFlag;  [32m// replaced by this->flagTrainingMode()[m
	int         m_preEpoch;[m
	real_t      m_batchSize; //[m

[36m@@ -58,8 +58,7 @@[m [mnamespace layers{[m
	BatchNormLayer([m
		const helpers::JsonValue &layerChild, [m
		const helpers::JsonValue &weightsSection,[m
		Layer<TDevice>           [31m&precedingLayer[m
[31m		);[m[32m&precedingLayer);[m

	virtual ~BatchNormLayer();[m

[1mdiff --git a/layers/CNNLayer.cu b/layers/CNNLayer.cu[m
[1mindex afbd88d..43747ff 100644[m
[1m--- a/layers/CNNLayer.cu[m
[1m+++ b/layers/CNNLayer.cu[m
[36m@@ -99,7 +99,7 @@[m [mnamespace{[m
	int     maxSeqLength;         // max length of one utterance[m

	int     causal;[m
	[32mint     outputTanh;[m
        __host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
        {[m
	    [m
[36m@@ -140,7 +140,10 @@[m [mnamespace{[m
	    }[m

	    // add bias and pass through the activation function[m
	    [32mif (outputTanh)[m
		t.get<0>() = cell_act_fn_t::fn(maxValue + biasWeight[dimIdx]);
	    [32melse[m
[32m		t.get<0>() = maxValue + biasWeight[dimIdx];[m
        }[m
    };[m

[36m@@ -164,7 +167,7 @@[m [mnamespace{[m
	int     maxSeqLength;         // max length of one utterance[m

	int     causal;[m

        __host__ __device__ void operator() (const thrust::tuple<const real_t&, int> &t) const[m
        {[m
	    [m
[36m@@ -375,6 +378,49 @@[m [mnamespace{[m
	}[m
    };[m



    [32mstruct ConvolutionCoreMemSaveMode[m
[32m    {[m

[32m	real_t *dataBuffer;[m
[32m	real_t *biasWeight;[m
[32m       [m
[32m	int     recFieldSize;     // recep field size[m
[32m	int     curLayerSize;     // output feature dimension[m
[32m	int     winTotalLength;   // dimension of the con buffer (3 * curLayerSize)[m

[32m	int     timeStep;         // absolute time index[m
[32m	int     parallel;[m
[32m	int     outputTanh;[m
[32m	const char *patTypes;[m

[32m	// for parallel * curLayerSize[m
[32m        __host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[32m        {[m
[32m            // calculate the index[m
[32m            int uttIdx  = t.get<1>() / curLayerSize;   // position in one parallel block[m
[32m	    int dimIdx  = t.get<1>() % curLayerSize;   // dimension idx[m

[32m	    if (patTypes[timeStep * parallel + uttIdx] == PATTYPE_NONE)[m
[32m		return;[m

[32m	    int timeIdxBuf1 = (timeStep % (recFieldSize+1)) * parallel + uttIdx;[m
[32m	    int timeIdxBuf2 = ((timeStep+1) % (recFieldSize+1)) * parallel + uttIdx;[m
[32m	    // (time+1) % (recFieldSize+1) = (time - recFieldSize) % (recFieldSize+1)[m
[32m	    int dimIdxBuf1  = dimIdx * 3 + 1;[m
[32m	    int dimIdxBuf2  = dimIdx * 3;	    [m
[32m	    real_t summedOutput = (dataBuffer[timeIdxBuf1 * winTotalLength + dimIdxBuf1] +[m
[32m				   dataBuffer[timeIdxBuf2 * winTotalLength + dimIdxBuf2]);[m
[32m	    // add bias and pass through the activation function[m
[32m	    if (outputTanh)[m
[32m		t.get<0>() = cell_act_fn_t::fn(summedOutput + biasWeight[dimIdx]);[m
[32m	    else[m
[32m		t.get<0>() = summedOutput + biasWeight[dimIdx];[m
[32m        }[m
[32m    };[m

    
} // namespace [m
} // namespace internal[m

[36m@@ -420,6 +466,16 @@[m [mnamespace CNNTools{[m
	return cnt;[m
    }[m

    [32m// return the total width of windows[m
[32m    // input:[m
[32m    //    opt: vector of half-width of each window[m
[32m    int getWinCausalLength(Cpu::int_vector &opt){[m
[32m	int cnt =0;[m
[32m	for (int i=0; i < opt.size(); i++){[m
[32m	    cnt += (opt[i] + 1);[m
[32m	}[m
[32m	return cnt;[m
[32m    }[m
    // parse the option for CNN configuration[m
    // input:[m
    //    cnnOpt: text string of configuration for original filter set[m
[36m@@ -648,7 +704,7 @@[m [mnamespace CNNTools{[m
		// count the total number of filter widths[m
		// parameter = width * pre_layer_size + #filter[m
		if (accurate)[m
		    return [31m((causal?SumCpuIntVec(width):getWinTotalLength(width))[m[32m((causal?getWinCausalLength(width):getWinTotalLength(width))[m *
			    preLayerSize + thisLayerSize);[m
		else	[m
		    return ((int)std::ceil(getWinTotalLength(width) *[m
[36m@@ -828,12 +884,15 @@[m [mnamespace layers {[m
					((*layerChild)["size"].GetInt()) : (0),[m
					precedingLayer.size(), false, false),[m
				    precedingLayer)[m
	[32m, m_outputTanh(1)[m
    {[m
	[m
	// Check casual filter[m
	m_causalFlag = (layerChild->HasMember("causal") ? [m
			static_cast<real_t>((*layerChild)["causal"].GetInt()) : 0);[m
	[32mm_outputTanh = (layerChild->HasMember("tanhoutput") ? [m
[32m			static_cast<real_t>((*layerChild)["tanhoutput"].GetInt()) : 1);[m
	
	if (m_winWidth_Opt.size() < 1)[m
	    throw std::runtime_error("Fail to find window_width in network.jsn");[m

[36m@@ -890,7 +949,7 @@[m [mnamespace layers {[m
	    Cpu::int_vector tmp;[m
	    CNNTools::fillInFilterWeightMap(m_winWidth_Opt, m_winHeight_Opt, m_weightNum, tmp);[m
	    m_weightFilter_map = tmp;[m
	    [32mm_1DCNNOnly = 0;[m
	}else{[m
	    // 1-D convolution[m
	    m_winColIndex_H.clear();[m
[36m@@ -904,6 +963,7 @@[m [mnamespace layers {[m
	    m_winWidthCol_D  = m_winWidthCol_H;[m
	    m_winShiftNum_D  = m_winShiftNum_H;[m
	    m_weightFilter_map.clear();[m
	    [32mm_1DCNNOnly = 1;[m
	}[m
	[m
	// parse the tap interval[m
[36m@@ -948,17 +1008,20 @@[m [mnamespace layers {[m
	    printf("\tCNN 1-D convolution\n");[m
	printf("\tCNN trainable weights: %d (weights in Network summary may be inaccurate)\n",[m
	       m_weightNum);[m
	if (m_causalFlag > [31m0){[m[32m0)[m
	    printf("\tCNN uses causal filter\n");[m
	[31m}[m[32mif (m_outputTanh == 0)[m
[32m	    printf("\nCNN without tanh output function\n");[m
	    
	if (Configuration::instance().verboseLevel() == OP_VERBOSE_LEVEL_3){[m
	    CNNTools::printCNNConfiguration(filterNum, this->size(), precedingLayer.size(),[m
					    m_winIndex_H,  m_winWidth_H,[m
					    m_winHeight_H, m_winStride_H);[m
	}else{[m
	    printf("\tCNN winwidth:    %s\n", m_winWidth_Opt.c_str());
	    printf("\tCNN [32mwinDilution: %s\n", m_winInterval_Opt.c_str());[m
[32m	    printf("\tCNN[m winHeight:   %s\n", m_winHeight_Opt.c_str());
	    printf("\tCNN winStride:   %s\n", m_winStride_Opt.c_str());
	}[m
    }[m
    [m
[36m@@ -1068,7 +1131,7 @@[m [mnamespace layers {[m
	    fn.maxSeqLength     = this->curMaxSeqLength();[m

	    fn.causal           = this->m_causalFlag;[m
	    [32mfn.outputTanh       = this->m_outputTanh;[m
	    int n =this->precedingLayer().curMaxSeqLength();[m
	    n = n*this->precedingLayer().parallelSequences();[m
	    n = n*this->size();[m
[36m@@ -1090,15 +1153,185 @@[m [mnamespace layers {[m
    template <typename TDevice>[m
    void CNNLayer<TDevice>::computeForwardPass(const int timeStep, const int nnState)[m
    {[m
	
	// [31mNot implemented[m
[31m	throw std::runtime_error("Not implemented yet");[m[32mStep0: prepare at the begining of the sentence[m
[32m	if (timeStep == 0){{[m
[32m	    if (m_winHeight_Opt.size()>0){[m

[32m		// 2-D convolution		[m
[32m		// Copy weight from this->weights() to m_weightBuffer[m
[32m		internal::CNNFilterWeightCopy fn1;[m
[32m		fn1.weightBuffer   = helpers::getRawPointer(m_weightBuffer);[m
[32m		fn1.weightCopyInfo = helpers::getRawPointer(m_wCopyInfo_D);[m
[32m		fn1.filterIndexMap = helpers::getRawPointer(m_weightFilter_map);[m
[32m		fn1.filterNum      = m_winNumOrignal;[m
[32m		fn1.preLayerSize   = this->precedingLayer().size();[m
[32m		fn1.biasPosition   = m_biasPos;[m
[32m		fn1.biasPositionBuf= m_biasPosInBuffer;[m
[32m		fn1.reverse        = false;[m
[32m		[m
[32m		int n = m_weightNum;[m
[32m		thrust::for_each([m
[32m		  thrust::make_zip_iterator([m
[32m			thrust::make_tuple(this->weights().begin(),[m
[32m					   thrust::counting_iterator<int>(0))),[m
[32m		  thrust::make_zip_iterator([m
[32m			thrust::make_tuple(this->weights().begin() + n, [m
[32m					   thrust::counting_iterator<int>(0) + n)),[m
[32m		  fn1);[m
[32m		[m
[32m		// duplicate weights inside m_weightBuffer[m
[32m		internal::CNNFilterDuplicate fn;[m
[32m		fn.weight       = helpers::getRawPointer(m_weightBuffer);[m
[32m		fn.wColIndex    = helpers::getRawPointer(m_winColIndex_D);[m
[32m		fn.wRowIndex    = helpers::getRawPointer(m_winRowIndex_D);[m
[32m		fn.wFilterHeight= helpers::getRawPointer(m_winColHeight_D);[m
[32m		fn.wFilterShift = helpers::getRawPointer(m_winShiftIndex_D);[m
[32m		fn.layerSize    = this->precedingLayer().size();[m
[32m		fn.matrixWNum   = m_biasPosInBuffer;[m
[32m		[m
[32m		n = this->m_winTotalL * this->precedingLayer().size() + this->size();[m
[32m		thrust::for_each([m
[32m		  thrust::make_zip_iterator([m
[32m			thrust::make_tuple(this->m_weightBuffer.begin(),[m
[32m					   thrust::counting_iterator<int>(0))),[m
[32m		  thrust::make_zip_iterator([m
[32m			thrust::make_tuple(this->m_weightBuffer.begin() + n, [m
[32m					   thrust::counting_iterator<int>(0) + n)),[m
[32m		  fn);[m
[32m		[m
[32m	    }else{[m

[32m		// 1-D convolution[m
[32m		// copy the weights from this->weights() to m_weightBuffer[m
[32m		thrust::copy(this->weights().begin(),[m
[32m			     this->weights().begin() + m_weightBufferNum,[m
[32m			     m_weightBuffer.begin());[m
[32m	    }[m

[32m	    // initialize the data buffer[m
[32m	    thrust::fill(m_conBuffer.begin(), m_conBuffer.end(), 0.0);[m
[32m	}}[m


[32m	int st = timeStep * this->parallelSequences();[m
[32m	int et = (timeStep + 1) * this->parallelSequences();[m
[32m	int shiftIn = this->precedingLayer().outputBufPtrBias(st, nnState);[m

[32m	// Step1-2: matrix transformation and data summation[m
[32m	if (this->getSaveMemoryFlag()){[m
[32m	    // memory save mode for wavenet[m
[32m	    [m
[32m	    // Step1. matrix transformation[m
[32m	    // receptive filed size[m
[32m	    int recField = m_winInterval_H[0];[m
[32m	    // absolute address in the conv buffer[m
[32m	    int bufAddr  = (timeStep % (recField+1)) * this->parallelSequences() * m_winTotalL;[m
[32m	    [m
[32m	    helpers::Matrix<TDevice> weightMatrix   (&this->m_weightBuffer,[m
[32m						     this->precedingLayer().size(),[m
[32m						     this->m_winTotalL);[m
[32m	    helpers::Matrix<TDevice> plOutputsMatrix(&this->precedingLayer().outputs(), [m
[32m						     this->precedingLayer().size(), [m
[32m						     this->parallelSequences(),[m
[32m						     st * this->precedingLayer().size() - shiftIn);[m
[32m            helpers::Matrix<TDevice> outputsMatrix  (&this->m_conBuffer,                 [m
[32m						     this->m_winTotalL,                   [m
[32m						     this->parallelSequences(),[m
[32m						     bufAddr);[m
[32m            outputsMatrix.assignProduct(weightMatrix, true, plOutputsMatrix, false);[m
[32m	    [m
[32m	    // Step2. data summation[m
[32m	    internal::ConvolutionCoreMemSaveMode fn;[m
[32m	    	    [m
[32m	    fn.dataBuffer       = helpers::getRawPointer(this->m_conBuffer);[m
[32m	    fn.biasWeight       = helpers::getRawPointer(this->m_weightBuffer) + m_biasPosInBuffer;[m

[32m	    fn.recFieldSize     = recField;[m
[32m	    fn.curLayerSize     = this->size();[m
[32m	    fn.winTotalLength   = this->m_winTotalL;[m

[32m	    fn.timeStep         = timeStep;[m
[32m	    fn.outputTanh       = this->m_outputTanh;[m
[32m	    fn.parallel         = this->precedingLayer().parallelSequences();[m
[32m	    fn.patTypes         = helpers::getRawPointer(this->patTypes());[m

[32m	    int numEle = this->parallelSequences() * this->size();[m
[32m	    thrust::for_each([m
[32m	     thrust::make_zip_iterator([m
[32m		thrust::make_tuple(this->outputs().begin(),[m
[32m				   thrust::counting_iterator<int>(0))),[m
[32m	     thrust::make_zip_iterator([m
[32m		thrust::make_tuple(this->outputs().begin() + numEle,[m
[32m				   thrust::counting_iterator<int>(0) + numEle)),[m
[32m	     fn);[m
[32m	    [m
[32m	    [m
[32m	}else{[m
[32m	    [m
[32m	    // normal mode[m
[32m	    // Step1: prepare the data buffer by matrix transformation[m
[32m	    {{[m
[32m	    helpers::Matrix<TDevice> weightMatrix   (&this->m_weightBuffer,[m
[32m						     this->precedingLayer().size(),[m
[32m						     this->m_winTotalL);[m
[32m	    [m
[32m	    helpers::Matrix<TDevice> plOutputsMatrix(&this->precedingLayer().outputs(), [m
[32m						     this->precedingLayer().size(), [m
[32m						     this->parallelSequences(),[m
[32m						     st * this->precedingLayer().size() - shiftIn);[m

[32m            helpers::Matrix<TDevice> outputsMatrix  (&this->m_conBuffer,                 [m
[32m						     this->m_winTotalL,                   [m
[32m						     this->parallelSequences(),[m
[32m						     st * this->m_winTotalL);[m

[32m            outputsMatrix.assignProduct(weightMatrix, true, plOutputsMatrix, false);[m
[32m	    }}[m

[32m	    // Step2: sum the result[m
[32m	    {{[m
[32m	    internal::ConvolutionCore fn;[m
[32m	    	    [m
[32m	    fn.dataBuffer       = helpers::getRawPointer(this->m_conBuffer);[m
[32m	    //fn.targetBuff       = helpers::getRawPointer(this->outputs());[m
[32m	    fn.biasWeight       = helpers::getRawPointer(this->m_weightBuffer) + m_biasPosInBuffer;[m
[32m	    [m
[32m	    fn.winSizeCum       = helpers::getRawPointer(m_winWidth_Cum_D);[m
[32m	    fn.winHalfSize      = helpers::getRawPointer(m_winWidth_D);[m
[32m	    fn.winTapInter      = helpers::getRawPointer(m_winInterval_D);[m
[32m		[m
[32m	    fn.curLayerSize     = this->size();[m
[32m	    fn.winTotalLength   = this->m_winTotalL;[m

[32m	    fn.patTypes         = helpers::getRawPointer(this->patTypes());[m
[32m	    fn.paral            = this->precedingLayer().parallelSequences();[m
[32m	    fn.maxSeqLength     = this->curMaxSeqLength();[m

[32m	    fn.causal           = this->m_causalFlag;[m
[32m	    fn.outputTanh       = this->m_outputTanh;[m
[32m	    int n =this->precedingLayer().curMaxSeqLength();[m
[32m	    n = n*this->precedingLayer().parallelSequences();[m
[32m	    n = n*this->size();[m

[32m	    thrust::for_each([m
[32m	     thrust::make_zip_iterator([m
[32m		thrust::make_tuple(this->outputs().begin() + st * this->size(),[m
[32m				   thrust::counting_iterator<int>(0)+ st * this->size())),[m
[32m	     thrust::make_zip_iterator([m
[32m		thrust::make_tuple(this->outputs().begin() + et * this->size(), [m
[32m				   thrust::counting_iterator<int>(0)+ et * this->size())),[m
[32m	     fn);[m

[32m	    }}[m
[32m	}[m
    }[m
    [m
    template <typename TDevice>[m
    void CNNLayer<TDevice>::computeBackwardPass(const int nnState)[m
    {[m
	// Step1: Pass throught the nonlinear function[m
	[31m{{[m[32mif (m_outputTanh){{[m
            internal::ComputeDeltaFn fn;[m
            int n = this->curMaxSeqLength() * this->parallelSequences() * this->size();[m
            thrust::for_each([m
[36m@@ -1314,8 +1547,49 @@[m [mnamespace layers {[m
        (*layersArray)[layersArray->Size() - 1].AddMember("window_stride",[m
							  m_winStride_Opt.c_str(),[m
							  allocator);[m
	[32m(*layersArray)[layersArray->Size() - 1].AddMember("causal", m_causalFlag,[m
[32m							  allocator);[m
[32m	(*layersArray)[layersArray->Size() - 1].AddMember("tanhoutput", m_outputTanh,[m
[32m							  allocator);	[m
[32m    }[m


[32m    template <typename TDevice>[m
[32m    void CNNLayer<TDevice>::reduceOutputBuffer()[m
[32m    {[m
[32m	// check whether this is the CNN in wavenet[m
[32m	// 1. causal filter[m
[32m	// 2. 1-D CNN[m
[32m	// 3. filter width is 3 (although 2 columns of weights are used by causal CNN)[m
[32m	// 4. filter interval is the same[m
[32m	if (m_causalFlag && m_1DCNNOnly && (m_winTotalL == (this->size() * 3))){[m
[32m	    // check the filter interval[m
[32m	    int recepField = m_winInterval_H[0];[m
[32m	    for (int i = 0; i < m_winInterval_H.size(); i++)[m
[32m		if (recepField != m_winInterval_H[i])[m
[32m		    return;[m
[32m	    [m
[32m	    // save the intermediate buffer[m
[32m	    m_conBuffer.resize(this->parallelSequences() * m_winTotalL * (recepField + 1), 0);[m
[32m	    m_conBuffer.shrink_to_fit();[m
[32m	    [m
[32m	    // save the output buffer size[m
[32m	    this->resizeOutputBuffer(this->parallelSequences() * this->size());[m
[32m	    printf("\t[mem saved]");[m
[32m	    this->setSaveMemoryFlag(true);[m
[32m	}[m
    }[m

    [32mtemplate <typename TDevice>[m
[32m    int CNNLayer<TDevice>::outputBufPtrBias(const int timeStepTimesParallel, const int nnState)[m
[32m    {[m
[32m	if (this->getSaveMemoryFlag()){[m
[32m	    return timeStepTimesParallel * this->size();[m
[32m	}else{[m
[32m	    return 0;[m
[32m	}[m
[32m    }[m
    
    template class CNNLayer<Gpu>;[m
    template class CNNLayer<Cpu>;[m
}[m
[1mdiff --git a/layers/CNNLayer.hpp b/layers/CNNLayer.hpp[m
[1mindex df51abe..9244b0d 100644[m
[1m--- a/layers/CNNLayer.hpp[m
[1m+++ b/layers/CNNLayer.hpp[m
[36m@@ -104,6 +104,9 @@[m [mnamespace layers {[m

	int             m_causalFlag;       // whether the CNN filter is casual filter[m
	[m
	[32mint             m_outputTanh;       //[m

[32m	int             m_1DCNNOnly;        // whether the CNN is only 1-D[m 
    public:[m
	// initializer and destructor[m
	CNNLayer(const helpers::JsonValue &layerChild,[m
[36m@@ -125,6 +128,12 @@[m [mnamespace layers {[m
	// export[m
	virtual void exportLayer(const helpers::JsonValue &layersArray, [m
				 const helpers::JsonAllocator &allocator) const;[m

	[32m// memory save mode for generation in wavenet[m
[32m	virtual void reduceOutputBuffer();[m

[32m	virtual int  outputBufPtrBias(const int timeStepTimesParallel, const int nnState);[m
	
    };[m
    [m
    [m
[1mdiff --git a/layers/FeatMatch.cu b/layers/FeatMatch.cu[m
[1mindex 03634fd..edc0c49 100644[m
[1m--- a/layers/FeatMatch.cu[m
[1m+++ b/layers/FeatMatch.cu[m
[36m@@ -138,6 +138,11 @@[m [mnamespace layers{[m
	if (this->size() != precedingLayer.size()){[m
	    throw std::runtime_error("Error featMatch must be same size as previous layer");[m
	}[m
	
	[32mif (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("layer before featmatch is reduced in mem");[m  

	
    }[m

    template <typename TDevice>[m
[1mdiff --git a/layers/FeedBackLayer.cu b/layers/FeedBackLayer.cu[m
[1mindex 7926f77..96ede7e 100644[m
[1m--- a/layers/FeedBackLayer.cu[m
[1m+++ b/layers/FeedBackLayer.cu[m
[36m@@ -62,14 +62,18 @@[m [mnamespace {[m
	// Copy the output of preceding layer to the output of this layer[m
	// Copy the output of target layer to the output of this layer[m

	int [31mdimInput1;[m[32mdimInput1End;[m    // dimension of [32mthe[m output [31mof preceding[m[32mfrom previous[m layer
	int [31mdimInput2;[m[32mdimInput1Start;[m  // [32mfrom which[m dimension of [31moutput of target[m[32mthe previous[m layer [31m(to be fed back, in total dim)[m[32mto read[m
	int [31mdimInput2Start;[m[32mdimInput1Total;[m  //[31mfrom which[m dimension of [31mthe target to load (may not be 0)[m[32moutput of preceding layer[m
[32m	int dimInput1Valid;[m
	[m
	int [31mdimOutput;[m[32mdimInput2End;    // dimension of output of target layer (to be fed back, in total dim)[m
[32m	int dimInput2Start;  // from which dimension of the target to load (may not be 0)[m
[32m	int dimInput2Total;[m  // dimension of output of this layer
	[32mint dimInput2Valid;[m
[32m	[m
[32m	int dimOutput;[m
	int parallel;       // number of parallel sentences[m

[31m	int dim1Step;[m
	[m
	real_t *input1;     // preceding layer[m
	real_t *input2;     // target layer[m
[36m@@ -89,26 +93,27 @@[m [mnamespace {[m
	    int outputIdx    = timeStep * dimOutput + dimIdx;[m
	    int lookBackTime = 0;[m

	    if (dimIdx < [31m(dimInput1[m[32m(dimInput1Valid[m + lookBackStepNM * [31mdim1Step)){[m[32mdimInput2Valid)){[m
		
		if (dimIdx >= [31mdimInput1){[m[32mdimInput1Valid){[m
		    // copy from the target layer (feedback part)[m
		    [m
		    // get the dimension index (across multiple time steps)[m
		    dimIdx       = dimIdx - [31mdimInput1;[m[32mdimInput1Valid;[m
		    [m
		    // get the time shift to be looked backwards[m
		    if (lookBack != NULL)[m
			lookBackTime = lookBack[dimIdx / [31mdim1Step][m[32mdimInput2Valid][m * parallel;
		    else[m
			lookBackTime = 1;[m
		    [m
		    // get the dimension index in each time step[m
		    dimIdx       = dimIdx % [31mdim1Step;[m[32mdimInput2Valid;[m
		    [m
		    if (timeStep < lookBackTime)      [m
			output[outputIdx] = 0.0; // if the previous step is unavailable[m
		    else{[m
			output[outputIdx] = input2[(timeStep - lookBackTime) * [31mdimInput2[m[32mdimInput2Total[m +
						   dimIdx + dimInput2Start];[m

			// Block20170702x06			[m
[36m@@ -116,7 +121,7 @@[m [mnamespace {[m
		    [m
		}else{[m
		    //output[outputIdx] = 0;[m
		    output[outputIdx] = input1[timeStep * [31mdimInput1[m[32mdimInput1Total[m + dimIdx];
		}[m
	    }else{[m
		// this is section for aggregating information[m
[36m@@ -130,12 +135,13 @@[m [mnamespace {[m
	// Copy the output of preceding layer to the output of this layer[m
	// Copy the output of target layer to the output of this layer[m

	int [31mdimInput2;[m[32mdimInput2Total;[m  // dimension of output of target layer (to be fed back, in total dim)
	int dimInput2Start; // from which dimension of the target to load (may not be 0)[m
	[32mint dim1Band;[m
	[m
	int dimOutput;      // dimension of output of this layer[m
	int dimOutputStart;[m
[31mint dim1Band;[m	
	int bandNum;[m
	[m
	real_t *input2;     // target layer[m
[36m@@ -193,20 +199,22 @@[m [mnamespace {[m
			      cell_act_fn_t::fn(input2[inputIdx]) / ((time-boundTime)+1.0));[m

		outputIdx += dimOutput;[m
		inputIdx  += [31mdimInput2;[m[32mdimInput2Total;[m
	    }[m
	}[m
    };[m


    [32m// vectorAggregateForwardInfer is different from vectorAggregateForward[m
[32m    // because previous frames' results must be saved as external data[m
    struct vectorAggregateForwardInfer[m
    {[m
	int [31mdimInput2;[m[32mdimInput2Total;[m // dimension of output of target layer (to be fed back, in total dim)
	int dimInput2Start; // from which dimension of the target to load (may not be 0)[m
	[32mint dim1Band;[m
	[m
	int dimOutput;      // dimension of output of this layer[m
	int dimOutputStart;[m
[31m	int dim1Band;[m
	int bandNum;[m
	[m
	real_t *input2;     // target layer[m
[36m@@ -237,7 +245,7 @@[m [mnamespace {[m
	    //int     inputIdx   = dimInput2Start + dimIdxRel % dim1Band;[m
	    // Modified[m
	    int     outputIdx = startTime * dimOutput + dimOutputStart + dimIdxRel;[m
	    int     inputIdx  = startTime * [31mdimInput2[m[32mdimInput2Total[m + dimInput2Start + dimIdxRel % dim1Band; 
	    [m
	    int     bandIdx   = dimIdxRel / dim1Band;      // which band this dimension is in?	    [m
	    int     preTime   = 0;[m
[36m@@ -296,7 +304,7 @@[m [mnamespace {[m

		    // aggregating the previous frame[m
		    aggreInfo  = ((preTime-boundTime) / (preTime - boundTime + 1.0)) *[m
			aggreInfo + cell_act_fn_t::fn(input2[inputIdx - [31mdimInput2])[m[32mdimInput2Total])[m /
			(preTime - boundTime+1.0);[m
		    [m
		    // propagate the info to the current frame[m
[36m@@ -321,7 +329,7 @@[m [mnamespace {[m
		    aggBuffer[bandNum * dim1Band + dimIdxRel] = boundTime; [m
		}[m
		outputIdx += dimOutput;[m
		inputIdx  += [31mdimInput2;[m[32mdimInput2Total;[m
	    }[m
	}[m
    };[m
[36m@@ -329,7 +337,9 @@[m [mnamespace {[m
    [m
    struct vectorFillBackward[m
    {[m
	int [31mdimInput1;[m[32mdimInput1Start;[m // dimension of the preceding layer
	[32mint dimInput1End;   //[m
[32m	int dimInput1Total;  //[m
	int dimOutput;      // dimension of this layer[m
	[m
	real_t *outputError;[m
[36m@@ -338,9 +348,14 @@[m [mnamespace {[m
	// Dim here is the dimension of the previous layer[m
	__host__ __device__ real_t operator() (const int &outputIdx) const[m
	{[m
	    int timeStep  = outputIdx / [31mdimInput1;[m[32mdimInput1Total;[m
	    int dimIdx    = outputIdx % [31mdimInput1;[m[32mdimInput1Total;[m
[32m	    [m
[32m	    if (dimIdx >= dimInput1Start && dimIdx < dimInput1End){[m
		return outputError[timeStep * dimOutput + [31mdimIdx];[m[32mdimIdx - dimInput1Start];[m
[32m	    }else{[m
[32m		return 0;[m
[32m	    }[m
	}[m
    };[m
    [m
[36m@@ -434,6 +449,19 @@[m [mnamespace layers{[m
	    // default, don't use aggregate[m
	    m_aggOpt.clear(); [m
	}[m

	[32m// configuration for the previous state[m
[32m	m_prevDimEnd    = ((layerChild->HasMember("previousDimEnd")) ? [m
[32m			  ((*layerChild)["previousDimEnd"].GetInt()) : precedingLayer.size());[m
[32m	m_prevDimStart  = ((layerChild->HasMember("previousDimStart")) ? [m
[32m			  ((*layerChild)["previousDimStart"].GetInt()) : 0);[m
[32m	if (m_prevDimStart < 0 || m_prevDimEnd > precedingLayer.size() ||[m
[32m	    m_prevDimStart > m_prevDimEnd)[m
[32m	    throw std::runtime_error("Error in previousDim and previousDimStart configuration");[m

[32m	if (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("layer before feedback is reduced in mem");[m  

    }[m

    template <typename TDevice>[m
[36m@@ -453,6 +481,13 @@[m [mnamespace layers{[m
        (*layersArray)[layersArray->Size() - 1].AddMember("aggregate_cross_boundary", [m
							  m_crossBoundary,[m
							  allocator);[m
	[32mif (m_prevDimStart != 0 || m_prevDimEnd != this->precedingLayer().size()){[m
[32m	    (*layersArray)[layersArray->Size() - 1].AddMember("previousDimStart", [m
[32m							      m_prevDimStart, allocator);[m
[32m	    (*layersArray)[layersArray->Size() - 1].AddMember("previousDimEnd", [m
[32m							      m_prevDimEnd, allocator);[m
[32m	}[m
	
    }[m

    template <typename TDevice>[m
[36m@@ -462,19 +497,20 @@[m [mnamespace layers{[m
	m_targetLayer    = &targetLayer;[m

	// Now, use all target features for feedback[m
	// [31mTo[m[32m[To[m be [31mcompleted[m[32mcompleted][m
	m_targetDimStart = 0;[m
	m_targetDimEnd   = m_targetDim;[m

	// dim * look_back + dim * aggregate + preceding_layer[m
	int dimExpected = ((m_targetDimEnd - m_targetDimStart) * m_lookBack.size() +[m
			   (m_targetDimEnd - m_targetDimStart) * m_aggOpt.size()   +[m
			   [31mthis->precedingLayer().size());[m[32m(m_prevDimEnd - m_prevDimStart));[m
	[m
	if (dimExpected !=this->size()){[m
	    printf("Feedback dim + Feedforward dim = %d\n", dimExpected);[m
	    throw std::runtime_error("Error in network.jsn feedback layer size");[m
	}[m
	
	if (m_targetDimEnd > m_targetDim || m_targetDimStart > m_targetDim ||[m
	    m_targetDimEnd < m_targetDimStart){[m
	    throw std::runtime_error("Error in configuration of targetDimStart, targetDimEnd");[m
[36m@@ -488,6 +524,7 @@[m [mnamespace layers{[m
	[m
	// print information[m
	printf("\nCreating the feedback link:\n");[m
	[32mprintf("\tReading previous layer [%d-%d] dim\n", m_prevDimStart, m_prevDimEnd);[m
	printf("\tFrom %s [%d-%d]", targetLayer.type().c_str(), m_targetDimStart, m_targetDimEnd);[m
	printf("\tLook Back [%s]", m_lookBackStr.c_str());[m
	if (m_aggOpt.size()){[m
[36m@@ -568,11 +605,15 @@[m [mnamespace layers{[m
	    int previousSize  = this->precedingLayer().size();[m
	    [m
	    internal::vectorFillForward fn;[m
	    [31mfn.dimInput1[m[32mfn.dimInput1Start = m_prevDimStart;[m
[32m	    fn.dimInput1End   = m_prevDimEnd;[m
[32m	    fn.dimInput1Valid = m_prevDimEnd - m_prevDimStart;[m
[32m	    fn.dimInput1Total[m = previousSize;     // the dimension from preceding layer
	    [m
[31m	    fn.dimInput2      = m_targetDim;      // the dimension of the output of target layer[m
	    fn.dimInput2Start = m_targetDimStart; // from which dimension to load from target layer[m
	    [31mfn.dim1Step[m[32mfn.dimInput2End   = m_targetDimEnd;   // the dimension of the output of target layer[m
[32m	    fn.dimInput2Valid[m = m_targetDimEnd - m_targetDimStart;
	    [32mfn.dimInput2Total = m_targetDim;[m      // dimension for 1 step
		[m
	    fn.dimOutput      = this->size();     [m
	    fn.parallel       = this->parallelSequences();[m
[36m@@ -598,14 +639,13 @@[m [mnamespace layers{[m
	    // aggregating[m
	    if (m_aggOpt.size()){[m
		internal::vectorAggregateForward fn;[m

	    [m
		[31mfn.dimInput2[m[32mfn.dimInput2Total[m = m_targetDim;      // 
		fn.dimInput2Start = m_targetDimStart; //[m

		fn.dim1Band       = m_targetDimEnd - m_targetDimStart; // dimension for 1 band[m
		
		fn.dimOutput      = this->size();[m
		fn.dimOutputStart = [31m(this->precedingLayer().size()[m[32m((m_prevDimEnd - m_prevDimStart)[m +
				     this->m_lookBack.size() * (m_targetDimEnd - m_targetDimStart));[m
		[m
		fn.input2         = helpers::getRawPointer(m_targetLayer->feedbackOutputs(true));[m
[36m@@ -669,20 +709,23 @@[m [mnamespace layers{[m
	    // Concatenate the feature vector [m
	    // (by treating the 1 dimensional softmax Index as a normal feature)[m
	    internal::vectorFillForward fn;[m
	    [32mfn.dimInput1Start = m_prevDimStart;[m
[32m	    fn.dimInput1End   = m_prevDimEnd;[m
[32m	    fn.dimInput1Valid = m_prevDimEnd - m_prevDimStart;[m
[32m	    fn.dimInput1Total = previousSize;     // the dimension from preceding layer[m
	    [m
	    [31mfn.dimInput1[m[32mfn.dimInput2Start[m = [31mpreviousSize;[m
[31m	    fn.dimInput2[m[32mm_targetDimStart; // from which dimension to load from target layer[m
[32m	    fn.dimInput2End   = m_targetDimEnd;   // the dimension of the output of target layer[m
[32m	    fn.dimInput2Valid = m_targetDimEnd - m_targetDimStart;[m
[32m	    fn.dimInput2Total[m = m_targetDim;      [32m// dimension for 1 step[m
		
	    fn.dimOutput      = this->size();     
	    fn.parallel       = this->parallelSequences();[m
[31mfn.dimInput2Start = m_targetDimStart;[m	    
	    fn.input1         = helpers::getRawPointer(this->precedingLayer().outputs());[m
	    fn.input2         = helpers::getRawPointer(m_targetLayer->feedbackOutputs(false));[m
	    fn.output         = helpers::getRawPointer(this->outputs());[m
[31mfn.dim1Step       = m_targetDimEnd - m_targetDimStart; // dimension for 1 step[m	    
	    fn.lookBack       = helpers::getRawPointer(this->m_lookBack);[m

	    fn.lookBackStepNM = this->m_lookBack.size();[m
[36m@@ -710,10 +753,10 @@[m [mnamespace layers{[m
		    thrust::fill(this->m_aggBuffer.begin(), this->m_aggBuffer.end(), 0.0);[m
		[m
		internal::vectorAggregateForwardInfer fn;[m
		[31mfn.dimInput2[m[32mfn.dimInput2Total[m = m_targetDim;      // 
		fn.dimInput2Start = m_targetDimStart; //[m

		fn.dim1Band       = m_targetDimEnd - m_targetDimStart; // dimension for 1 band[m
		
		fn.dimOutput      = this->size();[m
		fn.dimOutputStart = (this->precedingLayer().size() +[m
				     this->m_lookBack.size() * (m_targetDimEnd - m_targetDimStart));[m
[36m@@ -744,25 +787,27 @@[m [mnamespace layers{[m
		int previousSize  = this->precedingLayer().size();[m
		[m
		internal::vectorFillForward fn;[m
		[32mfn.dimInput1Start = m_prevDimStart;[m
[32m		fn.dimInput1End   = m_prevDimEnd;[m
[32m		fn.dimInput1Valid = m_prevDimEnd - m_prevDimStart;[m
[32m		fn.dimInput1Total = previousSize;[m     
		[m
		[31mfn.dimInput1[m[32mfn.dimInput2Start[m = [31mpreviousSize;[m
[31m		fn.dimInput2[m[32mm_targetDimStart; [m
[32m		fn.dimInput2End   = m_targetDimEnd;   [m
[32m		fn.dimInput2Valid = m_targetDimEnd - m_targetDimStart;[m
[32m		fn.dimInput2Total[m = m_targetDim;      

		fn.dimOutput      = this->size();[m
		fn.parallel       = this->parallelSequences();[31mfn.dimInput2Start = m_targetDimStart;[m		

		fn.input1         = helpers::getRawPointer(this->precedingLayer().outputs());[m
		fn.input2         = helpers::getRawPointer(m_targetLayer->feedbackOutputs(false));[m
		fn.output         = helpers::getRawPointer(this->outputs());[m

[31m		fn.dim1Step       = m_targetDimEnd - m_targetDimStart; // dimension for 1 step[m
		fn.crossBoundary  = m_crossBoundary;[m
		
		Cpu::int_vector  tmp(2,1);[m
		int_vector       tmpGPU = tmp;[m
		fn.lookBack       = helpers::getRawPointer(tmpGPU);[m

		fn.lookBackStepNM = m_aggOpt.size();[m
		[m
		thrust::for_each([m
[36m@@ -785,10 +830,16 @@[m [mnamespace layers{[m
    template <typename TDevice>[m
    void FeedBackLayer<TDevice>::computeBackwardPass(const int nnState)[m
    {[m
	[31m{{[m[32mif (m_prevDimEnd == m_prevDimStart){[m
[32m	    // previous layer is not used at all[m
[32m	    [m
[32m	}else{[m
	    
	   // Copy the gradient for the preceding layer[m
	   internal::vectorFillBackward fn;[m
	   [31mfn.dimInput1[m[32mfn.dimInput1Start = m_prevDimStart;[m
[32m	   fn.dimInput1End   = m_prevDimEnd;[m
[32m	   fn.dimInput1Total[m = this->precedingLayer().size();
	   fn.dimOutput      = this->size();[m
	   fn.outputError    = helpers::getRawPointer(this->outputErrors());[m

[36m@@ -799,7 +850,8 @@[m [mnamespace layers{[m
			     thrust::counting_iterator<int>(0)+n,[m
			     this->precedingLayer().outputErrors().begin(),[m
			     fn);	   [m
	
	[31m}}[m[32m}[m
    }[m
    [m
    template class FeedBackLayer<Cpu>;[m
[1mdiff --git a/layers/FeedBackLayer.hpp b/layers/FeedBackLayer.hpp[m
[1mindex 48a2ec4..5f480b4 100644[m
[1m--- a/layers/FeedBackLayer.hpp[m
[1m+++ b/layers/FeedBackLayer.hpp[m
[36m@@ -56,15 +56,18 @@[m [mnamespace layers {[m
	// look back and aggregation[m
	std::string     m_aggStr;         //[m
	int_vector      m_aggOpt;         //[m
	[32mreal_vector     m_aggBuffer;[m
[32m	int             m_crossBoundary;  //[m
[32m	int             m_aggOptSyn;[m
	// Fatal Error: [m
	// pattype is char, which can only be -128 -> 128[m
	// pattype_vector  m_boundaryInfo;   // buffer to store the boundary information[m
	int_vector      m_boundaryInfo;[m

	[32m// configuration of the previous layer[m
[32m	int             m_prevDimStart;[m
[32m	int             m_prevDimEnd;[m
	[m
[31m	real_vector     m_aggBuffer;[m
[31m	int             m_crossBoundary;  //[m
[31m	int             m_aggOptSyn;[m
    public:[m
	[m
	FeedBackLayer([m
[1mdiff --git a/layers/FeedForwardLayer.cu b/layers/FeedForwardLayer.cu[m
[1mindex c29af23..bc301ac 100644[m
[1m--- a/layers/FeedForwardLayer.cu[m
[1m+++ b/layers/FeedForwardLayer.cu[m
[36m@@ -278,7 +278,7 @@[m [mnamespace {[m
namespace layers {[m

    // Additional weight due to batch normalization[m
    int [31mwNum(const[m[32mweightForBatchNorm(const[m helpers::JsonValue &layerChild){
	if (layerChild->HasMember("batchnorm") && ((*layerChild)["batchnorm"].GetInt()))[m
	    return 3; // alpha, mean, std[m
	else[m
[36m@@ -289,15 +289,16 @@[m [mnamespace layers {[m
    FeedForwardLayer<TDevice, TActFn>::FeedForwardLayer(const helpers::JsonValue &layerChild, [m
							const helpers::JsonValue &weightsSection, [m
							Layer<TDevice> &precedingLayer)[m
        : TrainableLayer<TDevice>(layerChild, weightsSection, 1, [31mwNum(layerChild),[m[32mweightForBatchNorm(layerChild),[m
				  precedingLayer)
    {[m
	
	[32m// Initialization for batch normalization[m
	m_batchNorm = [31m(wNum(layerChild)>0)?[m[32m(weightForBatchNorm(layerChild)>0)?[m true : false;
	[m
	if (m_batchNorm){[m
	    // initialization[m
	    m_stdConst  = 0.001; m_batchCnt  = 0.0; m_preEpoch  = 1;
	    [m
	    // mean, std[m
	    Cpu::real_vector tmp;[m
[36m@@ -314,7 +315,6 @@[m [mnamespace layers {[m
		    [m
	    if (weightsSection.isValid() && weightsSection->HasMember(this->name().c_str())) {[m
		// read [m
[31m	    [m
	    }else{[m
		// initialize [m
		int transMatrixWeightNum = this->size() * this->precedingLayer().size();[m
[36m@@ -323,15 +323,16 @@[m [mnamespace layers {[m
		thrust::fill(this->weights().begin() + transMatrixWeightNum,[m
			     this->weights().begin() + transMatrixWeightNum + this->size(),[m
			     BATCHNORM_GAMMA_INITIAL);[m
		
		// beta, mean, std[m
		thrust::fill(this->weights().begin() + transMatrixWeightNum + this->size(),[m
			     this->weights().end(),[m
			     0.0);[m
	    }[m
	    [31mconst[m[32m//const[m Configuration &config = Configuration::instance();
	    [31mm_trainFlag[m[32m//m_trainFlag[m = config.trainingMode();
	}[m
	
    }[m

    template <typename TDevice, typename TActFn>[m
[36m@@ -408,7 +409,8 @@[m [mnamespace layers {[m
	    int transMatrixWeightNum = this->size() * this->precedingLayer().size();[m
	    [m
	    // Re-initialize the batch mean and variance[m
	    if [31m(m_trainFlag[m[32m(this->flagTrainingMode()[m && m_preEpoch > 0 &&
		m_preEpoch != this->getCurrTrainingEpoch()){
		// always update the mean, std for each epoch[m
		m_batchCnt = 0;[m
		thrust::fill(this->weights().begin() + transMatrixWeightNum + 2 * this->size(),[m
[36m@@ -505,7 +507,7 @@[m [mnamespace layers {[m
		fn3);[m

	       // Step4. accumulate the mean and std, for generation stage[m
	       if [31m(m_trainFlag){[m[32m(this->flagTrainingMode()){[m
		   internal::AveMeanStd fn5;[m
		   fn5.meanStdBuf = (helpers::getRawPointer(this->weights()) +[m
				     transMatrixWeightNum + this->size() * 2);[m
[36m@@ -530,7 +532,7 @@[m [mnamespace layers {[m
	       fn2.meanStd   = helpers::getRawPointer(this->m_stats);[m
	       fn2.meanStdBuf= (helpers::getRawPointer(this->weights()) +[m
				transMatrixWeightNum + this->size() * 2);[m
	       fn2.trainFlag = [31mm_trainFlag;[m[32mthis->flagTrainingMode();[m
	   [m
	       thrust::for_each([m
		thrust::make_zip_iterator([m
[36m@@ -555,8 +557,16 @@[m [mnamespace layers {[m
	if (m_batchNorm){[m
	    throw std::runtime_error("Error: batchnorm not available for online processing");[m
	}[m

	[32mint effTimeStart = timeStep * this->parallelSequences();[m
[32m	int effTimeEnd   = (timeStep+1) * this->parallelSequences();[m
[32m	[m
[32m	// Pointer to the output of previous layer (input buffer)[m
[32m	int shiftIn  = this->precedingLayer().outputBufPtrBias(timeStep * this->parallelSequences(),[m
[32m							       nnState);[m
[32m	// Pointer to the output of this layer[m
[32m	int shiftOut = this->outputBufPtrBias(timeStep * this->parallelSequences(), nnState);[m
	[m
[31m	int effTimeStep = timeStep * this->parallelSequences();[m
	// collect outputs from preceding layer[m
        {{[m
            helpers::Matrix<TDevice> weightsMatrix  (&this->weights(),                  [m
[36m@@ -566,12 +576,14 @@[m [mnamespace layers {[m
            helpers::Matrix<TDevice> plOutputsMatrix(&this->precedingLayer().outputs(), [m
						     this->precedingLayer().size(), [m
						     this->parallelSequences(),[m
						     [31meffTimeStep[m[32m(effTimeStart[m * [31mthis->precedingLayer().size());[m[32mthis->precedingLayer().size()[m
[32m						      - shiftIn));[m

            helpers::Matrix<TDevice> outputsMatrix  (&this->_outputs(),                 [m
						     this->size(), [m
						     this->parallelSequences(),[m
						     [31meffTimeStep[m[32m(effTimeStart[m * [31mthis->size());[m[32mthis->size()[m
[32m						      - shiftOut));[m

            outputsMatrix.assignProduct(weightsMatrix, true, plOutputsMatrix, false);[m
        }}[m
[36m@@ -585,10 +597,10 @@[m [mnamespace layers {[m
				   this->size() * this->precedingLayer().size());[m

            thrust::transform([m
		this->_outputs().begin() + [31meffTimeStep[m[32meffTimeStart[m * [31mthis->size(),[m[32mthis->size() - shiftOut,[m
		this->_outputs().begin() + [31m(effTimeStep+this->parallelSequences())[m[32meffTimeEnd[m   * [31mthis->size(),[m[32mthis->size() - shiftOut,[m
		thrust::counting_iterator<int>(0),[m
		this->_outputs().begin() + [31meffTimeStep[m[32meffTimeStart[m * [31mthis->size(),[m[32mthis->size() - shiftOut,[m
		fn);[m
        }}[m
    }[m
[36m@@ -791,6 +803,30 @@[m [mnamespace layers {[m
    }[m


    [32mtemplate <typename TDevice, typename TActFn>[m
[32m    void FeedForwardLayer<TDevice, TActFn>::reduceOutputBuffer()[m
[32m    {[m
[32m	// only for no-batch-normalized module[m
[32m	if (m_batchNorm){[m
[32m	    // do thing[m
[32m	}else{[m
[32m	    this->resizeOutputBuffer(this->parallelSequences() * this->size());[m
[32m	    this->setSaveMemoryFlag(true);[m
[32m	    printf("\t[mem saved]");[m
[32m	}[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice, typename TActFn>[m
[32m    int FeedForwardLayer<TDevice, TActFn>::outputBufPtrBias(const int timeStepTimesParallel,[m
[32m							    const int nnState)[m
[32m    {[m
[32m	if (this->getSaveMemoryFlag()){[m
[32m	    return timeStepTimesParallel * this->size();[m
[32m	}else{[m
[32m	    return 0;[m
[32m	}[m
[32m    }[m	

    // explicit template instantiations[m
    template class FeedForwardLayer<Cpu, activation_functions::Tanh>;[m
    template class FeedForwardLayer<Gpu, activation_functions::Tanh>;[m
[1mdiff --git a/layers/FeedForwardLayer.hpp b/layers/FeedForwardLayer.hpp[m
[1mindex 58db2da..057c9b4 100644[m
[1m--- a/layers/FeedForwardLayer.hpp[m
[1m+++ b/layers/FeedForwardLayer.hpp[m
[36m@@ -48,7 +48,7 @@[m [mnamespace layers {[m
	[m
	real_t      m_stdConst;      // const floor for the var[m
	real_t      m_batchCnt;[m
	[32m//[m bool        m_trainFlag;  [32m// replaced by m_flagTraining[m
	int         m_preEpoch;[m
	real_t      m_batchSize;     //[m
[m
[36m@@ -100,6 +100,13 @@[m [mnamespace layers {[m
	virtual void exportLayer(const helpers::JsonValue &layersArray, [m
				 const helpers::JsonAllocator &allocator) const;[m
[m
	[32m/*[m
[32m	 * to optimize the memory usage[m
[32m	 */[m
[32m	virtual void reduceOutputBuffer();[m
[32m[m
[32m	virtual int outputBufPtrBias(const int timeStepTimesParallel, const int nnState);[m
	
    };[m
[m
} // namespace layers[m
[1mdiff --git a/layers/InputLayer.cpp b/layers/InputLayer.cpp[m
[1mindex 666fd50..ad9ae9d 100644[m
[1m--- a/layers/InputLayer.cpp[m
[1m+++ b/layers/InputLayer.cpp[m
[36m@@ -48,27 +48,7 @@[m
namespace internal {[m
namespace {[m

    [31mstruct ReadInput[m
[31m    {[m
[31m	const real_t *sourceW;[m
[31m	real_t       *targetW;[m
[31m	const real_t *index;[m
[31m	int           sourceDim;[m
[31m	int           targetDim;[m
[31m	int           startDim;[m
[31m	[m
[31m	__host__ __device__ void operator() (const int idx) const[m
[31m	{[m
[31m	    int dim  = (idx % targetDim);[m
[31m	    int time = (idx / targetDim);[m
[31m	    int sourcePos = index[time] * sourceDim + dim;[m
[31m	    int targetPos = time * targetDim + dim + startDim;[m
[31m	    *(targetW + targetPos) = *(sourceW + sourcePos);[m
[31m	}[m
[31m    };[m[32m// Block20170904x02[m
}[m
}[m

[36m@@ -79,24 +59,15 @@[m [mnamespace layers {[m
    InputLayer<TDevice>::InputLayer(const helpers::JsonValue &layerChild,[m
				    int parallelSequences,[m
				    int maxSeqLength)[m
        : Layer<TDevice>(layerChild, parallelSequences, [31mmaxSeqLength)[m[32mmaxSeqLength,[m
[32m			 Configuration::instance().trainingMode())[m
	, m_weDim(-1)[m
	, m_flagWeUpdate(false)[m
    {[m
	[32m//[m m_weBufferInput.resize(parallelSequences*maxSeqLength*this->size(), 0.0);
	m_weMask.clear();[m
	m_weMaskFlag = false;[m

[31m	const Configuration &config = Configuration::instance();[m
[31m	if (config.exInputDir().size()){[m
[31m	    ParseIntOpt(config.exInputDim(), m_extInputDim);[m
[31m	    ParseStrOpt(config.exInputDir(), m_extInputDir);[m
[31m	    ParseStrOpt(config.exInputExt(), m_extInputExt);[m
[31m	}else{[m
[31m	    m_extInputDir.clear();[m
[31m	    m_extInputDim.clear();[m
[31m	    m_extInputExt.clear();[m
[31m	}[m
    }[m

    template <typename TDevice>[m
[36m@@ -121,11 +92,9 @@[m [mnamespace layers {[m
		throw std::runtime_error("WE dimension is larger than input data dimension");[m
	    }[m
	    if (this->size() != fraction.inputPatternSize()-1+m_weDim){[m
		[32mprintf("Input layer size should be: %d", fraction.inputPatternSize() -1 + m_weDim);[m
		throw std::runtime_error("Input's dimension -1 + weDim != input layer size");[m
	    }[m
[31m	}else if (m_extInputDir.size()){[m
[31m	    // no need to check again[m
[31m	    [m
	}else{[m
	    if (fraction.inputPatternSize() != this->size())[m
		throw std::runtime_error([m
[36m@@ -138,54 +107,29 @@[m [mnamespace layers {[m
        Layer<TDevice>::loadSequences(fraction, nnState);[m
	[m
	/* Add 16-02-22 Wang: for WE updating */[m
	[32m// Original code of CURRENNT, just copy the input data from fraction to this->outputs()[m
	// thrust::copy(fraction.inputs().begin(),fraction.inputs().end(),this->_outputs().begin());[m

[31m	// Three possible cases[m
[31m	// 1. external word vectors [m
[31m	// 2. external input features[m
[31m	// 3. normal input[m
	if (m_flagWeUpdate){[m
	    // [31mload in the[m[32mwhen[m embedded vectors [31mfrom weBank[m[32mare used[m

[32m	    // Before loadSequences(), readWeBank() should have been called[m
	    int weidx=0;[m
	    long unsigned int bias=0;[m
	    long unsigned int fracTime=(fraction.inputs().size()/fraction.inputPatternSize());[m
	    if [31m(fracTime>m_weIdx.size()){[m[32m(fracTime>m_weIdx.size())[m
		throw std::runtime_error("m_weIdx size is smaller than fracTime\n");[m
[31m	    }[m
	    thrust::fill(m_weIdx.begin(), m_weIdx.end(), -1);[m
	    [m
	    [31m/* Code based on Thrust parallel */[m
[31m	    /*{{[m
[31m		internal::ReadInput fn;[m
[31m		fn.sourceW   = helpers::getRawPointer(fraction.inputs());[m
[31m		fn.targetW   = helpers::getRawPointer(m_weBufferInput);[m
[31m		fn.weBank    = helpers::getRawPointer(m_weBank);[m
[31m		fn.sourceDim = fraction.inputs().size()/fracTime;[m
[31m		fn.targetDim = this->size();[m
[31m		fn.weDim     = m_weDim;[m
[31m		fn.weIdxDim  = m_weIDDim;[m
[31m		[m
[31m		int n = fracTime * this->size();[m
[31m		thrust::for_each(thrust::host, [m
[31m				 thrust::counting_iterator<int> (0),[m
[31m				 thrust::counting_iterator<int> (0)+n,[m
[31m				 fn);[m
[31m		[m
[31m		thrust::copy(m_weBufferInput.begin(),[m
[31m			     m_weBufferInput.begin()+n,[m
[31m			     this->_outputs().begin());[m
[31m	    }}[m
[31m	    */[m[32m// Block20170904x01[m
	    
	    //Code based on CPU sequential method[m
	    Cpu::real_vector tempInput;[m
	    tempInput.resize(this->size(), 0.0);[m
	    for (int [31mi=0; i<fracTime;[m[32mi = 0; i < fracTime;[m i++){
		bias = [31mi*fraction.inputPatternSize();[m[32mi * fraction.inputPatternSize();[m
		[m
		// copy the [31moriginal[m[32mnormal[m input data
		thrust::copy(fraction.inputs().begin()+bias, [m
			     fraction.inputs().begin()+bias+fraction.inputPatternSize(), [m
			     tempInput.begin());[m
[36m@@ -193,20 +137,19 @@[m [mnamespace layers {[m
		// retrieve the embedded vector idx and save m_weIdx[m
		weidx = (long unsigned int)(fraction.inputs()[i * fraction.inputPatternSize() + [m
							      m_weIDDim]);[m
		if [31m(weidx*m_weDim>m_weBank.size()){[m[32m(weidx * m_weDim > m_weBank.size()){[m
		    printf("Vector idx: %d\t", weidx);[m
		    throw std::runtime_error("vector idx larger than weBank size");[m
		}[m
		// [32mstore[m the [31mnumber[m[32midx[m of [31mvalid m_weIdx is always equal to fracTime[m
[31m		m_weIdx[i]=weidx;[m[32membedded vector[m
[32m		m_weIdx[i] = weidx;[m
		[m
		// [31mcopy[m[32mretrieve[m the [31mwe data into the input data (output of the InputLayer)[m[32membedded vector from m_weBank[m
		thrust::copy(m_weBank.begin()  + weidx     * m_weDim, [m
			     m_weBank.begin()  + (weidx+1) * m_weDim, [m
			     tempInput.begin() + fraction.inputPatternSize()  - 1);

		// [31mAdd 0902:[m[32mOptinal:[m add noise to the input
		// Note: this is different from the input_noise_sigma[m
		//       here, the noise will be added every epoch[m
		if (this->m_weNoiseStartDim > 0){		    [m
[36m@@ -237,10 +180,11 @@[m [mnamespace layers {[m
		std::cout << tempVec.size() << std::endl;[m
	    }[m

	[31m}else{[m[32m}[m
[32m	else[m
[32m	{[m
	    // [31mif no we is utilized, just copy the input[m[32mNormal case[m
	    thrust::copy(fraction.inputs().begin(), fraction.inputs().end(),
			 this->_outputs().begin());[m
	}[m
    }[m
[1mdiff --git a/layers/InputLayer.hpp b/layers/InputLayer.hpp[m
[1mindex da4e3a1..45d8476 100644[m
[1m--- a/layers/InputLayer.hpp[m
[1m+++ b/layers/InputLayer.hpp[m
[36m@@ -43,10 +43,11 @@[m [mnamespace layers {[m
	/* The vector to store the readWeBank */[m
	Cpu::real_vector  m_weBank;[m
	Cpu::real_vector  m_weIdx;[m
[31m	Cpu::real_vector  m_weBufferInput;[m
	unsigned int      m_weDim;[m
	unsigned int      m_weIDDim;[m
	bool              m_flagWeUpdate;[m
	[32m//Cpu::real_vector  m_weBufferInput;[m
	
	/* Add 17/01/29 */[m
	Cpu::real_vector  m_weMask;[m
	bool              m_weMaskFlag;[m
[36m@@ -57,13 +58,7 @@[m [mnamespace layers {[m
	int               m_weNoiseStartDim;[m
	int               m_weNoiseEndDim;[m
	real_t            m_weNoiseDev;[m
[31m/* Add 170326 Add external input*/[m
[31m	Cpu::int_vector   m_extInputDim;[m
[31m	std::vector<std::string>       m_extInputDir;[m
[31m	std::vector<std::string>       m_extInputExt;[m		
    public:[m
        /**[m
         * Constructs the Layer[m
[1mdiff --git a/layers/Layer.cpp b/layers/Layer.cpp[m
[1mindex 8e2ce59..bf1bb1c 100644[m
[1m--- a/layers/Layer.cpp[m
[1m+++ b/layers/Layer.cpp[m
[36m@@ -54,8 +54,9 @@[m [mnamespace layers {[m
    [m

    template <typename TDevice>[m
    Layer<TDevice>::Layer(const helpers::JsonValue &layerChild,
			  int parallelSequences,   int maxSeqLength,
			  bool [32mflagTrainingMode, bool[m createOutputs)
        : m_name             (layerChild->HasMember("name") ? [m
			      (*layerChild)["name"].GetString()  : "")[m
        , m_size             (layerChild->HasMember("size") ? [m
[36m@@ -66,6 +67,8 @@[m [mnamespace layers {[m
        , m_curMinSeqLength  (0)[m
        , m_curNumSeqs       (0)[m
	, m_InputWeUpdate    (false)[m
	[32m, m_flagTrainingMode (true)[m
[32m	, m_flagSaveOutputMemory (false)[m
    {[m
        // check if the name and size values exist[m
        if (!layerChild->HasMember("name"))[m
[36m@@ -75,55 +78,25 @@[m [mnamespace layers {[m
        if (!layerChild->HasMember("size"))[m
            throw std::runtime_error(std::string("Missing value 'size' in layer '") + m_name + "'");[m

        // allocate [31mspace[m[32mmemory[m for [31mthe vectors[m[32moutput[m
        if (createOutputs)[m
            m_outputs = Cpu::real_vector(m_parallelSequences * m_maxSeqLength * m_size);[m
	
	[32m// allocate memory for time mark[m
        m_patTypes = Cpu::pattype_vector(m_parallelSequences * m_maxSeqLength);[m
	[m
        [32m// allocate memory for gradients buffer[m
[32m	if (flagTrainingMode)[m
[32m	    m_outputErrors  = Cpu::real_vector(this->_outputs().size(), (real_t)0);    [m
[32m	else[m
[32m	    m_outputErrors.clear();[m
[32m	m_outputErrorsCopy.clear();[m
	[m
	// [31mresize[m[32minitialize[m the [31moutput errors vector[m
[31m        m_outputErrors = Cpu::real_vector(this->_outputs().size(), (real_t)0);[m
[31m	m_outputErrorsCopy = Cpu::real_vector(this->_outputs().size(), (real_t)0);[m

[31m	//[m[32mtraining epoch counter[m
	m_currTrainingEpoch = -1;[m
[31m    }[m

[31m/* Not fully implemented */[m	// [31moverload for CNN[m
[31m    template <typename TDevice>[m
[31m    Layer<TDevice>::Layer(const helpers::JsonValue &layerChild, [m
[31m			  int parallelSequences, int maxSeqLength, [m
[31m			  int outputSize,        bool createOutputs)[m
[31m        : m_name             (layerChild->HasMember("name") ? [m
[31m			      (*layerChild)["name"].GetString()  : "")[m
[31m        , m_size             (outputSize)[m
[31m        , m_parallelSequences(parallelSequences)[m
[31m        , m_maxSeqLength     (maxSeqLength)[m
[31m        , m_curMaxSeqLength  (0)[m
[31m        , m_curMinSeqLength  (0)[m
[31m        , m_curNumSeqs       (0)[m
[31m	, m_InputWeUpdate    (false)[m
[31m    {[m
[31m        // check if[m[32mset[m the [31mname and size values exist[m
[31m        if (!layerChild->HasMember("name"))[m
[31m            throw std::runtime_error("Missing value 'name' in layer description");[m
[31m        if (m_name.empty())[m
[31m            throw std::runtime_error("Empty layer name in layer description");[m
[31m        if (!layerChild->HasMember("size"))[m
[31m            throw std::runtime_error(std::string("Missing value 'size' in layer '") + m_name + "'");[m

[31m        // allocate space for the vectors[m
[31m        if (createOutputs)[m
[31m            m_outputs = Cpu::real_vector(m_parallelSequences * m_maxSeqLength * m_size);[m

[31m        m_patTypes[m[32mflag[m
[32m	m_flagTrainingMode[m  = [31mCpu::pattype_vector(m_parallelSequences * m_maxSeqLength);[m
[31m	[m
[31m	[m
[31m        // resize the output errors vector[m
[31m        m_outputErrors = Cpu::real_vector(this->_outputs().size(), (real_t)0);[m
[31m	m_outputErrorsCopy = Cpu::real_vector(this->_outputs().size(), (real_t)0);[m[32m(flagTrainingMode ? true : false);[m
    }[m

    template <typename TDevice>[m
[36m@@ -194,12 +167,15 @@[m [mnamespace layers {[m
    template <typename TDevice>[m
    Cpu::real_vector& Layer<TDevice>::outputErrorsCpu()[m
    {[m
	[32mif (m_outputErrorsCopy.size() != m_outputErrors.size())[m
[32m	    m_outputErrorsCopy = m_outputErrors;[m
	thrust::copy(m_outputErrors.begin(), m_outputErrors.end(), m_outputErrorsCopy.begin());[m
        return m_outputErrorsCopy;[m
    }[m
    [m
    template <typename TDevice>[m
    void Layer<TDevice>::loadSequences(const data_sets::DataSetFraction &fraction,
				       const int nnState)
    {[m
        m_curMaxSeqLength = fraction.maxSeqLength();[m
        m_curMinSeqLength = fraction.minSeqLength();[m
[36m@@ -288,6 +264,74 @@[m [mnamespace layers {[m
	//thrust::fill(m_outputErrors.begin(), m_outputErrors.end(), 0.0);[m
    }[m

    [32mtemplate <typename TDevice>[m
[32m    int Layer<TDevice>::hiddenStateSize()[m
[32m    {[m
[32m	return 0;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void Layer<TDevice>::retrieveHiddenState(const int timeStep, real_vector& readBuffer)[m
[32m    {	[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void Layer<TDevice>::setHiddenState(const int timeStep, real_vector& writeBuffer)[m
[32m    {	[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    bool Layer<TDevice>::flagTrainingMode() const[m
[32m    {[m
[32m	return m_flagTrainingMode;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void Layer<TDevice>::clearOutputBuffer()[m
[32m    {[m
[32m	m_outputs.clear();[m
[32m	m_outputs.shrink_to_fit();[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void Layer<TDevice>::resizeOutputBuffer(const int bufferSize)[m
[32m    {[m
[32m	this->clearOutputBuffer();[m
[32m	m_outputs = Cpu::real_vector(bufferSize);[m
[32m	// Can't use m_outputs.resize() because:[m
[32m	// Not really. The extent of Thrust's CUDA support for pure C++ code is[m
[32m	// the bare minimum to allow device_vector to be constructed and[m
[32m	// destroyed for POD types. That's why thrust::fill has a pure C++ [m
[32m	// implementation even for the CUDA backend -- so it can be called by [m
[32m	// device_vector's constructor.[m
[32m	// https://groups.google.com/forum/#!topic/thrust-users/abVI3htMrkw[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void Layer<TDevice>::reduceOutputBuffer()[m
[32m    {[m
[32m	// default: do nothing[m
[32m    }[m

[32m    [m
[32m    template <typename TDevice>[m
[32m    int Layer<TDevice>::outputBufPtrBias(const int timeStepTimesParallel, const int nnState)[m
[32m    {[m
[32m	// don't shift[m
[32m	return 0;[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void Layer<TDevice>::setSaveMemoryFlag(const bool newFlag)[m
[32m    {[m
[32m	m_flagSaveOutputMemory = newFlag;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    bool Layer<TDevice>::getSaveMemoryFlag() const[m
[32m    {[m
[32m	return m_flagSaveOutputMemory;[m
[32m    }[m
    [m
    // explicit template instantiations[m
    template class Layer<Cpu>;[m
[1mdiff --git a/layers/Layer.hpp b/layers/Layer.hpp[m
[1mindex 9e4ce10..0044305 100644[m
[1m--- a/layers/Layer.hpp[m
[1m+++ b/layers/Layer.hpp[m
[36m@@ -42,7 +42,8 @@[m [mnamespace layers {[m
    {[m
        typedef typename TDevice::real_vector    real_vector;[m
        typedef typename TDevice::pattype_vector pattype_vector;[m
	typedef typename Cpu::real_vector        cpu_real_vector;
	
    private:[m
        const std::string m_name;[m
        const int         m_size;[m
[36m@@ -66,7 +67,12 @@[m [mnamespace layers {[m
	/* Add 16-09-28 Wang: the current training epoch */[m
	int               m_currTrainingEpoch; // epoch number [m
	int               m_currTrainingFrac;  // frac number in each epoch[m

	
	[32mbool              m_flagTrainingMode;[m
[32m	/* Add 17-09-06 Wang: for optimizing the memory usage during generation */[m
[32m	bool              m_flagSaveOutputMemory;[m
	
    protected:[m
        real_vector& _outputs();[m
	[m
[36m@@ -86,13 +92,9 @@[m [mnamespace layers {[m
         * @param maxSeqLength      The maximum length of a sequence[m
         * @param createOutputs     If false, then the outputs vector will be left empty[m
         */[m
        Layer(const helpers::JsonValue &layerChild, int parallelSequences, int maxSeqLength,
	      bool [32mflagTrainingMode, bool[m createOutputs = true);
	[m
[31m	// overload for CNN[m
[31m	Layer(const helpers::JsonValue &layerChild, int parallelSequences, int maxSeqLength, [m
[31m	      int outputSize, bool createOutputs = true);[m
[31m[m
        /**[m
         * Destructs the Layer[m
         */[m
[36m@@ -234,9 +236,9 @@[m [mnamespace layers {[m
	virtual const std::string& layerAddInfor(const int opt) const;[m
	[m
	/*[m
	 * Add 1221,[31m*[m prepareStepGeneration: 
	 [32m*[m  prepare the layer for generating the timeStep-th frame
	 *[31mcomputeForwardPass: compute the output for the timeStep-th frame[m  
	 */[m
	virtual void prepareStepGeneration(const int timeStep);[m
	[m
[36m@@ -245,6 +247,38 @@[m [mnamespace layers {[m
	virtual real_vector& feedbackOutputs(const bool flagTrain);[m
[m
	virtual void cleanGradidents();[m


	[32m/*[m
[32m	 * Layer state[m
[32m	 */[m
[32m	// return the number of elements in internal state[m
[32m	virtual int hiddenStateSize();[m
[32m	[m
[32m	// retreve the hidden state[m
[32m	virtual void retrieveHiddenState(const int timeStep, real_vector& readBuffer);[m
[32m[m
[32m	// set the hidden state[m
[32m	virtual void setHiddenState(const int timeStep, real_vector& writeBuffer);[m
[32m[m
[32m	[m
[32m	/*[m
[32m	 * To optimize the memory usage in compuuteForwardPass(const int timeStep)[m
[32m	 */[m
[32m	bool flagTrainingMode() const;[m
[32m	[m
[32m	void clearOutputBuffer();[m
[32m[m
[32m	virtual void reduceOutputBuffer();[m
[32m[m
[32m	void resizeOutputBuffer(const int bufferSize);[m
[32m[m
[32m	virtual int  outputBufPtrBias(const int timeStepTimesParallel, const int nnState);[m
[32m[m
[32m	void setSaveMemoryFlag(const bool newFlag);[m
[32m[m
[32m	bool getSaveMemoryFlag() const;[m
	
    };[m
[m
} // namespace layers[m
[1mdiff --git a/layers/Layer.hpp~ b/layers/Layer.hpp~[m
[1mindex 66db407..054af47 100644[m
[1m--- a/layers/Layer.hpp~[m
[1m+++ b/layers/Layer.hpp~[m
[36m@@ -57,9 +57,18 @@[m [mnamespace layers {[m
        pattype_vector    m_patTypes;[m
	[m
	/* Add 16-02-22 Wang: for WE updating */[m
	bool              m_InputWeUpdate;     // the whether layer is the input [32mlayer[m with [31mwe[m[32mWE [m
[32m	                                       //[m to be [31mupdated?[m[32mupdated ?[m
[32m	                                       // We can not define it in Trainablelayer [m
[32m	                                       // because input layer only inherits from layer[m
	cpu_real_vector   m_outputErrorsCopy;  // make a CPU copy
[m
	[32m/* Add 16-09-28 Wang: the current training epoch */[m
[32m	int               m_currTrainingEpoch; // epoch number [m
[32m	int               m_currTrainingFrac;  // frac number in each epoch[m
[32m[m
[32m	bool              m_flagTrainingMode;[m
	
    protected:[m
        real_vector& _outputs();[m
	[m
[36m@@ -75,12 +84,13 @@[m [mnamespace layers {[m
         * Constructs the Layer[m
         *[m
         * @param layerChild        The layer child of the JSON configuration for this layer[m
         * @param parallelSequences The maximum number of sequences[31mthat shall be[m  computed in parallel
         * @param maxSeqLength      The maximum length of a sequence[m
         * @param createOutputs     If false, then the outputs vector will be left empty[m
         */[m
        Layer(const helpers::JsonValue &layerChild, int parallelSequences, int maxSeqLength,
	      bool [32mcreateOutputErrors, bool[m createOutputs = true);
	
        /**[m
         * Destructs the Layer[m
         */[m
[36m@@ -98,7 +108,8 @@[m [mnamespace layers {[m
         * [m
         * @return The number of blocks in the layer[m
         */[m
	[32m// modify 0612: to virtual size so that LstmLayerChaW can be supported[m
[32m        virtual[m int size() const;
[m
        /**[m
         * Returns the maximum number of sequences that can be computed in parallel[m
[36m@@ -140,7 +151,7 @@[m [mnamespace layers {[m
         *[m
         * @return The output error[m
         */[m
        [32mvirtual[m real_vector& outputErrors();
[m
	[m
	cpu_real_vector& outputErrorsCpu();   [m
[36m@@ -171,25 +182,86 @@[m [mnamespace layers {[m
         *[m
         * @param fraction The fraction of the data set to load[m
         */[m
        virtual void loadSequences(const data_sets::DataSetFraction [31m&fraction);[m[32m&fraction, const int nnState);[m
[m
        /**[m
         * Computes the forward pass[m
         */[m
        virtual void [31mcomputeForwardPass()[m[32mcomputeForwardPass(const int nnState)[m =0;
[m
        /**[m
         * Computes the backward pass, including the weight updates[m
         */[m
        virtual void [31mcomputeBackwardPass()[m[32mcomputeBackwardPass(const int nnState)[m =0;
	
        /**[m
         * Stores the description of the layer in a JSON object[m
         *[m
         * @param layersArray The array of layers in the document[m
         * @param allocator   The allocator to use[m
         */[m
        virtual void exportLayer(const helpers::JsonValue &layersArray, 
				 const helpers::JsonAllocator &allocator) const;

	[32m/**[m
[32m	 * Re-initialize the network[m
[32m	   only defines for Trainable Layers[m
[32m	 */[m
[32m	virtual void reInitWeight() = 0;[m
[32m	[m
[32m	[m
[32m	/*[m
[32m	  [m
[32m	 */[m
[32m	virtual void linkTargetLayer(Layer<TDevice> &targetLayer);[m
[32m	    [m
[32m	/**[m
[32m	 * Set and read the m_currTrainingEpoch[m
[32m	 */[m
[32m	virtual void setCurrTrainingEpoch(const int curTrainingEpoch);[m
[32m	[m
[32m	virtual int& getCurrTrainingEpoch();[m
[32m	[m
[32m	virtual void setCurrTrainingFrac(const int curTrainingFrac);[m
[32m	[m
[32m	virtual int& getCurrTrainingFrac();[m
[32m[m
[32m	/*[m
[32m	 *  Provide additional information[m
[32m	 */[m
[32m	virtual const std::string& layerAddInfor(const int opt) const;[m
[32m	[m
[32m	/*[m
[32m	 * Add 1221, prepareStepGeneration: [m
[32m	 *  prepare the layer for generating the timeStep-th frame[m
[32m	 *  [m
[32m	 */[m
[32m	virtual void prepareStepGeneration(const int timeStep);[m
[32m	[m
[32m	virtual void computeForwardPass(const int timeStep, const int nnState)=0;[m
[32m[m
[32m	virtual real_vector& feedbackOutputs(const bool flagTrain);[m
[32m[m
[32m	virtual void cleanGradidents();[m
[32m[m
[32m[m
[32m	/*[m
[32m	 * Layer state[m
[32m	 */[m
[32m	// return the number of elements in internal state[m
[32m	virtual int hiddenStateSize();[m
[32m	[m
[32m	// retreve the hidden state[m
[32m	virtual void retrieveHiddenState(const int timeStep, real_vector& readBuffer);[m
[32m[m
[32m	// set the hidden state[m
[32m	virtual void setHiddenState(const int timeStep, real_vector& writeBuffer);[m
[32m[m
[32m[m
[32m	/*[m
[32m	 * [m
[32m	 */[m
[32m	const bool flagTrainingMode() const;[m
    };[m
[m
} // namespace layers[m
[1mdiff --git a/layers/LstmLayer.cu b/layers/LstmLayer.cu[m
[1mindex db9df12..7f72b95 100644[m
[1m--- a/layers/LstmLayer.cu[m
[1m+++ b/layers/LstmLayer.cu[m
[36m@@ -801,8 +801,7 @@[m [mnamespace layers {[m
                                  Layer<TDevice> &precedingLayer,[m
                                  bool bidirectional)[m
        : TrainableLayer<TDevice>([m
		layerChild, weightsSection, 4,
		(bidirectional ? 2 : 4) * helpers::safeJsonGetInt(layerChild, "size") + 3,[m
		precedingLayer)[m
        , m_isBidirectional      (bidirectional)[m
[36m@@ -904,24 +903,27 @@[m [mnamespace layers {[m
	    [m
            if (m_isBidirectional) {[m
                fwbw->tmpOutputs      = tmp;[m
                [32mif (this->flagTrainingMode())[m
		    fwbw->tmpOutputErrors = tmp;
            [31m}[m
[31m            else[m[32m}else[m {
                fwbw->tmpOutputs     .swap(this->_outputs());[m
		[32mif (this->flagTrainingMode())[m
		    fwbw->tmpOutputErrors.swap(this->outputErrors());
            }[m
	    [m
            fwbw->cellStates      = tmp;[m
[31m            fwbw->cellStateErrors = tmp;[m
            fwbw->niActs          = tmp;[m
            fwbw->igActs          = tmp;[m
            fwbw->fgActs          = tmp;[m
            fwbw->ogActs          = tmp;[m
	    [32mif (this->flagTrainingMode()){[m
[32m		fwbw->cellStateErrors = tmp;[m
		fwbw->niDeltas        = tmp;
		fwbw->igDeltas        = tmp;
		fwbw->fgDeltas        = tmp;
		fwbw->ogDeltas        = tmp;
	    [32m}[m
            
            // weight matrices[m
            weight_matrices_t* wmArr [] = { &fwbw->weightMatrices, &fwbw->weightUpdateMatrices };[m
            real_vector*       wtsArr[] = { &this->weights(),      &this->_weightUpdates() };[m
[36m@@ -968,8 +970,6 @@[m [mnamespace layers {[m
                timestep_matrices_t tm;[m
                tm.tmpOutputs      = helpers::Matrix<TDevice>(&fwbw->tmpOutputs,[m
							      rows, cols, offset);[m
[31m                tm.tmpOutputErrors = helpers::Matrix<TDevice>(&fwbw->tmpOutputErrors,[m
[31m							      rows, cols, offset);[m
                tm.niActs          = helpers::Matrix<TDevice>(&fwbw->niActs,[m
							      rows, cols, offset);[m
                tm.igActs          = helpers::Matrix<TDevice>(&fwbw->igActs,[m
[36m@@ -978,15 +978,19 @@[m [mnamespace layers {[m
							      rows, cols, offset);[m
                tm.ogActs          = helpers::Matrix<TDevice>(&fwbw->ogActs,[m
							      rows, cols, offset);[m
		[32mif (this->flagTrainingMode()){[m
[32m		    tm.tmpOutputErrors = helpers::Matrix<TDevice>(&fwbw->tmpOutputErrors,[m
[32m								  rows, cols, offset);[m
		    tm.niDeltas        = helpers::Matrix<TDevice>(&fwbw->niDeltas,
								  rows, cols, offset);
		    tm.igDeltas        = helpers::Matrix<TDevice>(&fwbw->igDeltas,
								  rows, cols, offset);
		    tm.fgDeltas        = helpers::Matrix<TDevice>(&fwbw->fgDeltas,
								  rows, cols, offset);
		    tm.ogDeltas        = helpers::Matrix<TDevice>(&fwbw->ogDeltas,
								  rows, cols, offset);
		[32m}[m
		
		// clock configuration[m
		if (m_clockRNN){[m
		    // [m
[36m@@ -1040,7 +1044,8 @@[m [mnamespace layers {[m

        if (!m_isBidirectional) {[m
            m_fw.tmpOutputs     .swap(this->_outputs());[m
	    [32mif (this->flagTrainingMode())[m
		m_fw.tmpOutputErrors.swap(this->outputErrors());
        }[m
    }[m

[36m@@ -1159,9 +1164,16 @@[m [mnamespace layers {[m
    {[m
        TrainableLayer<TDevice>::loadSequences(fraction, nnState);[m

	[32mif (this->precedingLayer().getSaveMemoryFlag()){[m
[32m	    // This step is fact useless. The matrix will be defined in prepareStepGeneration[m
[32m	    m_precLayerOutputsMatrix = helpers::Matrix<TDevice>([m
[32m		&this->precedingLayer().outputs(), this->precedingLayer().size(), [m
[32m		this->parallelSequences());[m
[32m	}else{[m
	    m_precLayerOutputsMatrix = helpers::Matrix<TDevice>(
		&this->precedingLayer().outputs(), this->precedingLayer().size(), [m
		this->curMaxSeqLength() * this->parallelSequences());[m
	[32m}[m

        // ---- prepare the matrix for LSTM[m
	// update the niag matrices[m
[36m@@ -1177,16 +1189,17 @@[m [mnamespace layers {[m
            fwbw->fgActsMatrix = helpers::Matrix<TDevice>(&fwbw->fgActs, rows, cols);[m
            fwbw->ogActsMatrix = helpers::Matrix<TDevice>(&fwbw->ogActs, rows, cols);[m

	    [32mif (this->flagTrainingMode()){[m
		fwbw->niDeltasMatrix = helpers::Matrix<TDevice>(&fwbw->niDeltas, rows, cols);
		fwbw->igDeltasMatrix = helpers::Matrix<TDevice>(&fwbw->igDeltas, rows, cols);
		fwbw->fgDeltasMatrix = helpers::Matrix<TDevice>(&fwbw->fgDeltas, rows, cols);
		fwbw->ogDeltasMatrix = helpers::Matrix<TDevice>(&fwbw->ogDeltas, rows, cols);
		
		// set ogDeltas to zero
		thrust::fill(fwbw->ogDeltas.begin(), fwbw->ogDeltas.end(), 0.0);
	    [32m}[m
        }[m


	[m
	// ---- prepare the matrix for CLLSTM[m
	if (m_clockRNN){[m
[36m@@ -1202,7 +1215,10 @@[m [mnamespace layers {[m
		Cpu::pattype_vector clockTime = fraction.auxPattypeData();[m
		if (clockTime.size() != this->curMaxSeqLength())[m
		    throw std::runtime_error("Error unequal length of clockTime size");[m
		[32mif (this->parallelSequences()>1){[m
[32m		    printf("Please use parallel_sequences = 1\n");[m
[32m		    throw std::runtime_error("Not implemented: clockRNN for parallel training");[m
[32m		}[m
		int h2hMatrixIdx = 0;[m
		for (int t = 0; t < this->curMaxSeqLength(); t++){		    [m
		    // assign h2hIdx[m
[36m@@ -1381,9 +1397,10 @@[m [mnamespace layers {[m
    void LstmLayer<TDevice>::prepareStepGeneration(const int timeStep)[m
    {[m
	m_precLayerOutputsMatrix = helpers::Matrix<TDevice>([m
		&this->precedingLayer().outputs(),
		this->precedingLayer().size(), this->parallelSequences(),
		[31mtimeStep[m[32m(timeStep[m * this->parallelSequences() * [31mthis->precedingLayer().size());[m[32mthis->precedingLayer().size() - [m
[32m		 this->precedingLayer().outputBufPtrBias(timeStep*this->parallelSequences(), 0)));[m
    }[m


[36m@@ -1596,13 +1613,14 @@[m [mnamespace layers {[m
        // sum up the activations from the preceding layer for one time step[m
        {{[m
	    // forward states[m
	    [32m// m_preLayerOutputsMatrix is assigned to one frame by prePareStepGeneration(timeStep)[m
	    m_fw.timestepMatrices[timeStep].niActs.assignProduct(
			m_fw.weightMatrices.niInput, true, m_precLayerOutputsMatrix, false);[m
	    m_fw.timestepMatrices[timeStep].igActs.assignProduct(
			m_fw.weightMatrices.igInput, true, m_precLayerOutputsMatrix, false);[m
	    m_fw.timestepMatrices[timeStep].fgActs.assignProduct(
			m_fw.weightMatrices.fgInput, true, m_precLayerOutputsMatrix, false);[m
	    m_fw.timestepMatrices[timeStep].ogActs.assignProduct(
			m_fw.weightMatrices.ogInput, true, m_precLayerOutputsMatrix, false);[m
        }}[m

[36m@@ -2007,6 +2025,55 @@[m [mnamespace layers {[m
        (*layersArray)[layersArray->Size() - 1].AddMember("clock", m_crStepStr.c_str(), allocator);[m
    }[m


    [32mtemplate <typename TDevice>[m
[32m    int LstmLayer<TDevice>::hiddenStateSize()[m
[32m    {[m
[32m	// return the size of hidden state and cell state for one frame[m
[32m	return this->size() * 2;[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void LstmLayer<TDevice>::retrieveHiddenState(const int timeStep, real_vector& readBuffer)[m
[32m    {[m
[32m	// cannot be used for bi-directional layer[m
[32m	if (m_isBidirectional)[m
[32m	    throw std::runtime_error("retrieveHiddenState not implemented for BLSTM");[m
[32m	if (timeStep >= this->curMaxSeqLength())[m
[32m	    throw std::runtime_error("retrieveHiddenState time larger than expected");[m
[32m	// assume readbuffer has been allocated[m
[32m	int rows   = this->size();[m
[32m	int cols   = this->parallelSequences();[m
[32m	int offset = timeStep * rows * cols;[m
[32m	thrust::copy(this->_outputs().begin() + offset, this->_outputs().begin() + offset + rows,[m
[32m		     readBuffer.begin());[m
[32m	thrust::copy(m_fw.cellStates.begin() + offset, m_fw.cellStates.begin() + offset + rows,[m
[32m		     readBuffer.begin() + rows);[m
[32m       [m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    void LstmLayer<TDevice>::setHiddenState(const int timeStep, real_vector& writeBuffer)[m
[32m    {[m
[32m	// cannot be used for bi-directional layer[m
[32m	if (m_isBidirectional)[m
[32m	    throw std::runtime_error("setHiddenState not implemented for BLSTM");[m
[32m	if (timeStep >= this->curMaxSeqLength())[m
[32m	    throw std::runtime_error("setHiddenState time larger than expected");[m
[32m	if (writeBuffer.size() < this->size()*2)[m
[32m	    throw std::runtime_error("setHiddenState vector dimension is smaller");[m
[32m	int rows   = this->size();[m
[32m	int cols   = this->parallelSequences();[m
[32m	int offset = timeStep * rows * cols;[m
[32m	thrust::copy(writeBuffer.begin(), writeBuffer.begin()+rows, [m
[32m		     this->_outputs().begin() + offset);[m
[32m	thrust::copy(writeBuffer.begin()+rows, writeBuffer.begin()+rows*2,[m
[32m		     m_fw.cellStates.begin() + offset);[m

[32m    }[m
    

    
    // explicit template instantiations[m
    template class LstmLayer<Cpu>;[m
    template class LstmLayer<Gpu>;[m
[1mdiff --git a/layers/LstmLayer.hpp b/layers/LstmLayer.hpp[m
[1mindex b176978..ebd8b35 100644[m
[1m--- a/layers/LstmLayer.hpp[m
[1m+++ b/layers/LstmLayer.hpp[m
[36m@@ -284,6 +284,19 @@[m [mnamespace layers {[m
	virtual void exportLayer(const helpers::JsonValue &layersArray, [m
				 const helpers::JsonAllocator &allocator) const;[m


	[32m/*[m
[32m	 * [m
[32m	 */[m
[32m	virtual int hiddenStateSize();[m
[32m	[m
[32m	// retreve the hidden state[m
[32m	virtual void retrieveHiddenState(const int timeStep, real_vector& readBuffer);[m

[32m	// set the hidden state[m
[32m	virtual void setHiddenState(const int timeStep, real_vector& writeBuffer);[m

	
    };[m

} // namespace layers[m
[1mdiff --git a/layers/LstmLayerCharW.cu b/layers/LstmLayerCharW.cu[m
[1mindex d895b1d..69a26d5 100644[m
[1m--- a/layers/LstmLayerCharW.cu[m
[1m+++ b/layers/LstmLayerCharW.cu[m
[36m@@ -1237,11 +1237,11 @@[m [mnamespace layers {[m
	m_seqCurLength = fraction.maxSeqLength();[m
	[m
	// read in the string of word[m
	m_chaCurLength = [31mfraction.maxTxtLength();[m[32m0;//fraction.maxTxtLength();[m
	int_vector tmpIdx;[m
	[31mtmpIdx[m[32m//tmpIdx[m = [31mfraction.txtData();[m
[31m	if[m[32m//fraction.txtData();[m
[32m	//if[m (m_chaCurLength != fraction.txtData().size())
	[32m//[m    throw std::runtime_error("Please set parallel sequence = 1");
	[m
	m_strWord.resize(m_chaCurLength * m_chaDim, 0.0);[m
	m_strWordRev.resize(m_chaCurLength * m_chaDim, 0.0);[m
[1mdiff --git a/layers/MDNLayer.cu b/layers/MDNLayer.cu[m
[1mindex 7abdcc4..2b74123 100644[m
[1m--- a/layers/MDNLayer.cu[m
[1m+++ b/layers/MDNLayer.cu[m
[36m@@ -95,6 +95,10 @@[m [mnamespace layers {[m
	: PostOutputLayer<TDevice>(layerChild, precedingLayer, -1)[m
    {[m
        const Configuration &config = Configuration::instance();[m

	[32mif (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("The layer before MDN is reduced in mem");[m
	
	[m
        // parse the MDN vector[m
	int numEle;[m
[36m@@ -697,14 +701,17 @@[m [mnamespace layers {[m
    template <typename TDevice>[m
    void MDNLayer<TDevice>::computeForwardPass(const int timeStep, const int nnState)[m
    {[m

	BOOST_FOREACH (boost::shared_ptr<MDNUnit<TDevice> > &mdnUnit, m_mdnUnits){[m
	    mdnUnit->computeForward(timeStep);[m

	    // if this MDN is in the middle of a network, generate the output[m
	    if (this->_postLayerType() == NN_POSTOUTPUTLAYER_NOTLASTMDN && [m
		nnState != [31mNN_STATE_GAN_NOGAN)[m[32mNN_STATE_GAN_NOGAN){[m
[32m		mdnUnit->getOutput(timeStep,    0.0001, (this->_targets()));[m
[32m		mdnUnit->getParameter(timeStep, helpers::getRawPointer(this->m_mdnParaVec));[m
[32m	    }[m
[32m	    //[m this->getOutput(timeStep, 0.0001); // by default, use 0.0001 as the parameter
	}[m
    }[m

[36m@@ -1086,12 +1093,32 @@[m [mnamespace layers {[m
		dimStart += mdnUnit->feedBackDim();[m
		cnt++;[m
	    }[m
	[32m}*/[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    void MDNLayer<TDevice>::setFeedBackData(const int timeStep, const int state)[m
[32m    {[m
[32m	int dimStart = 0;[m
[32m	int cnt      = 0;[m
[32m	BOOST_FOREACH (boost::shared_ptr<MDNUnit<TDevice> > &mdnUnit, m_mdnUnits){[m
[32m	    mdnUnit->setFeedBackData(this->m_secondOutput,  m_secondOutputDim,  dimStart,[m
[32m				     state, timeStep);[m
[32m	    dimStart += mdnUnit->feedBackDim();[m
[32m	    cnt++;[m
	}[m
[31m	*/[m
[31m	[m
    }[m

    template <typename TDevice>[m
    [32mreal_t MDNLayer<TDevice>::retrieveProb(const int timeStep, const int state)[m
[32m    {[m
[32m	// This function is specifically used by MDNSoftmax ![m
[32m	if (m_mdnUnits.size()>1)[m
[32m	    throw std::runtime_error("Not implemented for retrieveProb");[m
[32m	return m_mdnUnits[0]->retrieveProb(timeStep, state);[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
    void MDNLayer<TDevice>::loadSequences(const data_sets::DataSetFraction &fraction,[m
					  const int nnState)[m
    {[m
[1mdiff --git a/layers/MDNLayer.hpp b/layers/MDNLayer.hpp[m
[1mindex 5a2d5ed..0dfeadb 100644[m
[1m--- a/layers/MDNLayer.hpp[m
[1m+++ b/layers/MDNLayer.hpp[m
[36m@@ -182,6 +182,10 @@[m [mnamespace layers {[m
	virtual void retrieveFeedBackData(real_vector& randNum, const int method);[m

	virtual void retrieveFeedBackData(const int timeStep, const int method=0);[m

	[32mvirtual real_t retrieveProb(const int timeStep, const int state);[m
[32m	[m
[32m	virtual void setFeedBackData(const int timeStep, const int state);[m
	[m
	virtual real_vector& feedbackOutputs(const bool flagTrain);[m

[1mdiff --git a/layers/MDNUnit.cu b/layers/MDNUnit.cu[m
[1mindex 62dc91a..271f1f0 100644[m
[1m--- a/layers/MDNUnit.cu[m
[1m+++ b/layers/MDNUnit.cu[m
[36m@@ -140,28 +140,7 @@[m [mnamespace {[m
    };[m
    [m
    [m
    [31m// copy the data from t.get<0> to the memory block started from Output [m
[31m    struct CopySimple[m
[31m    {[m
[31m	real_t     *Output;           // output address to store data[m
[31m	int         paraDim;[m
[31m	[m
[31m	const char *patTypes;     // sentence termination check[m
[31m	[m
[31m        __host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[31m        {[m
[31m            // unpack the tuple[m
[31m            int outputIdx = t.get<1>();[m
[31m	    int timeIdx   = outputIdx / paraDim;[m

[31m	    // skip dummy frame (for parallel sentence processing)[m
[31m	    if (patTypes[timeIdx] == PATTYPE_NONE)[m
[31m                return;[m

[31m            // store the result[m
[31m            *(Output+outputIdx) = t.get<0>();[m
[31m        }[m
[31m    };[m[32m//Block20170904x03[m
    [m
    struct CopySimple2[m
    {[m
[36m@@ -748,6 +727,33 @@[m [mnamespace {[m
        }[m
    };[m

    [32m// Definition for the softmax Errors[m
[32m    struct setOneHotVectorSoftmaxOneFrame[m
[32m    {[m
[32m	real_t *buffer;[m
[32m	int bufDim;[m
[32m	int bufS;[m
[32m	int paraDim;[m
[32m	int targetDim;[m
[32m	[m
[32m	bool uvSigmoid;[m
[32m	const char *patTypes;[m
[32m	// from 1 to timesteps[m
[32m        __host__ __device__ void operator() (const thrust::tuple<real_t&, const int&> &values) const[m
[32m        {[m
[32m	    const int outputIdx = values.get<1>() / paraDim;[m
[32m	    const int dimIdx    = values.get<1>() % paraDim;[m
[32m	    if (patTypes[outputIdx] == PATTYPE_NONE){[m
[32m		return;[m
[32m	    }[m
[32m	    if (dimIdx== targetDim)[m
[32m		buffer[outputIdx * bufDim + bufS + dimIdx] = 1.0;[m
[32m	    else[m
[32m		buffer[outputIdx * bufDim + bufS + dimIdx] = 0.0;[m
[32m        }[m
[32m    };[m

    
    //Block20170702x04[m
    [m
    struct setSoftVectorSoftmax[m
[36m@@ -983,198 +989,7 @@[m [mnamespace {[m
	}[m
    };[m

    // [31mCalculate the mixture distance \sum_d (x_d-\mu_d)^2/(2*std^2) for mixture model[m
[31m    // This function is used in EM-style generation[m
[31m    //  and forward anc backward propagation of mixture unit[m
[31m    struct ComputeMixtureDistanceWithTransForm[m
[31m    {[m
[31m	int startDOut;[m
[31m	int layerSizeOut;[m
[31m	int mixture_num;[m
[31m	int featureDim;[m
[31m	int totaltime;[m
[31m	bool tieVar;[m

[31m	const char *patTypes;[m
[31m	const real_t *output;    // targets data[m
[31m	const real_t *mdnPara;   // mean value of the mixture[m
[31m	const real_t *tranData; [m
[31m	// from 1 to timesteps * num_mixture[m
[31m	__host__ __device__ real_t operator() (const int idx) const[m
[31m	{[m
[31m	    [m
[31m	    int timeStep = idx / mixture_num; //t.get<0>();[m
[31m	    int mixIndex = idx % mixture_num; //t.get<1>(); [m
[31m	    [m
[31m	    // point to the targets data x[m
[31m	    int pos_data = (layerSizeOut * timeStep)+startDOut;[m
[31m		[m
[31m	    const real_t *data, *mean, *var, *trans;[m

[31m	    if (patTypes[timeStep] == PATTYPE_NONE)[m
[31m		return 0;[m
[31m	    [m
[31m	    // point to the mixture data (mean and variance)[m
[31m	    int pos =  totaltime * mixture_num;[m
[31m	    int pos_mean = pos+timeStep*featureDim*mixture_num+mixIndex*featureDim;[m
[31m	    pos     =  totaltime * (mixture_num + mixture_num * featureDim);[m

[31m            #ifdef ALTER_TIEVAR[m
[31m	    int pos_var  = pos+timeStep*mixture_num;[m
[31m            #else[m
[31m	    int pos_var  = pos+ (tieVar?[m
[31m				 (timeStep * mixture_num + mixIndex) :[m
[31m				 (timeStep * mixture_num * featureDim + mixIndex*featureDim));[m
[31m            #endif[m
[31m	    var  = mdnPara + pos_var;[m
[31m	    [m
[31m	    // pointer to the transformation part [m
[31m	    int pos_trans = idx;[m
[31m	    [m
[31m	    // accumulate the distance over dimension[m
[31m	    real_t tmp = 0.0;[m
[31m	    for (int i = 0; i<featureDim; i++){[m
[31m		data = output    + pos_data + i;[m
[31m		mean = mdnPara   + pos_mean + i;[m
[31m		var  = mdnPara   + pos_var  + (tieVar?0:i);[m
[31m		trans= tranData  + pos_trans+ i;[m
[31m		tmp += (*data-*mean-*trans)*(*data-*mean-*trans)/((*var)*(*var))/2;[m
[31m		[m
[31m	    }[m
[31m	    return tmp;[m
[31m	}[m
[31m    };[m

[31m    // Copy the data from the output buffer to the target unit (for mixture_dyn unit)[m
[31m    struct CopyTargetData[m
[31m    {[m
[31m	int startDOut;[m
[31m	int layerSizeOut;[m
[31m	int featureDim;[m

[31m	const char *patTypes;[m
[31m	const real_t *output;   // targets data[m
[31m	real_t *target;   // [m

[31m	// from 1 to timesteps * num_mixture[m
[31m	__host__ __device__ void operator() (const int idx) const[m
[31m	{[m
[31m	    [m
[31m	    int timeStep  = idx / featureDim; //t.get<0>();[m
[31m	    int featIndex = idx % featureDim; //t.get<1>(); [m
[31m	    [m
[31m	    // point to the targets data x[m
[31m	    int pos_data = (layerSizeOut * timeStep)+startDOut+featIndex;[m
[31m		[m
[31m	    if (patTypes[timeStep] == PATTYPE_NONE)[m
[31m		return;[m
[31m	    *(target+idx) = *(output + pos_data);[m
[31m	    [m
[31m	}[m
[31m    };[m

[31m    // Shift the mean value u => u + w^To + b[m
[31m    struct ShiftBiasStep1[m
[31m    {[m
[31m	int featureDim;[m
[31m	int mixNum;[m
[31m	int totalTime;[m

[31m	real_t   *linearPart;   // Wx'[m
[31m	real_t   *biasPart;     // b[m
[31m	real_t   *mdnPara;      // [m

[31m	// from 1 to timesteps * num_mixture[m
[31m	__host__ __device__ void operator() (const int idx) const[m
[31m	{[m
[31m	    [m
[31m	    int temp      = idx % (featureDim * mixNum); [m
[31m	    int featIndex = temp % featureDim; [m
[31m	    int timeStep  = idx / (featureDim * mixNum);[m
[31m	    int mixIndex  = temp / featureDim;[m

[31m	    int pos_mean;[m
[31m	    // Add to the mean value[m
[31m	    pos_mean = (totalTime * mixNum + [m
[31m			timeStep  * featureDim * mixNum + [m
[31m			mixIndex  * featureDim + featIndex); [m
[31m		[m
[31m	    if (timeStep == 0){[m
[31m		// skip the first time step[m
[31m		return;[m
[31m	    }else{[m
[31m		*(mdnPara + pos_mean) = (*(mdnPara + pos_mean) + [m
[31m					 *(linearPart + idx)   + [m
[31m					 *(biasPart   + mixIndex * featureDim + featIndex)[m
[31m					 );[m
[31m	    }	    	    	    [m
[31m	}[m
[31m    };[m

[31m    // Accumulating the statistics for BP on the linear regression part W^T o+b[m
[31m    // -1 * posteriorP(k) * (O_t - (u + W_k ^ T O_t-1 + b_k)) / var^k_d / var^k_d[m
[31m    struct ShiftBiasStep2[m
[31m    {[m
[31m	int featureDim;[m
[31m	int mixNum;[m
[31m	int totalTime;[m
[31m	int startDOut;[m
[31m	int layerSizeOut;[m

[31m	real_t   *linearPart;   // Wx'[m
[31m	real_t   *biasPart;     // b[m
[31m	real_t   *target;       // x[m
[31m	real_t   *mdnPara;      // [m
[31m	real_t   *postPbuff;[m
[31m	bool    tieVar;[m

[31m	// from 1 to timesteps * num_mixture[m
[31m	__host__ __device__ void operator() (const int idx) const[m
[31m	{[m
[31m	    [m
[31m	    int temp      = idx % (featureDim * mixNum); [m
[31m	    int featIndex = temp % featureDim; [m
[31m	    int timeStep  = idx / (featureDim * mixNum);[m
[31m	    int mixIndex  = temp / featureDim;[m

[31m	    // skip the first time step[m
[31m	    if (timeStep == 0)[m
[31m		return;[m
[31m	    [m
[31m	    // set the pointer[m
[31m	    int pos_mean, pos_var, pos_data;[m
[31m	    pos_mean = (totalTime * mixNum + [m
[31m			timeStep  * featureDim * mixNum + [m
[31m			mixIndex  * featureDim + featIndex); [m
[31m	    pos_var  = (totalTime * (mixNum + mixNum * featureDim)      + [m
[31m			timeStep  *  mixNum * (tieVar ? 1 : featureDim) + [m
[31m			mixIndex  * (tieVar ? 1 : featureDim)           +[m
[31m			(tieVar ? 0 : featIndex)); [m
[31m	    [m
[31m	    /*** FATAL ERROR **[m
[31m	     * Posterior probability should be updated [m
[31m	     * Particularly, the size of the posterior probability buffer  will change !!![m
[31m	     * Split ShiftBias into ShiftBiasStep1 and ShiftBiasStep2[m
[31m	     ******/[m
[31m	    // pointer to the posterior P and sum of posterior P[m
[31m	    const real_t *postP   = postPbuff + timeStep  * mixNum + mixIndex;[m
[31m	    const real_t *sumPost = postPbuff + totalTime * mixNum + timeStep;[m
[31m	    real_t posterior = helpers::safeExp((*postP) - (*sumPost));[m
[31m	    [m
[31m	    // point to the targets data x[m
[31m	    pos_data = (layerSizeOut * timeStep) + startDOut + featIndex;[m
[31m	    [m
[31m	    // save x - u - wx'-b to dataBuff now[m
[31m	    *(linearPart + idx) = (-1 * posterior * [m
[31m				   (*(target + pos_data) - *(mdnPara + pos_mean)) /[m
[31m				   (*(mdnPara + pos_var)) / (*(mdnPara + pos_var)));[m
[31m	    // No need to re-order the data[m
[31m	    // save \phi(i)\sigma()(x-u-wx'-b) in time order[m
[31m	    // pos_data = (mixIndex * totalTime + timeStep) * featureDim + featIndex; [m
[31m	    // *(tmpBuff+pos_data)=-1* posterior * (*(mdnPara + pos_var)) * (*(linearPart + idx));[m
[31m	    [m
[31m	}[m
[31m    };[m[32mBlock20170904x04[m

    //ShiftBiasStep1TiedCase[m
    struct ChangeMeanofMDN[m
[36m@@ -1251,58 +1066,7 @@[m [mnamespace {[m
	}[m
    };[m

[31mstruct ShiftBiasStep1TiedCaseDimensionAxis[m
[31m    {[m    // [31mShift the mean value u => u + w^To + b[m
[31m	// This function is used by mixture_dyn and mixture_dynSqr[m
[31m	// The difference is the source of the parameter w and b[m
[31m	int startDOut;[m
[31m	int layerSizeOut;[m
[31m	int featureDim;[m
[31m	int mixNum;[m
[31m	int totalTime;[m
[31m	int trainableAPos;      // w, the w predicted by the network, I name it as a now[m
[31m	int trainableBPos;      // b, the b which is predicted by the network[m
[31m	int stepBack;           // how many steps to look back ?[m

[31m	real_t   *linearPart;   // w, where w is trainable but shared across time steps[m
[31m	real_t   *biasPart;     // b, where b is trainable but shared across time steps[m
[31m	real_t   *targets;      // o_t-1[m
[31m	real_t   *mdnPara;      // [m
[31m	[m
[31m	bool      tieVar;[m

[31m	// from 1 to timesteps * num_mixture[m
[31m	__host__ __device__ void operator() (const int idx) const[m
[31m	{[m
[31m	    [m
[31m	    int timeStep  = idx  / (featureDim * mixNum);[m
[31m	    int temp      = idx  % (featureDim * mixNum); [m
[31m	    int mixIndex  = temp /  featureDim;[m
[31m	    int featIndex = temp %  featureDim;[m
[31m	    [m
[31m	    if (featIndex < stepBack){[m
[31m		// skip the first dimension[m
[31m		return;[m
[31m	    }[m
[31m	    [m
[31m	    int pos_mean, pos_data;[m
[31m	    // Add to the mean value[m
[31m	    pos_mean = (totalTime * mixNum + [m
[31m			timeStep  * featureDim * mixNum + [m
[31m			mixIndex  * featureDim + featIndex); [m
[31m	    pos_data = (timeStep  * layerSizeOut) + startDOut + featIndex - stepBack;[m
[31m	    [m
[31m	    if (linearPart != NULL && biasPart != NULL){[m
[31m		*(mdnPara + pos_mean) = ((*(mdnPara      + pos_mean))  + [m
[31m					 ((*(linearPart)) * (*(targets+pos_data))) + [m
[31m					 ((stepBack==1)  ? (*(biasPart)):0)[m
[31m					 );[m
[31m	    }else{[m
[31m		/* not implemented for context-dependent case */[m
[31m	    }[m
[31m	}[m
[31m    };[m[32mBlock20170904x05[m

    // ShiftBiasStep2TiedCase[m
    struct AccumulateGradient[m
[36m@@ -1398,31 +1162,7 @@[m [mnamespace {[m
    [m
    // Block 1025x10[m

    [31mstruct TanhAutoRegWeightStep1[m
[31m    {[m
[31m	int     featureDim;[m
[31m	int     backOrder;[m
[31m	real_t *weight;[m
[31m	real_t *weightOut;[m
[31m        __host__ __device__ void operator() (const int &Idx) const[m
[31m        {[m
[31m            *(weightOut + Idx) =  (Idx < (backOrder * featureDim))? [m
[31m		(activation_functions::Tanh::fn(*(weight+Idx))):0;[m
[31m        }[m
[31m    };[m

[31m    struct TanhAutoRegWeightStep2[m
[31m    {[m
[31m	int     featureDim;[m
[31m	real_t *weight;[m
[31m	real_t *weightOut;[m
[31m        __host__ __device__ void operator() (const int &idx) const[m
[31m        {[m
[31m	    *(weightOut + idx) = (idx < featureDim) ? [m
[31m		((*(weight + idx)) + (*(weight + idx + featureDim))) : [m
[31m		(-1 * (*(weight + idx - featureDim)) * (*(weight + idx)));[m
[31m        }[m
[31m    };[m[32m// Block20170904x06[m
    [m
    struct TanhAutoRegWeightTime[m
    {[m
[36m@@ -2259,18 +1999,19 @@[m [mnamespace {[m
	    }else if (genMethod == NN_SOFTMAX_GEN_SOFT){[m
		// SoftMerge[m
		*targetClass = 0;[m
		[32m//[m int j = 0;
		for (int i = 0; i<paradim; i++){[m
		    pos = outputIdx * paradim + i;[m
		    [31mif[m[32m*targetClass = (*targetClass) + prob[pos]*(real_t)i;[m
[32m		    /*if[m (prob[pos]>temp){
			temp = prob[pos];[m
			j = i;[m
			[31m}[m
[31m		    *targetClass = (*targetClass) + prob[pos]*(real_t)i;[m[32m}*/[m
		}[m
		[31mif[m[32m/*if[m (j==0){
		    *targetClass = (real_t)j;[m
		    }
		[32m*/[m
		// for plain softmax, special care should be taken to handle the first dimension[m
	    }else if (genMethod == NN_SOFTMAX_GEN_SAMP){[m
		real_t probAccum = 0.0;[m
[36m@@ -2931,6 +2672,19 @@[m [mnamespace layers {[m
					    const int timeStep, const int method)[m
    {	[m
    }[m

    [32mtemplate <typename TDevice>[m
[32m    void MDNUnit<TDevice>::setFeedBackData(real_vector &fillBuffer, const int bufferDim,[m
[32m					   const int dimStart,      const int state,[m
[32m					   const int timeStep)[m
[32m    {[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
[32m    real_t MDNUnit<TDevice>::retrieveProb(const int timeStep, const int state)[m
[32m    {[m
[32m	return 0.0;[m
[32m    }[m
    [m
    template <typename TDevice>[m
    int MDNUnit<TDevice>::feedBackDim()[m
[36m@@ -4319,6 +4073,50 @@[m [mnamespace layers {[m
    }[m

    template <typename TDevice>[m
    [32mvoid MDNUnit_softmax<TDevice>::setFeedBackData(real_vector &fillBuffer, const int bufferDim,[m
[32m						   const int dimStart, const int state,[m
[32m						   const int timeStep)[m
[32m    {[m
[32m	int ts = timeStep * this->m_precedingLayer.parallelSequences();[m
[32m	int te = ts + this->m_precedingLayer.parallelSequences();[m

[32m	internal::setOneHotVectorSoftmaxOneFrame fn;[m
[32m	fn.buffer    = helpers::getRawPointer(fillBuffer);[m
[32m	fn.bufDim    = bufferDim;[m
[32m	fn.bufS      = dimStart;[m
[32m	fn.paraDim   = this->m_paraDim;[m
[32m	fn.uvSigmoid = m_uvSigmoid;[m
[32m	fn.targetDim = state;[m
[32m	fn.patTypes  = helpers::getRawPointer(this->m_precedingLayer.patTypes());[m
[32m	    [m
[32m	thrust::for_each([m
[32m		thrust::make_zip_iterator([m
[32m		   thrust::make_tuple(this->m_paraVec.begin() + ts * this->m_paraDim, [m
[32m				      thrust::counting_iterator<int>(0)+ ts*this->m_paraDim)),[m
[32m		thrust::make_zip_iterator([m
[32m		   thrust::make_tuple(this->m_paraVec.begin() + te * this->m_paraDim, [m
[32m				      thrust::counting_iterator<int>(0)+ te*this->m_paraDim)),[m
[32m		fn);   	[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    real_t MDNUnit_softmax<TDevice>::retrieveProb(const int timeStep, const int state)[m
[32m    {[m
[32m	cpu_real_vector tmpPara = this->m_paraVec;[m
[32m	int idx = timeStep * this->m_paraDim + state;[m
[32m	if (idx >= this->m_paraVec.size())[m
[32m	    throw std::runtime_error("retrieveProb softmax timeStep larger than expected");[m
[32m	if (m_uvSigmoid){[m
[32m	    if (state == 0)[m
[32m		return (1.0 - tmpPara[idx]);[m
[32m	    else[m
[32m		return tmpPara[idx - state] * tmpPara[idx];[m
[32m	}else{[m
[32m	    return tmpPara[idx];[m
[32m	}[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
    int MDNUnit_softmax<TDevice>::feedBackDim()[m
    {[m
	return (this->m_endDim - this->m_startDim);[m
[1mdiff --git a/layers/MDNUnit.hpp b/layers/MDNUnit.hpp[m
[1mindex 1a743d0..37ddb91 100644[m
[1m--- a/layers/MDNUnit.hpp[m
[1m+++ b/layers/MDNUnit.hpp[m
[36m@@ -185,6 +185,12 @@[m [mnamespace layers{[m
				      const int dimStart,      real_vector &targets,[m
				      const int timeStep,      const int method);[m

	[32mvirtual void setFeedBackData(real_vector &fillBuffer, const int bufferDim,[m
[32m				     const int dimStart,      const int state,[m
[32m				     const int timeStep);[m
[32m	[m
[32m	virtual real_t retrieveProb(const int timeStep, const int state);[m
	
	virtual int feedBackDim();[m


[36m@@ -321,6 +327,12 @@[m [mnamespace layers{[m
				      const int dimStart, real_vector &targets, const int timeStep,[m
				      const int method=0);[m

	[32mvirtual real_t retrieveProb(const int timeStep, const int state);[m
[32m	[m
[32m	virtual void setFeedBackData(real_vector &fillBuffer, const int bufferDim,[m
[32m				     const int dimStart,      const int state,[m
[32m				     const int timeStep);[m

	virtual int  feedBackDim();[m

	virtual void biasProb(real_vector &secondOutput,  const int bufferDim,  const int dimStart,[m
[1mdiff --git a/layers/Maxpooling.cu b/layers/Maxpooling.cu[m
[1mindex 14a4aca..556a0d7 100644[m
[1m--- a/layers/Maxpooling.cu[m
[1m+++ b/layers/Maxpooling.cu[m
[36m@@ -182,6 +182,9 @@[m [mnamespace layers {[m
					      Layer<TDevice>           &precedingLayer)[m
	: TrainableLayer<TDevice>(layerChild, weightsSection, 0, 0, precedingLayer)[m
    {[m

	[32mthrow std::runtime_error("Maxpooling is not fully implemented");[m
	
	m_width = ((layerChild->HasMember("width")) ? [m
		   ((*layerChild)["width"].GetString()) : (""));[m
	m_stride = ((layerChild->HasMember("stride")) ? [m
[36m@@ -209,6 +212,9 @@[m [mnamespace layers {[m
	[m
	m_maxWidth =  MaxCpuIntVec(m_width_H);[m
	m_maxPos.resize(precedingLayer.outputs().size() * (2 * m_maxWidth + 1), 0);[m

	[32mif (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("layer before maxpooling is reduced in mem");[m
    }[m

    template <typename TDevice>[m
[1mdiff --git a/layers/MiddleOutputLayer.cu b/layers/MiddleOutputLayer.cu[m
[1mindex bc504b2..18d360a 100644[m
[1m--- a/layers/MiddleOutputLayer.cu[m
[1m+++ b/layers/MiddleOutputLayer.cu[m
[36m@@ -213,43 +213,45 @@[m [mnamespace layers{[m
    {[m
	m_dataOutputDim     = (layerChild->HasMember("dataOutputDim")) ? [m
			       (*layerChild)["dataOutputDim"].GetInt() : (-1);[m
[31m	m_generatorEpoch    = (layerChild->HasMember("generator_only")) ? [m
[31m			       (*layerChild)["generator_only"].GetInt() : (-1);[m
[31m	m_discriminatorEpoch= (layerChild->HasMember("discriminator_only")) ? [m
[31m			       (*layerChild)["discriminator_only"].GetInt() : (-1);[m
	m_ganRatio          = (layerChild->HasMember("ganRatio") ? [m
			       static_cast<real_t>((*layerChild)["ganRatio"].GetDouble()) :0.8);[m
	m_ganGradEnhance    = (layerChild->HasMember("ganGradMag") ? [m
			       static_cast<real_t>((*layerChild)["ganGradMag"].GetDouble()) :10.0);[m

	[32m/*m_generatorEpoch    = (layerChild->HasMember("generator_only")) ? [m
[32m			       (*layerChild)["generator_only"].GetInt() : (-1);[m
[32m	m_discriminatorEpoch= (layerChild->HasMember("discriminator_only")) ? [m
[32m	(*layerChild)["discriminator_only"].GetInt() : (-1);*/[m

	if (m_ganRatio > 1.0005 || m_ganRatio < 0.0000){[m
	    throw std::runtime_error("ganRatio must be within (0, [31m1)");[m[32m1]");[m
	}[m
	[m
	if (m_dataOutputDim > 0 && m_dataOutputDim != this->size()){[m
	    throw std::runtime_error("Error dataOutputDim in middleoutput layer");[m
	}	[m

	[m
[31m	// This part is here secondary output. It is not used now[m
	m_natPriDim     = this->size();[m
[31m	[m
	if (m_dataOutputDim > 0) {[m
	    m_natSecDim     = m_dataOutputDim - m_natPriDim;[m
	    if (m_natSecDim){[m
		[32m// This part is here secondary output. It is not used now[m
		m_natSecTarget.reserve(this->outputs().size() / m_natPriDim * m_natSecDim);[m
		thrust::fill(m_natSecTarget.begin(), m_natSecTarget.end(), 0.0);[m
	    }else[m
		m_natSecTarget.clear();[m
	}[m
[31m	[m
	m_natPriTarget  = this->outputs();[m
	[m
	m_stateRandom.resize(this->outputs().size() / m_natPriDim, 0.0);[m


	// print the information[m
	printf("\n\tGAN configure: ganRatio [31m%1.2f, ganMag %1.2f\n",[m[32m%f, ganGradMag %f\n",[m m_ganRatio, m_ganGradEnhance);
	[32mprintf("\n\tGAN criterion: (1 - ganRatio) * 0.5 * (synthetic - natural) ^ 2 + ");[m
[32m	printf("ganRatio * ganGradMag * Loss_of_discriminator\n");[m
[32m	[m
[32m	if (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("layer before MiddleOutput is reduced in mem");[m
    }[m

    template <typename TDevice>[m
[36m@@ -295,7 +297,7 @@[m [mnamespace layers{[m
	    // use all generated output[m
	    thrust::fill(tmp.begin(), tmp.end(), 0.0);[m
	    [m
	}else if (nnState == [31mNN_STATE_GAN_GENERATION_STAGE){[m[32mNN_STATE_GENERATION_STAGE){[m
	    printf(" for generation");[m
	    m_state = GENERATOR_ONLY;                  // any value is OK[m
	    thrust::fill(tmp.begin(), tmp.end(), 0.0); // any value is OK[m
[36m@@ -454,7 +456,8 @@[m [mnamespace layers{[m
	/*[m
	int st = timeStep * this->parallelSequences() * this->size();[m
	int et = st + this->parallelSequences() * this->size();[m
	*/
	[32mthrow std::runtime_error("GAN is not implemented for feedback structure");[m
    }[m

    template <typename TDevice>[m
[1mdiff --git a/layers/MiddleOutputLayer.hpp b/layers/MiddleOutputLayer.hpp[m
[1mindex 5e852b7..b5ae652 100644[m
[1m--- a/layers/MiddleOutputLayer.hpp[m
[1m+++ b/layers/MiddleOutputLayer.hpp[m
[36m@@ -48,8 +48,8 @@[m [mnamespace layers {[m
	int          m_natSecDim;[m
	[m
	int          m_state;            // state of this layer[m
	[31mint[m[32m//int[m          m_generatorEpoch;
	[31mint[m[32m//int[m          m_discriminatorEpoch;
	real_t       m_ganRatio;[m
	real_t       m_ganGradEnhance;[m
	[m
[1mdiff --git a/layers/OperationLayer.cu b/layers/OperationLayer.cu[m
[1mindex bf3395e..c39f175 100644[m
[1m--- a/layers/OperationLayer.cu[m
[1m+++ b/layers/OperationLayer.cu[m
[36m@@ -51,8 +51,10 @@[m

#include "../Configuration.hpp"[m

[32m#define NN_OPE_LAST_SHOT_MODE1 1  // use the last shot of sentence end[m
[32m#define NN_OPE_LAST_SHOT_MODE2 2  // use the last shot of sentence end, repeat across frames[m
[32m#define NN_OPE_LAST_SHOT_MODE3 3  // use the last shot of segments[m
[32m#define NN_OPE_LAST_SHOT_MODE4 4  // use the last shot of segments, repeat across frames[m

namespace internal{[m
namespace {[m
[36m@@ -236,7 +238,115 @@[m [mnamespace {[m
	    }[m
	}[m
    };[m


    [32mstruct lastShotForward[m
[32m    {[m
[32m	int     featureDim;[m
[32m	int     paralSeqNm;[m
[32m	int     lastShotOp;[m
[32m	[m
[32m	int    *seqLengthD;[m
[32m	real_t *sourceData;[m
[32m	[m
[32m	const char *patTypes;[m
[32m	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[32m	{[m
[32m	    int outputIdx = t.get<1>();[m
[32m	    int dimIdx    = outputIdx % featureDim;[m
[32m	    int timeIdx   = outputIdx / featureDim;[m

[32m	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[32m		t.get<0>() = 0.0;[m
[32m		return;[m
[32m	    }[m
[32m	    [m
[32m	    int paralBlk  = timeIdx / paralSeqNm;[m
[32m	    int sentIdx   = timeIdx % paralSeqNm;[m
[32m	    int seqLength = seqLengthD[sentIdx];[m

[32m	    if (lastShotOp == 1){[m
[32m		// only copy the last timestep to the first timestep[m
[32m		if (paralBlk == 0)[m
[32m		    t.get<0>() = sourceData[((seqLength-1) * paralSeqNm + sentIdx) * featureDim[m
[32m					    + dimIdx];[m
[32m		else[m
[32m		    t.get<0>() = 0.0;[m
[32m	    }else{[m
[32m		// copy the last timestep to all timesteps[m
[32m		t.get<0>() = sourceData[((seqLength-1) * paralSeqNm + sentIdx) * featureDim[m
[32m					+ dimIdx];[m
[32m	    }[m
[32m	}[m
[32m	[m
[32m    };[m


[32m    struct lastShotForwardSegBoundary[m
[32m    {[m
[32m	int     featureDim;[m
[32m	int     paralSeqNm;[m
[32m	int     lastShotOp;[m
[32m	[m
[32m	int    *segBoundary;[m
[32m	real_t *sourceData;[m
[32m	[m
[32m	const char *patTypes;[m
[32m	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[32m	{[m
[32m	    int outputIdx = t.get<1>();[m
[32m	    int dimIdx    = outputIdx % featureDim;[m
[32m	    int timeIdx   = outputIdx / featureDim;[m

[32m	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[32m		t.get<0>() = 0.0;[m
[32m		return;[m
[32m	    }[m
[32m	    [m
[32m	    //int paralBlk  = timeIdx / paralSeqNm;[m
[32m	    int sentIdx   = timeIdx % paralSeqNm;[m
[32m	    int boundary  = segBoundary[timeIdx];[m

[32m	    if (boundary < 0){[m
[32m		t.get<0>() = 0.0;[m
[32m	    }else{[m
[32m		t.get<0>() = sourceData[(boundary * paralSeqNm + sentIdx) * featureDim + dimIdx];[m
[32m	    }[m
[32m	}[m
[32m    };[m

[32m    struct lastShotForwardSegBoundaryGrad[m
[32m    {[m
[32m	int     featureDim;[m
[32m	int     paralSeqNm;[m
[32m	int     lastShotOp;[m
[32m	[m
[32m	int    *segBoundary;[m
[32m	real_t *targetData;[m
[32m	[m
[32m	const char *patTypes;[m
[32m	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[32m	{[m
[32m	    int outputIdx = t.get<1>();[m
[32m	    int dimIdx    = outputIdx % featureDim;[m
[32m	    int timeIdx   = outputIdx / featureDim;[m

[32m	    if (patTypes[timeIdx] == PATTYPE_NONE){[m
[32m		t.get<0>() = 0.0;[m
[32m		return;[m
[32m	    }[m
[32m	    [m
[32m	    //int paralBlk  = timeIdx / paralSeqNm;[m
[32m	    int sentIdx   = timeIdx % paralSeqNm;[m
[32m	    int boundary  = segBoundary[timeIdx];[m

[32m	    if (boundary < 0){[m
[32m		// not boundary[m
[32m	    }else{[m
[32m		targetData[(boundary * paralSeqNm + sentIdx) * featureDim + dimIdx] = t.get<0>();[m
[32m	    }[m
[32m	}[m
[32m    };[m
}[m
}[m

[36m@@ -253,7 +363,9 @@[m [mnamespace layers{[m
	, m_noiseRepeat (0)[m
    {[m

	[31m//[m[32m/* ------[m Configuration for noise generation [32m------ */[m
[32m	// Note: in operation layer, noise is concatenated with input features[m
[32m	//       for noises added to the input features, use skipIni layer[m
	m_noiseMag    = (layerChild->HasMember("noiseRatio") ? [m
			 static_cast<real_t>((*layerChild)["noiseRatio"].GetDouble()) : 1.0);[m
	m_noiseSize   = (layerChild->HasMember("noiseDim") ? [m
[36m@@ -267,10 +379,10 @@[m [mnamespace layers{[m
	m_noiseInput.resize(m_noiseSize * (this->precedingLayer().outputs().size() /[m
					   this->precedingLayer().size()), 0.0);[m

	[32m/* ------ Configuration for weighting the input features ------ */[m
	// Configuration for the weight of input vector[m
	m_setZeroStr  = ((layerChild->HasMember("setZero")) ? [m
			 ((*layerChild)["setZero"].GetString()) : (""));[m
[31m	[m
	if (m_setZeroStr.size()){[m
	    m_setZeroVec_H.clear();[m
	    ParseFloatOpt(m_setZeroStr, m_setZeroVec_H);[m
[36m@@ -281,21 +393,52 @@[m [mnamespace layers{[m
	[m
	if (this->precedingLayer().size() != m_setZeroVec_D.size())[m
	    throw std::runtime_error("Error operator setZero, unequal to previous layer size");[m

	[31m//[m[32m/* ------[m Configuration of the output downsampling [32m------ */[m
	m_downSampRes   = (layerChild->HasMember("outputDownSampling") ? 
			   [31mstatic_cast<real_t>((*layerChild)["outputDownSampling"].GetInt())[m[32mstatic_cast<int>((*layerChild)["outputDownSampling"].GetInt()) : 0);[m

[32m	/* ------ Configuration of last shot mode ------ */[m
[32m	//[m
[32m	// m_lastShot = 1: use last frame of precedingLayer as the first frame of this layer[m
[32m	// m_lastShot = 2: use last frame of precedingLayer as the all frames of this layer[m
[32m	// m_lastShot = 3: do m_lastShot=1 for every segment defined by the boundary[m
[32m	// m_lastShot = 4: not implemented now[m
[32m	m_lastShot      = (layerChild->HasMember("lastShot")?[m
[32m			   static_cast<int>((*layerChild)["lastShot"].GetInt())[m : 0);
	[32m// Configuration of the extraction of the last time steps[m
[32m	m_segLevel      = (layerChild->HasMember("segLevel")?[m
[32m			   static_cast<int>((*layerChild)["segLevel"].GetInt()) : -1);[m
	if [31m(m_downSampRes[m[32m(m_lastShot[m > [31m1){[m
[31m	    int tmpMaxLength = (this->precedingLayer().outputs().size() /[m
[31m				this->precedingLayer().size());[m
[31m	    m_downSampVec.resize(tmpMaxLength[m[32m0){[m
[32m	    // only use the utterance end boundary	    [m
[32m	    if (m_lastShot == NN_OPE_LAST_SHOT_MODE1 || m_lastShot == NN_OPE_LAST_SHOT_MODE2){[m
[32m		m_seqLengthBuffH.resize(this->parallelSequences(), 0);[m
[32m		m_seqLengthBuffD = m_seqLengthBuffH;[m
[32m		cpu_real_vector tmp(this->parallelSequences()[m * [31mtmpMaxLength,[m[32mthis->maxSeqLength() +[m
[32m				    this->parallelSequences() - 1,[m 0.0);
		[31mm_tmp.clear();[m[32mif (m_lastShot == NN_OPE_LAST_SHOT_MODE2){[m
[32m		    for (int i = this->parallelSequences(); i<tmp.size();[m
[32m			 i+=this->parallelSequences())[m
[32m			tmp[i-1] = 1.0;[m
[32m		}else{[m
[32m		    tmp[this->parallelSequences()-1] = 1.0;[m
[32m		}[m
[32m		m_oneVec = tmp;[m
[32m		[m
[32m	    // use the segmental boundary[m
[32m	    }else if (m_lastShot == NN_OPE_LAST_SHOT_MODE3 || NN_OPE_LAST_SHOT_MODE4){[m
[32m		if (m_segLevel < 0)[m
[32m		    throw std::runtime_error("segLevel is not configured for Operationlayer");[m
[32m		m_segBoundaryH.resize(this->maxSeqLength() * this->parallelSequences(), 0);[m
[32m		m_segBoundaryD = m_segBoundaryH;[m
		
	    }else{
		[31mm_downSampVec.clear();[m[32mthrow std::runtime_error("Unknown lastShot option number");[m
[32m	    }[m
	}[m


	[31m//[m[32m/* ------[m print the information [32m------ */[m
	printf("\tOperator layer: \n");[m
	if (m_noiseSize > 0)[m
	    printf("\tinject noise: dim %d, u[-%f, %f]\n", m_noiseSize, m_noiseMag, m_noiseMag);[m
[36m@@ -314,6 +457,18 @@[m [mnamespace layers{[m
	[m
	if (m_downSampRes > 1)[m
	    printf("\tdown sampling the output by %d\n", m_downSampRes);[m

	[32mif (m_lastShot > 0){[m
[32m	    printf("\tlast shot is used [%d]\n", m_lastShot);[m
[32m	    if (m_noiseSize > 0 || m_downSampRes > 1 || m_setZeroStr.size())[m
[32m		throw std::runtime_error("lastShot mode can't be used with nother operation");[m
[32m	    if (this->size() != this->precedingLayer().size())[m
[32m		throw std::runtime_error("Layer size is unequal to previous one");[m
[32m	}[m

[32m	if (this->precedingLayer().getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("layer before operator(downsamp) is reduced in mem");[m  
	
    }[m

    template <typename TDevice>[m
[36m@@ -326,19 +481,86 @@[m [mnamespace layers{[m
					      const helpers::JsonAllocator &allocator) const[m
    {[m
        TrainableLayer<TDevice>::exportLayer(layersArray, allocator);[m
	[32mif (m_setZeroStr.size())[m
	    (*layersArray)[layersArray->Size() - 1].AddMember("setZero",     m_setZeroStr.c_str(),
							      allocator);
	[32mif (m_noiseSize > 0){[m
	    (*layersArray)[layersArray->Size() - 1].AddMember("noiseRatio",  m_noiseMag,
							      allocator);
	    (*layersArray)[layersArray->Size() - 1].AddMember("noiseDim",    m_noiseSize,
							      allocator);
	    (*layersArray)[layersArray->Size() - 1].AddMember("noiseRepeat", m_noiseRepeat,
							      allocator);
	[32m}[m
[32m	[m
[32m	if (m_downSampRes > 1)[m
	    (*layersArray)[layersArray->Size() - 1].AddMember("outputDownSampling", m_downSampRes,
							      allocator);
	[32mif (m_lastShot > 0){[m
[32m	    (*layersArray)[layersArray->Size() - 1].AddMember("lastShot", m_lastShot,[m
[32m							      allocator);[m
[32m	    (*layersArray)[layersArray->Size() - 1].AddMember("segLevel", m_segLevel,[m
[32m							      allocator);[m
[32m	}[m
	
    }[m

    template <typename TDevice>[m
    [32mvoid OperationLayer<TDevice>::loadSequences(const data_sets::DataSetFraction &fraction,[m
[32m						const int nnState)[m
[32m    {[m
[32m	TrainableLayer<TDevice>::loadSequences(fraction, nnState);[m

[32m	// Load information for lst shot mode[m
[32m	[m
[32m	[m
[32m	if (m_lastShot  == NN_OPE_LAST_SHOT_MODE1 || m_lastShot == NN_OPE_LAST_SHOT_MODE2){[m
[32m	    [m
[32m	    // load the sequence length [m
[32m	    for (int i = 0; i<fraction.numSequences(); i++)[m
[32m		m_seqLengthBuffH[i] = fraction.seqInfo(i).length;[m
[32m	    m_seqLengthBuffD = m_seqLengthBuffH;[m

[32m	}else if (m_lastShot == NN_OPE_LAST_SHOT_MODE3 || m_lastShot == NN_OPE_LAST_SHOT_MODE4){[m
[32m	    [m
[32m	    // load the segments length[m
[32m	    if (m_segLevel > CHAR_BIT)[m
[32m		throw std::runtime_error("Operationlayer: segLevel is larger than expected");[m
[32m	    if (fraction.auxPattypeData().size() == 0)[m
[32m		throw std::runtime_error("Operationlayer: Last-shot requires boundary (auxData)");[m
[32m	    [m
[32m	    int pos;[m
[32m	    int boundary;[m
[32m	    char bitOp = (0b01 << m_segLevel);	[m
[32m	    for (int i = 0; i < fraction.numSequences(); i++){[m
[32m		// the last segment boundry is the end of utterance[m
[32m		boundary = (fraction.seqInfo(i).length - 1);[m
[32m		for (int time = fraction.seqInfo(i).length - 1; time>=0; time--){[m
[32m		    pos = time * this->parallelSequences() + i;  // abosolute position[m
[32m		    if (m_lastShot == NN_OPE_LAST_SHOT_MODE3){[m
[32m			if (fraction.auxPattypeData()[pos] & bitOp)[m
[32m			    m_segBoundaryH[pos] = boundary; // this is the start of segment[m
[32m			else[m
[32m			    m_segBoundaryH[pos] = -1;       // other frames[m
[32m		    }else{[m
[32m			m_segBoundaryH[pos] = boundary;     [m
[32m		    }[m
[32m			[m
[32m		    if (fraction.auxPattypeData()[pos] & bitOp){[m
[32m			// update the boundary[m
[32m			boundary = time - 1;[m
[32m		    }[m
[32m		}[m
[32m	    }[m
[32m	    m_segBoundaryD = m_segBoundaryH;[m
[32m	    [m
[32m	// nothing [m
[32m	}else{[m

[32m	}[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
    const std::string& OperationLayer<TDevice>::type() const[m
    {[m
        static std::string s;[m
[36m@@ -350,20 +572,63 @@[m [mnamespace layers{[m
    void OperationLayer<TDevice>::computeForwardPass(const int nnState)[m
    {[m
	int timeLength = this->curMaxSeqLength() * this->parallelSequences();[m
[31m	[m
[31m	if (m_noiseSize > 0){[m
[31m	    // generate the noise for all frames[m
[31m	    thrust::counting_iterator<unsigned int> index_sequence_begin(0);[m
[31m	    thrust::transform([m
[31m			      index_sequence_begin,[m
[31m			      index_sequence_begin + timeLength * m_noiseSize,[m
[31m			      m_noiseInput.begin(),[m
[31m			      internal::genNoise(-1.0 * m_noiseMag, m_noiseMag,[m
[31m						 (int)(GetRandomNumber()*10000.0)));[m

	[32mif (m_lastShot == NN_OPE_LAST_SHOT_MODE1 || m_lastShot == NN_OPE_LAST_SHOT_MODE2){[m
[32m	    // use last shot mode[m
[32m	    internal::lastShotForward fn1;[m
[32m	    fn1.featureDim = this->size();[m
[32m	    fn1.paralSeqNm = this->parallelSequences();[m
[32m	    fn1.lastShotOp = this->m_lastShot;[m
[32m	    fn1.seqLengthD = helpers::getRawPointer(m_seqLengthBuffD);[m
[32m	    fn1.sourceData = helpers::getRawPointer(this->precedingLayer().outputs());[m
[32m	    fn1.patTypes   = helpers::getRawPointer(this->patTypes());[m
[32m	    [m
[32m	    int n = timeLength * this->size();[m
[32m	    thrust::for_each([m
[32m               thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputs().begin(),[m
[32m				     thrust::counting_iterator<int>(0))),[m
[32m	       thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputs().begin()           + n,[m
[32m				     thrust::counting_iterator<int>(0) + n)),[m
[32m	       fn1);[m
[32m	    [m
[32m	}else if (m_lastShot == NN_OPE_LAST_SHOT_MODE3 || m_lastShot == NN_OPE_LAST_SHOT_MODE4){[m
[32m	    // use last shot mode based on segmental boundary[m
[32m	    internal::lastShotForwardSegBoundary fn1;[m
[32m	    fn1.featureDim  = this->size();[m
[32m	    fn1.paralSeqNm  = this->parallelSequences();[m
[32m	    fn1.lastShotOp  = this->m_lastShot;[m
[32m	    fn1.segBoundary = helpers::getRawPointer(m_segBoundaryD);[m
[32m	    fn1.sourceData  = helpers::getRawPointer(this->precedingLayer().outputs());[m
[32m	    fn1.patTypes    = helpers::getRawPointer(this->patTypes());[m
[32m	    [m
[32m	    int n = timeLength * this->size();[m
[32m	    thrust::for_each([m
[32m               thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputs().begin(),[m
[32m				     thrust::counting_iterator<int>(0))),[m
[32m	       thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputs().begin()           + n,[m
[32m				     thrust::counting_iterator<int>(0) + n)),[m
[32m	       fn1);[m
[32m	    	    [m
[32m	}else{[m
[32m	    [m
[32m	    // normal mode[m
[32m	    if (m_noiseSize > 0){[m
[32m		// generate the noise for all frames[m
[32m		thrust::counting_iterator<unsigned int> index_sequence_begin(0);[m
[32m		thrust::transform([m
[32m				  index_sequence_begin,[m
[32m				  index_sequence_begin + timeLength * m_noiseSize,[m
[32m				  m_noiseInput.begin(),[m
[32m				  internal::genNoise(-1.0 * m_noiseMag, m_noiseMag,[m
[32m						     (int)(GetRandomNumber()*10000.0)));[m

	    }
	[m
	    {
	    internal::fillOutputVec fn;[m
	    fn.curLayerSize = this->size();[m
	    fn.preLayerSize = this->precedingLayer().size();[m
[36m@@ -384,76 +649,103 @@[m [mnamespace layers{[m
		  thrust::make_tuple(this->outputs().begin()           + n,[m
				     thrust::counting_iterator<int>(0) + n)),[m
	       fn);[m
	    
	    [32m}[m
[32m	    [m
[32m	    if (m_downSampRes > 1){[m
[32m		internal::downSampOperation fn1;[m
[32m		fn1.featureDim = this->size();[m
[32m		fn1.resolution = m_downSampRes;[m
[32m		fn1.maxTimeLength = timeLength;[m
[32m		fn1.dataMatrix = helpers::getRawPointer(this->outputs());[m
[32m		fn1.parall     = this->parallelSequences();[m
[32m		fn1.patTypes  = helpers::getRawPointer(this->patTypes());[m
[32m		[m
[32m		int n = timeLength * this->size();[m
[32m		thrust::for_each([m
[32m		    thrust::make_zip_iterator([m
[32m			thrust::make_tuple(this->outputs().begin(),[m
[32m					   thrust::counting_iterator<int>(0))),[m
[32m		    thrust::make_zip_iterator([m
[32m			thrust::make_tuple(this->outputs().begin()           + n,[m
[32m					   thrust::counting_iterator<int>(0) + n)),[m
[32m		    fn1);[m
[32m	    }[m
	}[m

    [32m}[m

[32m    template <typename TDevice>[m
[32m    void OperationLayer<TDevice>::computeForwardPass(const int timeStep, const int nnState)[m
[32m    {[m
[32m	int timeLength = this->curMaxSeqLength() * this->parallelSequences();[m

[32m	if (m_lastShot == NN_OPE_LAST_SHOT_MODE1 || m_lastShot == NN_OPE_LAST_SHOT_MODE2){[m
[32m	    // Tricky code[m
[32m	    // Although operator with last shot should not be used after a feedback layer[m
[32m	    // (because it generates the output at the end of segment and uses it at the begining[m
[32m	    //  of a segment), the boundary information can be generated[m 
	    if [31m(m_downSampRes > 1){[m
[31m	    internal::downSampOperation[m[32m(timeStep == 0){[m
[32m		thrust::fill(this->precedingLayer().outputs().begin(),[m
[32m			     this->precedingLayer().outputs().begin()+timeLength * this->size(),[m
[32m			     1.0);[m
[32m		internal::lastShotForward[m fn1;
		fn1.featureDim = this->size();
		[31mfn1.resolution[m[32mfn1.paralSeqNm[m = [31mm_downSampRes;[m
[31m	    fn1.maxTimeLength[m[32mthis->parallelSequences();[m
[32m		fn1.lastShotOp[m = [31mtimeLength;[m
[31m	    fn1.dataMatrix[m[32mthis->m_lastShot;[m
[32m		fn1.seqLengthD[m = [31mhelpers::getRawPointer(this->outputs());[m
[31m	    fn1.parall[m[32mhelpers::getRawPointer(m_seqLengthBuffD);[m
[32m		fn1.sourceData[m = [31mthis->parallelSequences();[m[32mhelpers::getRawPointer(this->precedingLayer().outputs());[m
		fn1.patTypes   = helpers::getRawPointer(this->patTypes());
	    [m
		int n = timeLength * this->size();
		thrust::for_each(
		 thrust::make_zip_iterator(
		  thrust::make_tuple(this->outputs().begin(),[m
				     thrust::counting_iterator<int>(0))),[m
		 thrust::make_zip_iterator(
		  thrust::make_tuple(this->outputs().begin()           + n,[m
				     thrust::counting_iterator<int>(0) + n)),[m
		 fn1);
	    [31m/*[m[32m}[m
[32m	}else if (m_lastShot == NN_OPE_LAST_SHOT_MODE3 || m_lastShot == NN_OPE_LAST_SHOT_MODE4){[m
	    // [31mprepare the matrix[m
[31m	    {{[m
[31m		internal::downSampMatrix[m[32mLast shot mode can not be used here[m
[32m	    if (timeStep == 0){[m
[32m		thrust::fill(this->precedingLayer().outputs().begin(),[m
[32m			     this->precedingLayer().outputs().begin()+timeLength * this->size(),[m
[32m			     1.0);[m
[32m		internal::lastShotForwardSegBoundary[m fn1;
		[31mfn1.size[m[32mfn1.featureDim[m  = [31mtimeLength;[m
[31m		fn1.resolution = m_downSampRes;[m
[31m		fn1.parall[m[32mthis->size();[m
[32m		fn1.paralSeqNm[m  = this->parallelSequences();
		[32mfn1.lastShotOp  = this->m_lastShot;[m
[32m		fn1.segBoundary = helpers::getRawPointer(m_segBoundaryD);[m
[32m		fn1.sourceData  = helpers::getRawPointer(this->precedingLayer().outputs());[m
[32m		fn1.patTypes    = helpers::getRawPointer(this->patTypes());[m
[32m	    [m
[32m		int n = timeLength * this->size();[m
		thrust::for_each([m
                 thrust::make_zip_iterator(
		  [31mthrust::make_tuple(m_downSampVec.begin(),[m[32mthrust::make_tuple(this->outputs().begin(),[m
				     thrust::counting_iterator<int>(0))),[m
		 thrust::make_zip_iterator([m
		  [31mthrust::make_tuple(m_downSampVec.begin()[m[32mthrust::make_tuple(this->outputs().begin()[m           + [31mtimeLength * timeLength,[m[32mn,[m
				     thrust::counting_iterator<int>(0) + [31mtimeLength * timeLength)),[m[32mn)),[m
		 fn1);[m
	    [31m}}[m
[31m	    [m
[31m	    helpers::Matrix<TDevice> weightsMatrix  (&m_downSampVec, timeLength, timeLength);[m

[31m	    [m
[31m            helpers::Matrix<TDevice> plOutputsMatrix(&this->_outputs(), this->size(), timeLength);[m

[31m            helpers::Matrix<TDevice> outputsMatrix  (&m_tmp, this->size(), timeLength);[m

[31m            outputsMatrix.assignProduct(plOutputsMatrix, false, weightsMatrix, false);[m[32m}[m
	    [m
	[31mthrust::copy(m_tmp.begin(), m_tmp.begin() + timeLength * this->size(),[m
[31m			 this->_outputs().begin());[m
[31m	    */[m
[31m	}[m
[31m    }[m

[31m    template <typename TDevice>[m
[31m    void OperationLayer<TDevice>::computeForwardPass(const int timeStep, const int nnState)[m
[31m    {[m
[31m	int timeLength = this->curMaxSeqLength() * this->parallelSequences();[m[32m}else{[m
	[m
	    if (m_noiseSize > 0 && timeStep == 0){
		// generate the noise for all frames at the 1st timeStep
		thrust::counting_iterator<unsigned int> index_sequence_begin(0);
		thrust::transform(
				  index_sequence_begin,
				  index_sequence_begin + timeLength * m_noiseSize,
				  m_noiseInput.begin(),
				  internal::genNoise(-1.0 * m_noiseMag, m_noiseMag,
						     (int)(GetRandomNumber()*10000.0)));

	    }
	    {
	    internal::fillOutputVec fn;[m
	    fn.curLayerSize = this->size();[m
	    fn.preLayerSize = this->precedingLayer().size();[m
[36m@@ -481,28 +773,28 @@[m [mnamespace layers{[m
		  thrust::make_tuple(this->outputs().begin()           + et,[m
				     thrust::counting_iterator<int>(0) + et)),[m
	       fn);[m
	    }

	    if (m_downSampRes > 1){
		internal::downSampOperation fn1;
		fn1.featureDim = this->size();
		fn1.resolution = m_downSampRes;
		fn1.maxTimeLength = timeLength;
		fn1.dataMatrix = helpers::getRawPointer(this->outputs());
		fn1.parall     = this->parallelSequences();
		fn1.patTypes  = helpers::getRawPointer(this->patTypes());
	    [m
		int st = timeStep * this->size();
		int et = timeStep * this->size() + this->size();
		thrust::for_each(
		 thrust::make_zip_iterator(
		  thrust::make_tuple(this->outputs().begin()           + st,[m
				     thrust::counting_iterator<int>(0) + st)),[m
		 thrust::make_zip_iterator(
		  thrust::make_tuple(this->outputs().begin()           + et,[m
				     thrust::counting_iterator<int>(0) + et)),[m
		 fn1);
	    [32m}[m
	}[m

    }[m
[36m@@ -511,29 +803,80 @@[m [mnamespace layers{[m
    void OperationLayer<TDevice>::computeBackwardPass(const int nnState)[m
    {	[m
	int timeLength = this->curMaxSeqLength() * this->parallelSequences();[m

	if [31m(m_downSampRes > 1){[m[32m(m_lastShot == NN_OPE_LAST_SHOT_MODE1 || m_lastShot == NN_OPE_LAST_SHOT_MODE2){[m
[32m	    thrust::fill(this->precedingLayer().outputErrors().begin(),[m
[32m			 this->precedingLayer().outputErrors().end(), 0.0);[m
[32m	    for (int i = 0; i<this->parallelSequences(); i++){[m
[32m		timeLength = this->parallelSequences() * this->m_seqLengthBuffH[i];[m
[32m		// A trick for parallel training mode: circular move over one vector[m
[32m		//       A B C D A B C D[m
[32m		// 0 0 0 1 0 0 0 1 0 0 0          sum As, shift 3[m
[32m		//   0 0 0 1 0 0 0 1 0 0 0        sum Bs, shift 2[m
[32m		//     0 0 0 1 0 0 0 1 0 0 0      sum Cs, shift 1[m
[32m		//       0 0 0 1 0 0 0 1 0 0 0    sum Ds, shift 0[m
[32m		helpers::Matrix<TDevice> onevec  (&this->m_oneVec, timeLength, 1,[m
[32m						  (this->parallelSequences() - 1 - i));[m
[32m		helpers::Matrix<TDevice> source  (&this->outputErrors(), this->size(), timeLength);[m
[32m		helpers::Matrix<TDevice> output  (&this->precedingLayer().outputErrors(), [m
[32m						  this->size(), 1,[m
[32m						  (timeLength - this->parallelSequences() + i) *[m
[32m						  this->size());[m
[32m		// sum the gradients for a_k[m
[32m		output.assignProduct(source, false, onevec, false);		[m
[32m	    }[m
	    [m
	[32m}else if (m_lastShot == NN_OPE_LAST_SHOT_MODE3 || m_lastShot == NN_OPE_LAST_SHOT_MODE4){[m
[32m	    thrust::fill(this->precedingLayer().outputErrors().begin(),[m
[32m			 this->precedingLayer().outputErrors().end(), 0.0);[m
[32m	    if (m_lastShot == NN_OPE_LAST_SHOT_MODE3){[m
[32m		// use last shot mode based on segmental boundary[m
[32m		internal::lastShotForwardSegBoundaryGrad fn1;[m
[32m		fn1.featureDim  = this->size();[m
[32m		fn1.paralSeqNm  = this->parallelSequences();[m
[32m		fn1.lastShotOp  = this->m_lastShot;[m
[32m		fn1.segBoundary = helpers::getRawPointer(m_segBoundaryD);[m
[32m		fn1.targetData  = helpers::getRawPointer(this->precedingLayer().outputErrors());[m
[32m		fn1.patTypes    = helpers::getRawPointer(this->patTypes());[m
[32m	    [m
[32m		int n = timeLength * this->size();[m
[32m		thrust::for_each([m
[32m                 thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputErrors().begin(),[m
[32m				     thrust::counting_iterator<int>(0))),[m
[32m		 thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputErrors().begin() + n,[m
[32m				     thrust::counting_iterator<int>(0) + n)),[m
[32m		 fn1);[m

[32m	    }else{[m
[32m		// not implemented[m
[32m	    }[m
[32m	}else{[m
[32m	    if (m_downSampRes > 1){[m
	    
		internal::downSampGradOperation fn1;
		fn1.featureDim = this->size();
		fn1.resolution = m_downSampRes;
		fn1.maxTimeLength = timeLength;
		fn1.dataMatrix = helpers::getRawPointer(this->outputErrors());	
		fn1.parall     = this->parallelSequences();
		fn1.patTypes   = helpers::getRawPointer(this->patTypes());
		[m
		int n = timeLength * this->size();
		thrust::for_each(
                 thrust::make_zip_iterator(
		  thrust::make_tuple(this->outputErrors().begin(),[m
				     thrust::counting_iterator<int>(0))),[m
		 thrust::make_zip_iterator(
		  thrust::make_tuple(this->outputErrors().begin()      + n,[m
				     thrust::counting_iterator<int>(0) + n)),[m
		 fn1);
	    }
	[m
	    {
		[32m// Although it is tricky to use the same function as in computeForwardPass,[m
[32m		// it works by constraining the curLayerSize and preLayerSize[m
	    internal::fillOutputVec fn;[m
	    fn.curLayerSize = this->precedingLayer().size();[m
	    fn.preLayerSize = this->size();[m
[36m@@ -553,6 +896,7 @@[m [mnamespace layers{[m
		thrust::make_tuple(this->precedingLayer().outputErrors().begin() + n,[m
				     thrust::counting_iterator<int>(0)           + n)),[m
	       fn);[m
	    [32m}[m
	}[m

    }[m
[1mdiff --git a/layers/OperationLayer.hpp b/layers/OperationLayer.hpp[m
[1mindex 7b15e6d..c2f73f8 100644[m
[1m--- a/layers/OperationLayer.hpp[m
[1m+++ b/layers/OperationLayer.hpp[m
[36m@@ -54,9 +54,17 @@[m [mnamespace layers{[m
	real_vector     m_noiseInput;[m
	int             m_noiseRepeat;[m

[31m	real_vector     m_downSampVec;[m
	int             m_downSampRes;[m

	real_vector     [31mm_tmp;[m[32mm_oneVec;[m
[32m	int             m_lastShot;[m
[32m	[m
[32m	cpu_int_vector  m_seqLengthBuffH;  // the length of each sequence[m
[32m	int_vector      m_seqLengthBuffD;  // the length of each sequence[m
[32m	[m
[32m	cpu_int_vector  m_segBoundaryH;    // position of the end of segment (for each frame)[m
[32m	int_vector      m_segBoundaryD;[m
[32m	int             m_segLevel;        // which level to be used ?[m
	[m
	OperationLayer([m
	    const helpers::JsonValue &layerChild,[m
[36m@@ -81,7 +89,8 @@[m [mnamespace layers{[m
	// export[m
	virtual void exportLayer(const helpers::JsonValue &layersArray, [m
				 const helpers::JsonAllocator &allocator) const;[m
	[32m//[m
[32m	virtual void loadSequences(const data_sets::DataSetFraction &fraction, const int nnState);[m
    };[m
    [m
}[m
[1mdiff --git a/layers/PostOutputLayer.cu b/layers/PostOutputLayer.cu[m
[1mindex 640d699..85de53a 100644[m
[1m--- a/layers/PostOutputLayer.cu[m
[1m+++ b/layers/PostOutputLayer.cu[m
[36m@@ -24,6 +24,8 @@[m
#include "../helpers/getRawPointer.cuh"[m
#include "../helpers/misFuncs.hpp"[m
#include "../MacroDefine.hpp"[m
[32m#include "../Configuration.hpp"[m

#include <boost/lexical_cast.hpp>[m


[36m@@ -102,7 +104,9 @@[m [mnamespace layers {[m
        int requiredSize,[m
        bool createOutputs)[m
        : Layer<TDevice>  (layerChild, precedingLayer.parallelSequences(), [m
			   precedingLayer.maxSeqLength(),
			   [32mConfiguration::instance().trainingMode(),[m			   
			   createOutputs)
        , m_precedingLayer(precedingLayer)[m
	, m_precedingMiddleOutLayer (NULL)[m
	, m_postoutputFlag(NN_POSTOUTPUTLAYER_LAST)[m
[36m@@ -338,6 +342,14 @@[m [mnamespace layers {[m
	      fn);[m
	}[m
    }[m

    [32mtemplate <typename TDevice>[m
[32m    void PostOutputLayer<TDevice>::setFeedBackData(const int timeStep, const int state)[m
[32m    {[m
[32m	// This is not used at all;[m
[32m	throw std::runtime_error("setFeedBackData should be not used for non-MDN layer");[m
[32m	//return m_feedBackOutput;[m
[32m    }[m
    [m
    template <typename TDevice>[m
    typename PostOutputLayer<TDevice>::real_vector& PostOutputLayer<TDevice>::feedbackOutputs([m
[36m@@ -352,6 +364,12 @@[m [mnamespace layers {[m
    }[m

    template <typename TDevice>[m
    [32mreal_t PostOutputLayer<TDevice>::retrieveProb(const int timeStep, const int state)[m
[32m    {[m
[32m	return 0.0;[m
[32m    }[m
[32m    [m
[32m    template <typename TDevice>[m
    typename PostOutputLayer<TDevice>::real_vector& PostOutputLayer<TDevice>::secondOutputs()[m
    {[m
	// This is not used at all;[m
[1mdiff --git a/layers/PostOutputLayer.hpp b/layers/PostOutputLayer.hpp[m
[1mindex 241932a..6f44523 100644[m
[1m--- a/layers/PostOutputLayer.hpp[m
[1m+++ b/layers/PostOutputLayer.hpp[m
[36m@@ -137,6 +137,10 @@[m [mnamespace layers {[m
[m
	virtual void retrieveFeedBackData(const int timeStep, const int method=0);[m
[m
	[32mvirtual void setFeedBackData(const int timeStep, const int state);[m
[32m[m
[32m	virtual real_t retrieveProb(const int timeStep, const int state);[m
	
	// Used by feedbackLayer[m
	virtual real_vector& feedbackOutputs(const bool flagTrain);[m
[m
[1mdiff --git a/layers/RnnLayer.cu b/layers/RnnLayer.cu[m
[1mindex 869e851..847c449 100644[m
[1m--- a/layers/RnnLayer.cu[m
[1m+++ b/layers/RnnLayer.cu[m
[36m@@ -523,13 +523,15 @@[m [mnamespace layers {[m
	    // temporary buffer tmp [els, timeLength * parallel][m
	    Cpu::real_vector tmp(this->outputs().size()/2, 0);[m
	    m_fw.tmpOutputs        = tmp;    // initialize the Device vector using host vector[m
	    m_bw.tmpOutputs        = tmp;
	    [32mif (this->flagTrainingMode()){[m
		m_fw.tmpOutputErrors   = tmp;
		m_bw.tmpOutputErrors   = tmp;
		[32mm_fw.unitDeltas        = tmp;[m
[32m		m_bw.unitDeltas        = tmp;[m
[32m	    }[m
	    m_fw.unitActs          = tmp;[m
	    m_bw.unitActs          = tmp;[m
[31m	    m_fw.unitDeltas        = tmp;[m
[31m	    m_bw.unitDeltas        = tmp;[m
	    m_fw.unitActsBuf       = tmp;[m
	    m_bw.unitActsBuf       = tmp;[m
	    [m
[36m@@ -586,29 +588,31 @@[m [mnamespace layers {[m

                fm.tmpOutputsWrapT = [m
		    helpers::Matrix<TDevice>(&m_fw.tmpOutputs,      rows, cols, offset);[m
[31m		fm.tmpOutputErrorsWrapT = [m
[31m		    helpers::Matrix<TDevice>(&m_fw.tmpOutputErrors, rows, cols, offset);[m
		fm.unitActsWrapT = [m
		    helpers::Matrix<TDevice>(&m_fw.unitActs,        rows, cols, offset);[m
[31m		fm.unitDeltasWrapT = [m
[31m		    helpers::Matrix<TDevice>(&m_fw.unitDeltas,      rows, cols, offset);[m
		fm.unitActsBufWrapT = [m
		    helpers::Matrix<TDevice>(&m_fw.unitActsBuf,     rows, cols, offset);[m
		[32mif (this->flagTrainingMode()){[m
[32m		    fm.tmpOutputErrorsWrapT = [m
[32m			helpers::Matrix<TDevice>(&m_fw.tmpOutputErrors, rows, cols, offset);[m
[32m		    fm.unitDeltasWrapT = [m
[32m			helpers::Matrix<TDevice>(&m_fw.unitDeltas,      rows, cols, offset);[m
		    fm.unitDeltaP    = helpers::getRawPointer(m_fw.unitDeltas) + offset;
		[32m}[m

		bm.tmpOutputsWrapT = [m
		    helpers::Matrix<TDevice>(&m_bw.tmpOutputs,      rows, cols, offset);[m
[31m		bm.tmpOutputErrorsWrapT = [m
[31m		    helpers::Matrix<TDevice>(&m_bw.tmpOutputErrors, rows, cols, offset);[m
		bm.unitActsWrapT = [m
		    helpers::Matrix<TDevice>(&m_bw.unitActs,        rows, cols, offset);[m
[31m		bm.unitDeltasWrapT = [m
[31m		    helpers::Matrix<TDevice>(&m_bw.unitDeltas,      rows, cols, offset);[m
		bm.unitActsBufWrapT = [m
		    helpers::Matrix<TDevice>(&m_bw.unitActsBuf,     rows, cols, offset);[m
		[32mif (this->flagTrainingMode()){[m
[32m		    bm.tmpOutputErrorsWrapT = [m
[32m			helpers::Matrix<TDevice>(&m_bw.tmpOutputErrors, rows, cols, offset);[m
[32m		    bm.unitDeltasWrapT = [m
[32m			helpers::Matrix<TDevice>(&m_bw.unitDeltas,      rows, cols, offset);[m
		    bm.unitDeltaP    = helpers::getRawPointer(m_bw.unitDeltas) + offset;
		[32m}[m

		// for ClockRNN[m
		if (m_clockRNN){[m
[36m@@ -667,7 +671,8 @@[m [mnamespace layers {[m
		m_fw.skipCR        = tmp2;[m
	    }[m
	    m_fw.tmpOutputs        .swap(this->_outputs());     // just swap (directly use it)[m
	    [32mif (this->flagTrainingMode())[m
		m_fw.tmpOutputErrors   .swap(this->outputErrors()); // 
	    m_fw.unitActs          = tmp;[m
	    m_fw.unitDeltas        = tmp;[m
	    m_fw.unitActsBuf       = tmp;[m
[36m@@ -704,16 +709,18 @@[m [mnamespace layers {[m

                fm.tmpOutputsWrapT = [m
		    helpers::Matrix<TDevice>(&m_fw.tmpOutputs,      rows, cols, offset);[m
[31m		fm.tmpOutputErrorsWrapT = [m
[31m		    helpers::Matrix<TDevice>(&m_fw.tmpOutputErrors, rows, cols, offset);[m
		fm.unitActsWrapT = [m
		    helpers::Matrix<TDevice>(&m_fw.unitActs,        rows, cols, offset);[m
[31m		fm.unitDeltasWrapT = [m
[31m		    helpers::Matrix<TDevice>(&m_fw.unitDeltas,      rows, cols, offset);[m
		fm.unitActsBufWrapT = [m
		    helpers::Matrix<TDevice>(&m_fw.unitActsBuf,     rows, cols, offset);[m

		[32mif (this->flagTrainingMode()){[m
[32m		    fm.tmpOutputErrorsWrapT = [m
[32m			helpers::Matrix<TDevice>(&m_fw.tmpOutputErrors, rows, cols, offset);[m
[32m		    fm.unitDeltasWrapT = [m
[32m			helpers::Matrix<TDevice>(&m_fw.unitDeltas,      rows, cols, offset);[m
		    fm.unitDeltaP    = helpers::getRawPointer(m_fw.unitDeltas) + offset;
		[32m}[m

		// for ClockRNN[m
		if (m_clockRNN){		    [m
[36m@@ -744,7 +751,8 @@[m [mnamespace layers {[m
	// swap it back[m
	if (!m_isBidirectional) {[m
            m_fw.tmpOutputs     .swap(this->_outputs());[m
	    [32mif (this->flagTrainingMode())[m
		m_fw.tmpOutputErrors.swap(this->outputErrors());
        }[m
    }[m

[36m@@ -779,21 +787,29 @@[m [mnamespace layers {[m
	// the wrappers must be created for each training epoch[m
	int rows = this->size() / (m_isBidirectional ? 2 : 1);[m
	int cols = this->curMaxSeqLength() * this->parallelSequences();[m

	[32mif (this->precedingLayer().getSaveMemoryFlag()){[m
[32m	    m_precLayerOutputsWrapA = [m
[32m		helpers::Matrix<TDevice>(&this->precedingLayer().outputs(), [m
[32m					 this->precedingLayer().size(), this->parallelSequences());[m
[32m	}else{[m
	    m_precLayerOutputsWrapA = 
		helpers::Matrix<TDevice>(&this->precedingLayer().outputs(), 
					 this->precedingLayer().size(), cols);
	[32m}[m

	m_onesVecWrap            = helpers::Matrix<TDevice>(&m_onesVec, 1, cols);[m
	if (m_isBidirectional){[m
	    m_fw.unitActsWrapA   = helpers::Matrix<TDevice>(&m_fw.unitActs,   rows, cols);[m
[31m	    m_fw.unitDeltasWrapA = helpers::Matrix<TDevice>(&m_fw.unitDeltas, rows, cols);[m
	    m_bw.unitActsWrapA   = helpers::Matrix<TDevice>(&m_bw.unitActs,   rows, cols);[m
	    [32mif (this->flagTrainingMode()){[m
[32m		m_fw.unitDeltasWrapA = helpers::Matrix<TDevice>(&m_fw.unitDeltas, rows, cols);[m
		m_bw.unitDeltasWrapA = helpers::Matrix<TDevice>(&m_bw.unitDeltas, rows, cols);
	    [32m}[m
	}else{[m
	    m_fw.unitActsWrapA   = helpers::Matrix<TDevice>(&m_fw.unitActs,   rows, cols);[m
	    [32mif (this->flagTrainingMode())[m
		m_fw.unitDeltasWrapA = helpers::Matrix<TDevice>(&m_fw.unitDeltas, rows, cols);
	}[m
	[m
	// Copy the Hidden2Hidden Matrix to each possible Hidden2Hidden Matrix format[m
[36m@@ -805,6 +821,10 @@[m [mnamespace layers {[m
	    // Read in the context-dependent time clock[m
	    if (fraction.auxDataDim()>0){[m
		Cpu::pattype_vector clockTime = fraction.auxPattypeData();[m
		[32mif (this->parallelSequences()>1){[m
[32m		    printf("Please use parallel_sequences = 1\n");[m
[32m		    throw std::runtime_error("Not implemented: clockRNN for parallel training");[m
[32m		}[m
		if (clockTime.size() != this->curMaxSeqLength()){[m
		    throw std::runtime_error("Error unequal length of clockTime size");[m
		}[m
[36m@@ -975,7 +995,8 @@[m [mnamespace layers {[m
	m_precLayerOutputsWrapA = helpers::Matrix<TDevice>([m
		&this->precedingLayer().outputs(), [m
		this->precedingLayer().size(), this->parallelSequences(),[m
		[31mtimeStep[m[32m(timeStep[m * this->parallelSequences() * [31mthis->precedingLayer().size());[m[32mthis->precedingLayer().size() - [m
[32m		 this->precedingLayer().outputBufPtrBias(timeStep * this->parallelSequences(), 0)));[m
	[m
	int rows = this->size() / (m_isBidirectional ? 2 : 1);[m
	m_fw.unitActsWrapA   = helpers::Matrix<TDevice>([m
[1mdiff --git a/layers/SkipAddLayer.cu b/layers/SkipAddLayer.cu[m
[1mindex 7cbae05..e196a85 100644[m
[1m--- a/layers/SkipAddLayer.cu[m
[1m+++ b/layers/SkipAddLayer.cu[m
[36m@@ -48,8 +48,7 @@[m
namespace internal{[m
namespace{[m

    [32m// Generating noise[m
    struct tempPrg[m
    {[m
	float a, b;[m
[36m@@ -86,13 +85,16 @@[m [mnamespace layers{[m
	, m_noiseRatio      (-1.0)[m
	, m_flagSkipInit    (true)[m
    {[m
	[32m// Initial check[m
	if (precedingLayers.size() < [31m1){[m[32m1)[m
	    throw std::runtime_error("Error no precedinglayers in skipadd/skipini");[m
[31m	}[m
	[m
	[32m// Link previous layers[m
	m_previousSkipStr = (layerChild->HasMember("preSkipLayer") ? [m
			     ((*layerChild)["preSkipLayer"].GetString()) : "");[m
	if (m_previousSkipStr.size()){[m

	    [32m// previous layers are specified by preSkipLayer[m
	    std::vector<std::string> tmpOpt;[m
	    ParseStrOpt(m_previousSkipStr, tmpOpt, ",");[m
	    for (int cnt = 0 ; cnt < tmpOpt.size(); cnt++) {[m
[36m@@ -103,7 +105,6 @@[m [mnamespace layers{[m
		    }[m
		}[m
	    }[m

	    /*[m
	    boost::iterator_range<std::string::iterator> r;[m
	    BOOST_FOREACH (Layer<TDevice> *layer, precedingLayers) {[m
[36m@@ -115,7 +116,7 @@[m [mnamespace layers{[m
		m_preLayers.back()->name() != precedingLayers.back()->name()){[m
		// if the string doesn't specify the defaul previous layer[m
		m_preLayers.push_back(precedingLayers.back());[m
	    }*/

	}else{[m
	    // default cause, use only the previous 1 skip and previous normal output layer[m
[36m@@ -142,12 +143,20 @@[m [mnamespace layers{[m
	if (m_noiseRatio > 0)[m
	    printf("\n\tInject noise %f\n", m_noiseRatio);[m

	[32m// Post-processing check[m
[32m	// Note: here precedingLayers is checked.[m
[32m	// In NeuralNetwork(), only skipadd will receive more than one layer in precedingLayer[m
	if (precedingLayers.size()<2)[m
	    m_flagSkipInit = true;  // this is the skipinit[m
	else[m
	    m_flagSkipInit = false; // [31mnot[m[32mthis is the skipadd[m

	[32mif (m_flagSkipInit == true && m_noiseRatio < 0){[m
[32m	    m_virtualLayer = true;     // this layer is just a virtual layer[m
[32m	    this->clearOutputBuffer(); // clear the output memory (optional)[m
[32m	}else{[m
[32m	    m_virtualLayer = false;[m
[32m	}[m
    }	[m

    // Destructor[m
[36m@@ -155,80 +164,114 @@[m [mnamespace layers{[m
    SkipAddLayer<TDevice>::~SkipAddLayer()[m
    {[m
    }[m

    [32mtemplate <typename TDevice>[m
[32m    typename SkipAddLayer<TDevice>::real_vector& SkipAddLayer<TDevice>::outputs()[m
[32m    {[m
[32m	if (m_virtualLayer)[m
[32m	    return this->precedingLayer().outputs();[m
[32m	else[m
[32m	    return this->_outputs();[m
[32m    }[m
    
    // NN forward[m
    template <typename TDevice>[m
    void SkipAddLayer<TDevice>::computeForwardPass(const int nnState)[m
    {[m

	// initialization [32mfor backward pass[m
[32m	// (because gradients will be accumulated from multiple layers)[m
	if [31m(m_noiseRatio > 0){[m
[31m	    thrust::counting_iterator<unsigned int> index_sequence_begin(0);[m
[31m	    thrust::transform(index_sequence_begin,[m
[31m			      (index_sequence_begin[m[32m(this->flagTrainingMode()){[m
[32m	    thrust::fill(this->outputErrors().begin(), [m
[32m			 (this->outputErrors().begin()[m + 
			  this->curMaxSeqLength() * this->parallelSequences() * this->size()),
			 [31mthis->outputs().begin(),[m
[31m			      internal::tempPrg(-1.0 * m_noiseRatio, m_noiseRatio,[m
[31m						(int)(GetRandomNumber() * 10000.0)));[m
[31m	}else{[m
[31m	    thrust::fill(this->outputs().begin(), [m
[31m			 (this->outputs().begin()[m[32m0.0);[m

[32m	    thrust::fill(this->outputErrorsFromSkipLayer().begin(),[m
[32m			 (this->outputErrorsFromSkipLayer().begin()[m + 
			  this->curMaxSeqLength() * this->parallelSequences() * this->size()),[m
			 0.0);[m
	}[m

	[32m// processing[m
[32m	if (m_virtualLayer){[m
[32m	    // if virtual Layer, no need to do anything[m
[32m	    return;[m
[32m	}else{[m
[32m	    // initialization[m
[32m	    if (m_noiseRatio > 0){[m
[32m		thrust::counting_iterator<unsigned int> index_sequence_begin(0);[m
[32m		thrust::transform([m
[32m		    index_sequence_begin,[m
[32m		    (index_sequence_begin +[m
[32m		     this->curMaxSeqLength() * this->parallelSequences() * this->size()),[m
[32m		    this->outputs().begin(),[m
[32m		    internal::tempPrg(-1.0 * m_noiseRatio, m_noiseRatio,[m
[32m				      (int)(GetRandomNumber() * 10000.0)));[m
[32m	    }else{[m
[32m		thrust::fill(this->outputs().begin(), [m
[32m			     (this->outputs().begin() + [m
[32m			      this->curMaxSeqLength() * this->parallelSequences() * this->size()),[m
[32m			     0.0);[m
[32m	    }[m
	[m
	    //[31minitialization for backward pass[m
[31m	thrust::fill(this->outputErrors().begin(), [m
[31m		     (this->outputErrors().begin() + [m
[31m		      this->curMaxSeqLength() * this->parallelSequences() * this->size()),[m
[31m		     0.0[m
[31m		     );[m

[31m	thrust::fill(this->outputErrorsFromSkipLayer().begin(),[m
[31m		     (this->outputErrorsFromSkipLayer().begin() + [m
[31m		      this->curMaxSeqLength() * this->parallelSequences() * this->size()),[m
[31m		     0.0);[m

[31m	//[m accumulating the outputs of previous layers
	    BOOST_FOREACH (Layer<TDevice> *layer, m_preLayers) {
		[31mthrust::transform(layer->outputs().begin(),[m[32mthrust::transform([m
[32m		    layer->outputs().begin(),[m
		    (layer->outputs().begin() + 
		     this->curMaxSeqLength() * this->parallelSequences() * this->size()),
		    this->outputs().begin(),
		    this->outputs().begin(),
		    [31mthrust::plus<real_t>()[m
[31m			      );[m[32mthrust::plus<real_t>());	    [m
[32m	    }[m
	}
    }[m

    // NN forward[m
    template <typename TDevice>[m
    void SkipAddLayer<TDevice>::computeForwardPass(const int timeStep, const int nnState)[m
    {[m
	[32m// absolute time[m
	int effTimeS = timeStep     * this->parallelSequences();[m
	int effTimeE = (timeStep+1) * this->parallelSequences();[m
	
	[31mif (m_noiseRatio > 0){[m
[31m	    thrust::counting_iterator<unsigned int> index_sequence_begin(0);[m
[31m	    thrust::transform(index_sequence_begin  + effTimeS * this->size(),[m
[31m			      index_sequence_begin  + effTimeE * this->size(),[m
[31m			      this->outputs().begin() + effTimeS[m[32m// shift of the pointer to the data [m
[32m	int shiftIn  = 0; // value to assigned layer[m
[32m	int shiftOut = this->outputBufPtrBias(timeStep[m * [31mthis->size(),[m
[31m			      internal::tempPrg(-1.0 * m_noiseRatio, m_noiseRatio));[m[32mthis->parallelSequences(), nnState);[m
[32m	[m
[32m	if (m_virtualLayer){[m
[32m	    // virtual layer, no need to do anything[m
[32m	    return;[m
	}else{[m
	    // [32mphysical layer, compute the output[m
[32m	    if (m_noiseRatio > 0){[m
[32m		// initialize the output buffer with noise[m
[32m		thrust::counting_iterator<unsigned int> index_sequence_begin(0);[m
[32m		thrust::transform(index_sequence_begin    + effTimeS * this->size(),[m
[32m				  index_sequence_begin    + effTimeE * this->size(),[m
[32m				  this->outputs().begin() + effTimeS * this->size() - shiftOut,[m
[32m				  internal::tempPrg(-1.0 * m_noiseRatio, m_noiseRatio));[m
[32m	    }else{[m
[32m		//[m initialization without noise
		thrust::fill(this->outputs().begin() + effTimeS * [31mthis->size(),[m[32mthis->size() - shiftOut,[m 
			     this->outputs().begin() + effTimeE * [31mthis->size(),[m[32mthis->size() - shiftOut,[m 
			     0.0);
	    [32m}[m

[32m	    //int cnt = 0;[m
[32m	    //accumulating the outputs of previous layers[m
[32m	    BOOST_FOREACH (Layer<TDevice> *layer, m_preLayers) {[m
[32m		//if (m_preLayers.size() && cnt == 22)[m
[32m		shiftIn = layer->outputBufPtrBias(timeStep * this->parallelSequences(), nnState);[m
[32m		thrust::transform(layer->outputs().begin() + effTimeS * this->size() - shiftIn,[m
[32m				  layer->outputs().begin() + effTimeE * this->size() - shiftIn,[m
[32m				  this->outputs().begin()  + effTimeS * this->size() - shiftOut,[m
[32m				  this->outputs().begin()  + effTimeS * this->size() - shiftOut,[m
[32m				  thrust::plus<real_t>());[m
[32m		//cnt++;[m
[32m	    }[m
	}[m
[31m	[m
[31m	// accumulating the outputs of previous layers[m
[31m	BOOST_FOREACH (Layer<TDevice> *layer, m_preLayers) {[m
[31m	    thrust::transform(layer->outputs().begin() + effTimeS * this->size(),[m
[31m			      layer->outputs().begin() + effTimeE * this->size(),[m
[31m			      this->outputs().begin()  + effTimeS * this->size(),[m
[31m			      this->outputs().begin()  + effTimeS * this->size(),[m
[31m			      thrust::plus<real_t>()[m
[31m			      );	    [m
[31m	}	[m
    }[m


[36m@@ -236,7 +279,8 @@[m [mnamespace layers{[m
    template <typename TDevice>[m
    void SkipAddLayer<TDevice>::computeBackwardPass(const int nnState)[m
    {[m
	// [32mBoth physical and virtual layers need to handle the gradients[m
	
	// at first, add the errors in both this->outputErrorsFromSkipLayer() and m_outputErrors[m
	thrust::transform(this->outputErrorsFromSkipLayer().begin(),[m
			  (this->outputErrorsFromSkipLayer().begin() + [m
[36m@@ -281,8 +325,7 @@[m [mnamespace layers{[m

    template <typename TDevice>[m
    typename SkipAddLayer<TDevice>::real_vector& SkipAddLayer<TDevice>::outputFromGate()[m
    {	
	return this->outputFromGate();[m
    }[m
    [m
[36m@@ -318,6 +361,31 @@[m [mnamespace layers{[m
    }[m
    */[m

    [32mtemplate <typename TDevice>[m
[32m    void SkipAddLayer<TDevice>::reduceOutputBuffer()[m
[32m    {[m
[32m	if (m_virtualLayer){[m
[32m	    // this->clearOutputBuffer() // this has been done[m
[32m	}else{[m
[32m	    this->resizeOutputBuffer(this->parallelSequences() * this->size());[m
[32m	    this->setSaveMemoryFlag(true);[m
[32m	    printf("\t[mem saved]");[m
[32m	}[m
[32m    }[m

[32m    template <typename TDevice>[m
[32m    int SkipAddLayer<TDevice>::outputBufPtrBias(const int timeStepTimesParallel, const int nnState)[m
[32m    {[m
[32m	if (m_virtualLayer){[m
[32m	    return this->precedingLayer().outputBufPtrBias(timeStepTimesParallel, nnState);[m
[32m	}else if (this->getSaveMemoryFlag()){[m
[32m	    return timeStepTimesParallel * this->size();[m
[32m	}else{[m
[32m	    return 0;[m
[32m	}[m
[32m    }[m

    
    template class SkipAddLayer<Cpu>;[m
    template class SkipAddLayer<Gpu>;[m
    [m
[1mdiff --git a/layers/SkipAddLayer.hpp b/layers/SkipAddLayer.hpp[m
[1mindex f9c5860..f31038b 100644[m
[1m--- a/layers/SkipAddLayer.hpp[m
[1m+++ b/layers/SkipAddLayer.hpp[m
[36m@@ -53,7 +53,8 @@[m [mnamespace layers {[m
	// to receive the errors directly from next skip add layer[m
	// real_vector       m_outputErrorsFromSkipLayer;[m

	bool                         m_flagSkipInit; // this layer SkipInit or SkipAdd
	[32mbool                         m_virtualLayer;[m
	real_t                       m_noiseRatio;[m
	std::string                  m_previousSkipStr;[m
    public:[m
[36m@@ -93,6 +94,12 @@[m [mnamespace layers {[m
	virtual void exportLayer(const helpers::JsonValue &layersArray,[m
				 const helpers::JsonAllocator &allocator) const;[m

	[32mvirtual real_vector& outputs();[m

[32m	virtual void reduceOutputBuffer();[m

[32m	virtual int outputBufPtrBias(const int timeStepTimesParallel, const int nnState);[m
	
    };[m

}[m
[1mdiff --git a/layers/SkipCatLayer.cu b/layers/SkipCatLayer.cu[m
[1mindex 2c01c28..80d8563 100644[m
[1m--- a/layers/SkipCatLayer.cu[m
[1m+++ b/layers/SkipCatLayer.cu[m
[36m@@ -271,6 +271,10 @@[m [mnamespace layers{[m

	    int cnt = 0;[m
	    BOOST_FOREACH (Layer<TDevice> *layer, m_preLayers) {[m
		
		[32mif (layer->getSaveMemoryFlag())[m
[32m		    throw std::runtime_error("SkipCat is not ready for reduced memory input");[m

		fn.tarS    = m_preSkipDimAccu[cnt/2];[m
		fn.source  = helpers::getRawPointer(layer->outputs());[m
		fn.srcDim  = layer->size();[m
[1mdiff --git a/layers/SkipLayer.cu b/layers/SkipLayer.cu[m
[1mindex 10d7819..e9efa07 100644[m
[1m--- a/layers/SkipLayer.cu[m
[1m+++ b/layers/SkipLayer.cu[m
[36m@@ -42,7 +42,8 @@[m [mnamespace layers{[m
	: TrainableLayer<TDevice>(layerChild, weightsSection,[m
				  (trainable ? 1 : 0), 0, *(precedingLayers.back()))[m
    {[m
	[32mif (this->flagTrainingMode())[m
	    m_outputErrorsFromSkipLayer = Cpu::real_vector(this->outputs().size(), (real_t)0.0);
    }	[m

    // Destructor[m
[1mdiff --git a/layers/SkipParaLayer.cu b/layers/SkipParaLayer.cu[m
[1mindex 117b6c0..5dc9ff2 100644[m
[1m--- a/layers/SkipParaLayer.cu[m
[1m+++ b/layers/SkipParaLayer.cu[m
[36m@@ -251,11 +251,11 @@[m [mnamespace layers{[m
	// initialization for backward pass [m
	// (put it here just for convience, it is complicated to initialize the errors[m
	//  in backward pass since this layer links to multiple layers)[m
	[32mif (this->flagTrainingMode())[m
	    thrust::fill(this->outputErrors().begin(), 
			 (this->outputErrors().begin() + 
			  this->curMaxSeqLength() * this->parallelSequences() * this->size()),
			 [31m0.0[m
[31m		     );[m[32m0.0);[m
	[m
	// Do the Forward Pass[m
	// calculate the gate output (in the same way as feed-forward layer, but on the gate unit)[m
[36m@@ -325,10 +325,13 @@[m [mnamespace layers{[m
    template <typename TDevice, typename TActFn>[m
    void SkipParaLayer<TDevice, TActFn>::computeForwardPass(const int timeStep, const int nnState)[m
    {[m

	[32mif (this->precedingLayer().getSaveMemoryFlag() || this->preSkipLayer()->getSaveMemoryFlag())[m
[32m	    throw std::runtime_error("SkipPara layer is not ready for reduced memory input");[m
	
	int effTimeS = timeStep     * this->parallelSequences();[m
	int effTimeE = (timeStep+1) * this->parallelSequences();[m

[31m	[m
	// Do the Forward Pass[m
	// calculate the gate output (in the same way as feed-forward layer, but on the gate unit)[m
	// step1: linear transform[m
[1mdiff --git a/layers/TrainableLayer.cu b/layers/TrainableLayer.cu[m
[1mindex ac01eeb..58eab0f 100644[m
[1m--- a/layers/TrainableLayer.cu[m
[1m+++ b/layers/TrainableLayer.cu[m
[36m@@ -83,8 +83,10 @@[m [mnamespace layers {[m
                                            int inputWeightsPerBlock, [m
					    int internalWeightsPerBlock, [m
					    Layer<TDevice> &precedingLayer)[m
        : Layer<TDevice>           (layerChild,
				    precedingLayer.parallelSequences(), 
				    [31mprecedingLayer.maxSeqLength())[m[32mprecedingLayer.maxSeqLength(),[m
[32m				    Configuration::instance().trainingMode())[m
        , m_precedingLayer         (precedingLayer)[m
        , m_inputWeightsPerBlock   (inputWeightsPerBlock)[m
        , m_internalWeightsPerBlock(internalWeightsPerBlock)[m
[36m@@ -108,6 +110,9 @@[m [mnamespace layers {[m
	// extract the weights if they are given in the network file[m
        Cpu::real_vector weights;[m
[m
	[32mif (m_learningRate == 0)[m
[32m	    printf("\n\tlearning rate = 0\n");[m
	
        if (weightsSection.isValid() && weightsSection->HasMember(this->name().c_str())) {[m
	    printf("Trainable layer: re-read weight");[m
            if (!weightsSection->HasMember(this->name().c_str()))[m
[1mdiff --git a/layers/dustbin.txt b/layers/dustbin.txt[m
[1mindex c78958d..243d2d7 100644[m
[1m--- a/layers/dustbin.txt[m
[1m+++ b/layers/dustbin.txt[m
[36m@@ -1733,3 +1733,356 @@[m [mBlock20170702x07[m
         // set the previous frame to zero if it is silence[m
      }[m




[32m---------------[m
[32mBlock20170904x01[m
[32m	    /* Code based on Thrust parallel */[m
[32m	    /*{{[m
[32m		internal::ReadInput fn;[m
[32m		fn.sourceW   = helpers::getRawPointer(fraction.inputs());[m
[32m		fn.targetW   = helpers::getRawPointer(m_weBufferInput);[m
[32m		fn.weBank    = helpers::getRawPointer(m_weBank);[m
[32m		fn.sourceDim = fraction.inputs().size()/fracTime;[m
[32m		fn.targetDim = this->size();[m
[32m		fn.weDim     = m_weDim;[m
[32m		fn.weIdxDim  = m_weIDDim;[m
[32m		[m
[32m		int n = fracTime * this->size();[m
[32m		thrust::for_each(thrust::host, [m
[32m				 thrust::counting_iterator<int> (0),[m
[32m				 thrust::counting_iterator<int> (0)+n,[m
[32m				 fn);[m
[32m		[m
[32m		thrust::copy(m_weBufferInput.begin(),[m
[32m			     m_weBufferInput.begin()+n,[m
[32m			     this->_outputs().begin());[m
[32m	    }}[m
[32m	    */[m
[32m	    [m
[32mBlock20170904x02[m
[32m    /*[m
[32m    struct ReadInput[m
[32m    {[m
[32m	const real_t *sourceW;[m
[32m	real_t       *targetW;[m
[32m	const real_t *index;[m
[32m	int           sourceDim;[m
[32m	int           targetDim;[m
[32m	int           startDim;[m
[32m	[m
[32m	__host__ __device__ void operator() (const int idx) const[m
[32m	{[m
[32m	    int dim  = (idx % targetDim);[m
[32m	    int time = (idx / targetDim);[m
[32m	    int sourcePos = index[time] * sourceDim + dim;[m
[32m	    int targetPos = time * targetDim + dim + startDim;[m
[32m	    *(targetW + targetPos) = *(sourceW + sourcePos);[m
[32m	}[m
[32m    };[m
[32m    */    [m


[32mBlock20170904x03[m
[32m// copy the data from t.get<0> to the memory block started from Output [m
[32m    struct CopySimple[m
[32m    {[m
[32m	real_t     *Output;           // output address to store data[m
[32m	int         paraDim;[m
[32m	[m
[32m	const char *patTypes;     // sentence termination check[m
[32m	[m
[32m        __host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const[m
[32m        {[m
[32m            // unpack the tuple[m
[32m            int outputIdx = t.get<1>();[m
[32m	    int timeIdx   = outputIdx / paraDim;[m

[32m	    // skip dummy frame (for parallel sentence processing)[m
[32m	    if (patTypes[timeIdx] == PATTYPE_NONE)[m
[32m                return;[m

[32m            // store the result[m
[32m            *(Output+outputIdx) = t.get<0>();[m
[32m        }[m
[32m    };[m


[32mBlock20170904x04[m
[32m    // Calculate the mixture distance \sum_d (x_d-\mu_d)^2/(2*std^2) for mixture model[m
[32m    // This function is used in EM-style generation[m
[32m    //  and forward anc backward propagation of mixture unit[m
[32m    struct ComputeMixtureDistanceWithTransForm[m
[32m    {[m
[32m	int startDOut;[m
[32m	int layerSizeOut;[m
[32m	int mixture_num;[m
[32m	int featureDim;[m
[32m	int totaltime;[m
[32m	bool tieVar;[m

[32m	const char *patTypes;[m
[32m	const real_t *output;    // targets data[m
[32m	const real_t *mdnPara;   // mean value of the mixture[m
[32m	const real_t *tranData; [m
[32m	// from 1 to timesteps * num_mixture[m
[32m	__host__ __device__ real_t operator() (const int idx) const[m
[32m	{[m
[32m	    [m
[32m	    int timeStep = idx / mixture_num; //t.get<0>();[m
[32m	    int mixIndex = idx % mixture_num; //t.get<1>(); [m
[32m	    [m
[32m	    // point to the targets data x[m
[32m	    int pos_data = (layerSizeOut * timeStep)+startDOut;[m
[32m		[m
[32m	    const real_t *data, *mean, *var, *trans;[m

[32m	    if (patTypes[timeStep] == PATTYPE_NONE)[m
[32m		return 0;[m
[32m	    [m
[32m	    // point to the mixture data (mean and variance)[m
[32m	    int pos =  totaltime * mixture_num;[m
[32m	    int pos_mean = pos+timeStep*featureDim*mixture_num+mixIndex*featureDim;[m
[32m	    pos     =  totaltime * (mixture_num + mixture_num * featureDim);[m

[32m            #ifdef ALTER_TIEVAR[m
[32m	    int pos_var  = pos+timeStep*mixture_num;[m
[32m            #else[m
[32m	    int pos_var  = pos+ (tieVar?[m
[32m				 (timeStep * mixture_num + mixIndex) :[m
[32m				 (timeStep * mixture_num * featureDim + mixIndex*featureDim));[m
[32m            #endif[m
[32m	    var  = mdnPara + pos_var;[m
[32m	    [m
[32m	    // pointer to the transformation part [m
[32m	    int pos_trans = idx;[m
[32m	    [m
[32m	    // accumulate the distance over dimension[m
[32m	    real_t tmp = 0.0;[m
[32m	    for (int i = 0; i<featureDim; i++){[m
[32m		data = output    + pos_data + i;[m
[32m		mean = mdnPara   + pos_mean + i;[m
[32m		var  = mdnPara   + pos_var  + (tieVar?0:i);[m
[32m		trans= tranData  + pos_trans+ i;[m
[32m		tmp += (*data-*mean-*trans)*(*data-*mean-*trans)/((*var)*(*var))/2;[m
[32m		[m
[32m	    }[m
[32m	    return tmp;[m
[32m	}[m
[32m    };[m


[32m    // Copy the data from the output buffer to the target unit (for mixture_dyn unit)[m
[32m    struct CopyTargetData[m
[32m    {[m
[32m	int startDOut;[m
[32m	int layerSizeOut;[m
[32m	int featureDim;[m

[32m	const char *patTypes;[m
[32m	const real_t *output;   // targets data[m
[32m	real_t *target;   // [m

[32m	// from 1 to timesteps * num_mixture[m
[32m	__host__ __device__ void operator() (const int idx) const[m
[32m	{[m
[32m	    [m
[32m	    int timeStep  = idx / featureDim; //t.get<0>();[m
[32m	    int featIndex = idx % featureDim; //t.get<1>(); [m
[32m	    [m
[32m	    // point to the targets data x[m
[32m	    int pos_data = (layerSizeOut * timeStep)+startDOut+featIndex;[m
[32m		[m
[32m	    if (patTypes[timeStep] == PATTYPE_NONE)[m
[32m		return;[m
[32m	    *(target+idx) = *(output + pos_data);[m
[32m	    [m
[32m	}[m
[32m    };[m

[32m    // Shift the mean value u => u + w^To + b[m
[32m    struct ShiftBiasStep1[m
[32m    {[m
[32m	int featureDim;[m
[32m	int mixNum;[m
[32m	int totalTime;[m

[32m	real_t   *linearPart;   // Wx'[m
[32m	real_t   *biasPart;     // b[m
[32m	real_t   *mdnPara;      // [m

[32m	// from 1 to timesteps * num_mixture[m
[32m	__host__ __device__ void operator() (const int idx) const[m
[32m	{[m
[32m	    [m
[32m	    int temp      = idx % (featureDim * mixNum); [m
[32m	    int featIndex = temp % featureDim; [m
[32m	    int timeStep  = idx / (featureDim * mixNum);[m
[32m	    int mixIndex  = temp / featureDim;[m

[32m	    int pos_mean;[m
[32m	    // Add to the mean value[m
[32m	    pos_mean = (totalTime * mixNum + [m
[32m			timeStep  * featureDim * mixNum + [m
[32m			mixIndex  * featureDim + featIndex); [m
[32m		[m
[32m	    if (timeStep == 0){[m
[32m		// skip the first time step[m
[32m		return;[m
[32m	    }else{[m
[32m		*(mdnPara + pos_mean) = (*(mdnPara + pos_mean) + [m
[32m					 *(linearPart + idx)   + [m
[32m					 *(biasPart   + mixIndex * featureDim + featIndex)[m
[32m					 );[m
[32m	    }	    	    	    [m
[32m	}[m
[32m    };[m

[32m    // Accumulating the statistics for BP on the linear regression part W^T o+b[m
[32m    // -1 * posteriorP(k) * (O_t - (u + W_k ^ T O_t-1 + b_k)) / var^k_d / var^k_d[m
[32m    struct ShiftBiasStep2[m
[32m    {[m
[32m	int featureDim;[m
[32m	int mixNum;[m
[32m	int totalTime;[m
[32m	int startDOut;[m
[32m	int layerSizeOut;[m

[32m	real_t   *linearPart;   // Wx'[m
[32m	real_t   *biasPart;     // b[m
[32m	real_t   *target;       // x[m
[32m	real_t   *mdnPara;      // [m
[32m	real_t   *postPbuff;[m
[32m	bool    tieVar;[m

[32m	// from 1 to timesteps * num_mixture[m
[32m	__host__ __device__ void operator() (const int idx) const[m
[32m	{[m
[32m	    [m
[32m	    int temp      = idx % (featureDim * mixNum); [m
[32m	    int featIndex = temp % featureDim; [m
[32m	    int timeStep  = idx / (featureDim * mixNum);[m
[32m	    int mixIndex  = temp / featureDim;[m

[32m	    // skip the first time step[m
[32m	    if (timeStep == 0)[m
[32m		return;[m
[32m	    [m
[32m	    // set the pointer[m
[32m	    int pos_mean, pos_var, pos_data;[m
[32m	    pos_mean = (totalTime * mixNum + [m
[32m			timeStep  * featureDim * mixNum + [m
[32m			mixIndex  * featureDim + featIndex); [m
[32m	    pos_var  = (totalTime * (mixNum + mixNum * featureDim)      + [m
[32m			timeStep  *  mixNum * (tieVar ? 1 : featureDim) + [m
[32m			mixIndex  * (tieVar ? 1 : featureDim)           +[m
[32m			(tieVar ? 0 : featIndex)); [m
[32m	    [m
[32m	    /*** FATAL ERROR **[m
[32m	     * Posterior probability should be updated [m
[32m	     * Particularly, the size of the posterior probability buffer  will change !!![m
[32m	     * Split ShiftBias into ShiftBiasStep1 and ShiftBiasStep2[m
[32m	     ******/[m
[32m	    // pointer to the posterior P and sum of posterior P[m
[32m	    const real_t *postP   = postPbuff + timeStep  * mixNum + mixIndex;[m
[32m	    const real_t *sumPost = postPbuff + totalTime * mixNum + timeStep;[m
[32m	    real_t posterior = helpers::safeExp((*postP) - (*sumPost));[m
[32m	    [m
[32m	    // point to the targets data x[m
[32m	    pos_data = (layerSizeOut * timeStep) + startDOut + featIndex;[m
[32m	    [m
[32m	    // save x - u - wx'-b to dataBuff now[m
[32m	    *(linearPart + idx) = (-1 * posterior * [m
[32m				   (*(target + pos_data) - *(mdnPara + pos_mean)) /[m
[32m				   (*(mdnPara + pos_var)) / (*(mdnPara + pos_var)));[m
[32m	    // No need to re-order the data[m
[32m	    // save \phi(i)\sigma()(x-u-wx'-b) in time order[m
[32m	    // pos_data = (mixIndex * totalTime + timeStep) * featureDim + featIndex; [m
[32m	    // *(tmpBuff+pos_data)=-1* posterior * (*(mdnPara + pos_var)) * (*(linearPart + idx));[m
[32m	    [m
[32m	}[m
[32m    };[m

[32mBlock20170904x05[m

[32m    struct ShiftBiasStep1TiedCaseDimensionAxis[m
[32m    {[m
[32m	// Shift the mean value u => u + w^To + b[m
[32m	// This function is used by mixture_dyn and mixture_dynSqr[m
[32m	// The difference is the source of the parameter w and b[m
[32m	int startDOut;[m
[32m	int layerSizeOut;[m
[32m	int featureDim;[m
[32m	int mixNum;[m
[32m	int totalTime;[m
[32m	int trainableAPos;      // w, the w predicted by the network, I name it as a now[m
[32m	int trainableBPos;      // b, the b which is predicted by the network[m
[32m	int stepBack;           // how many steps to look back ?[m

[32m	real_t   *linearPart;   // w, where w is trainable but shared across time steps[m
[32m	real_t   *biasPart;     // b, where b is trainable but shared across time steps[m
[32m	real_t   *targets;      // o_t-1[m
[32m	real_t   *mdnPara;      // [m
[32m	[m
[32m	bool      tieVar;[m

[32m	// from 1 to timesteps * num_mixture[m
[32m	__host__ __device__ void operator() (const int idx) const[m
[32m	{[m
[32m	    [m
[32m	    int timeStep  = idx  / (featureDim * mixNum);[m
[32m	    int temp      = idx  % (featureDim * mixNum); [m
[32m	    int mixIndex  = temp /  featureDim;[m
[32m	    int featIndex = temp %  featureDim;[m
[32m	    [m
[32m	    if (featIndex < stepBack){[m
[32m		// skip the first dimension[m
[32m		return;[m
[32m	    }[m
[32m	    [m
[32m	    int pos_mean, pos_data;[m
[32m	    // Add to the mean value[m
[32m	    pos_mean = (totalTime * mixNum + [m
[32m			timeStep  * featureDim * mixNum + [m
[32m			mixIndex  * featureDim + featIndex); [m
[32m	    pos_data = (timeStep  * layerSizeOut) + startDOut + featIndex - stepBack;[m
[32m	    [m
[32m	    if (linearPart != NULL && biasPart != NULL){[m
[32m		*(mdnPara + pos_mean) = ((*(mdnPara      + pos_mean))  + [m
[32m					 ((*(linearPart)) * (*(targets+pos_data))) + [m
[32m					 ((stepBack==1)  ? (*(biasPart)):0)[m
[32m					 );[m
[32m	    }else{[m
[32m		/* not implemented for context-dependent case */[m
[32m	    }[m
[32m	}[m
[32m    };[m

[32mBlock20170904x06[m
[32m    struct TanhAutoRegWeightStep1[m
[32m    {[m
[32m	int     featureDim;[m
[32m	int     backOrder;[m
[32m	real_t *weight;[m
[32m	real_t *weightOut;[m
[32m        __host__ __device__ void operator() (const int &Idx) const[m
[32m        {[m
[32m            *(weightOut + Idx) =  (Idx < (backOrder * featureDim))? [m
[32m		(activation_functions::Tanh::fn(*(weight+Idx))):0;[m
[32m        }[m
[32m    };[m

[32m    struct TanhAutoRegWeightStep2[m
[32m    {[m
[32m	int     featureDim;[m
[32m	real_t *weight;[m
[32m	real_t *weightOut;[m
[32m        __host__ __device__ void operator() (const int &idx) const[m
[32m        {[m
[32m	    *(weightOut + idx) = (idx < featureDim) ? [m
[32m		((*(weight + idx)) + (*(weight + idx + featureDim))) : [m
[32m		(-1 * (*(weight + idx - featureDim)) * (*(weight + idx)));[m
[32m        }[m
[32m    };[m

[1mdiff --git a/layers/vaeMiddleLayer.cu b/layers/vaeMiddleLayer.cu[m
[1mindex da6501e..94db26f 100644[m
[1m--- a/layers/vaeMiddleLayer.cu[m
[1m+++ b/layers/vaeMiddleLayer.cu[m
[36m@@ -70,13 +70,27 @@[m [mnamespace {[m

    struct FrameNum[m
    {[m
	[32mint  featureDim;[m
	const char *patTypes;
	[32mconst real_t *nPara;[m
	__host__ __device__ real_t operator() (const thrust::tuple<const real_t&, const int&> &t) const[m
	{[m
	    [32mconst int timeStep = t.get<1>() / featureDim;[m
[32m	    const int dimIndex = t.get<1>() % featureDim;[m
	    
	    if [31m(patTypes[t.get<1>()][m[32m(patTypes[timeStep][m == [31mPATTYPE_NONE)[m[32mPATTYPE_NONE){[m
		return [31m0.0;[m
[31m	    else[m[32m0;		[m
[32m	    }else{[m
[32m		[m
[32m		real_t std  = nPara[timeStep * featureDim * 2 + dimIndex + featureDim];[m
[32m		real_t mean = nPara[timeStep * featureDim * 2 + dimIndex];[m
[32m		if (std == 0 && mean == 0){[m
[32m		    // When the input is set to zero, this timestep is assumed to be dummy[m
[32m		    return 0;[m
[32m		}else{[m
		    return [31m1.0;[m[32m1;[m
[32m		}[m
[32m	    }[m
	}[m
    };[m

[36m@@ -135,6 +149,31 @@[m [mnamespace {[m
	}[m
    };[m

    [32mstruct vaeNoiseNontransform[m
[32m    {[m
[32m	int     noiseDim;[m
[32m	real_t *nPara;     // mean and std of each frame[m
[32m	[m
[32m	const real_t *stdNoise;  // noise from N(0, 1)[m
[32m	const char   *patTypes;  [m
[32m	[m
[32m	__host__ __device__ void operator() (const thrust::tuple<real_t&, const int&> &t) const[m
[32m	{[m
[32m	    const int timeStep = t.get<1>() / noiseDim;[m
[32m	    const int dimIndex = t.get<1>() % noiseDim;[m
[32m	    if (patTypes[timeStep] == PATTYPE_NONE){[m
[32m		t.get<0>() = 0;[m
[32m	    }else{[m
[32m		if (nPara[timeStep*noiseDim*2+dimIndex+noiseDim] == 0 &&[m
[32m		    nPara[timeStep * noiseDim * 2 + dimIndex]    == 0)[m
[32m		    t.get<0>() = 0.0;[m
[32m		else[m
[32m		    t.get<0>() = stdNoise[timeStep * noiseDim + dimIndex];[m
[32m		[m
[32m	    }[m
[32m	}[m
[32m    };[m

    struct vaeNoiseTransform[m
    {[m
	int     noiseDim;[m
[36m@@ -155,10 +194,16 @@[m [mnamespace {[m
		real_t std  =  helpers::safeLog([m
				helpers::safeExp(nPara[timeStep*noiseDim*2+dimIndex+noiseDim])+1.0);[m
		real_t mean = nPara[timeStep * noiseDim * 2 + dimIndex];[m

		[32mif (nPara[timeStep*noiseDim*2+dimIndex+noiseDim] == 0 &&[m
[32m		    mean == 0){[m
[32m		    // the previous input is a all zero vector[m
[32m		    // (probably from the Operator last shot)[m
[32m		    t.get<0>() = 0.0;[m
[32m		}else{[m
		    // transform noise
		    t.get<0>() = std * stdNoise[timeStep * noiseDim + dimIndex] + mean;
		[32m}[m
		// save std[m
		// nPara[timeStep * noiseDim * 2 + dimIndex + noiseDim] = std;[m
	    }[m
[36m@@ -177,13 +222,18 @@[m [mnamespace {[m
	    const int dimIndex = t.get<1>() % featureDim;[m
	    [m
	    if (patTypes[timeStep] == PATTYPE_NONE){[m
		return 0;		
	    }else{[m
		
		real_t std  = nPara[timeStep * featureDim * 2 + dimIndex + featureDim];[m
[31m		std = helpers::safeLog(helpers::safeExp(std) + 1.0);[m
		real_t mean = nPara[timeStep * featureDim * 2 + dimIndex];[m
		[32mif (std == 0 && mean == 0){[m
[32m		    // When the input is set to zero, this timestep is assumed to be dummy[m
[32m		    return 0;[m
[32m		}else{[m
[32m		    std = helpers::safeLog(helpers::safeExp(std) + 1.0);[m
		    return (helpers::safeLog(std) * 2.0 + 1.0 - mean * mean - std * std);
		[32m}[m
	    }[m
	}[m

[36m@@ -205,20 +255,33 @@[m [mnamespace {[m
	    [m
	    if (patTypes[timeStep] == PATTYPE_NONE){[m
		t.get<0>() = 0.0;[m
		
	    }else{[m
		
		if (dimIndex >= noiseDim){[m
		    // std part[m
		    real_t std  = [31mhelpers::safeLog(helpers::safeExp(nPara[numIndex])+1.0);[m
[31m		    [m
[31m		    int    noiseIndex[m[32mnPara[numIndex];[m
[32m		    real_t mean[m = [31mtimeStep * noiseDim + dimIndex[m[32mnPara[numIndex[m - [31mnoiseDim;[m[32mnoiseDim];[m
		    [m
		    [32mif (std == 0 && mean == 0){[m
[32m			t.get<0>() = 0;[m
[32m		    }else{[m
[32m			int    noiseIndex =  timeStep * noiseDim + dimIndex - noiseDim;[m
[32m			std = helpers::safeLog(helpers::safeExp(std)+1.0);[m
			t.get<0>()  = (grad[noiseIndex] * noise[noiseIndex] + std - 1.0 / std) /
			    (1.0+helpers::safeExp(-1 * nPara[numIndex]));
		    [32m}[m
		}else{[m
		    // mean part[m
[31m//[m		    real_t mean  = nPara[numIndex];
		    [32mreal_t std   = nPara[numIndex + noiseDim];[m
[32m		    [m
[32m		    if (std == 0 && mean == 0){[m
[32m			t.get<0>() = 0;[m
[32m		    }else{[m
			int    noiseIndex =  timeStep * noiseDim + dimIndex;
			t.get<0>()  = grad[noiseIndex]  + nPara[numIndex];
		    [32m}[m
		}[m
	    }[m
	}[m
[36m@@ -258,10 +321,13 @@[m [mnamespace layers{[m
	if (precedingLayer.size() != (this->size() * 2)){[m
	    printf("Layer size: previous %d, this layer %d", precedingLayer.size(), this->size());[m
	    throw std::runtime_error("vae's layer size should = 1/2 of previous layer size");[m
	}	
	printf("\n\tVAE interface between encoder and decoder");[m

	[32mif (m_noiseRepeat) printf("\n\tVAE noise repeat across frames");[m
[32m	[m
[32m	// Note: in vaeNoiseTransform, it is assumed that, when the input mean and variance[m
[32m	//       parameters are zero, the output noise should be set to zero[m
    }[m

    template <typename TDevice>[m
[36m@@ -275,7 +341,7 @@[m [mnamespace layers{[m
    {[m
	Layer<TDevice>::loadSequences(fraction, nnState);[m

	if (nnState == [31mNN_STATE_GAN_GENERATION_STAGE[m[32mNN_STATE_GENERATION_STAGE[m && this->size() == 2 && m_vaeUsageOpt==1){
	    printf("Plot manifold");[m
	    real_vector tmp(fraction.outputs().size());[m
	    thrust::copy(fraction.outputs().begin(), fraction.outputs().end(), tmp.begin());[m
[36m@@ -310,28 +376,30 @@[m [mnamespace layers{[m
	// calculate the KL divergence[m
	real_t kld = 0.0;[m
	{{[m
	    int n =this->curMaxSeqLength() * [31mthis->parallelSequences();[m[32mthis->parallelSequences() * this->size();[m
	    [m
	    // count the number of valid frames[m
	    int frameNum = 0;[m
	    [31mif[m[32m//if[m (this->parallelSequences()>1){
	    internal::FrameNum fn3;
	    [32mfn3.featureDim= this->size();[m
	    fn3.patTypes  = helpers::getRawPointer(this->patTypes());
	    [32mfn3.nPara     = helpers::getRawPointer(this->precedingLayer().outputs());[m
	    frameNum      = thrust::transform_reduce(
				thrust::make_zip_iterator([m
				   [31mthrust::make_tuple(this->outputs().begin(),[m[32mthrust::make_tuple(this->precedingLayer().outputs().begin(),[m 
						      thrust::counting_iterator<int>(0))),
				thrust::make_zip_iterator([m
				   [31mthrust::make_tuple(this->outputs().begin()[m[32mthrust::make_tuple(this->precedingLayer().outputs().begin()[m + n, 
						      thrust::counting_iterator<int>(0) + n)),
				fn3,[m
				(real_t)0,[m
				thrust::plus<real_t>());[m
[31m}else{[m	    frameNum = [32mframeNum / this->size();[m
[32m	    //}else{[m
[32m	    //frameNum =[m this->curMaxSeqLength();
	    [31m}[m[32m//}[m

[31m	    n = n * this->size();[m
	    // compute the KLD divergence[m
	    internal::vaeKLDivergence fn2;[m
	    fn2.featureDim = this->size();[m
[36m@@ -358,8 +426,9 @@[m [mnamespace layers{[m
    void VaeMiddleLayer<TDevice>::computeForwardPass(const int nnState)[m
    {[m

	if (nnState == [31mNN_STATE_GAN_GENERATION_STAGE[m[32mNN_STATE_GENERATION_STAGE[m && this->size() == 2 && m_vaeUsageOpt == 1){
	    printf("Plot manifold");[m
	    [32m// No need to generate anything; data has been loaded in loadSequences()[m
	    return;[m
	}[m
	[m
[36m@@ -372,7 +441,6 @@[m [mnamespace layers{[m
			  m_noiseInput.begin(),[m
			  internal::genNoise(m_noiseMean, m_noiseStd,[m
					     (int)(GetRandomNumber()*10000.0)));[m

	if (m_noiseRepeat){[m
	    internal::noiseRepeat fn;[m
	    fn.noiseDim = this->size();[m
[36m@@ -389,10 +457,15 @@[m [mnamespace layers{[m
				     thrust::counting_iterator<int>(0) + n)),[m
	       fn);[m
	}[m

	[32m// Generate the output noise[m
	[m
	[32mif (nnState == NN_STATE_GENERATION_STAGE && m_vaeUsageOpt == 2){[m
	    // [31mTransform and generate the noise[m
[31m	{{[m
[31m	    internal::vaeNoiseTransform[m[32mno transformation[m
[32m	    /*int n = this->curMaxSeqLength() * this->parallelSequences() * this->size();[m
[32m	    thrust::copy(m_noiseInput.begin(), m_noiseInput.begin()+n, this->outputs().begin());[m
[32m	    printf("vaeGenMethod=2 ");*/[m
[32m	    internal::vaeNoiseNontransform[m fn1;
	    fn1.noiseDim   = this->size();[m
	    fn1.nPara      = helpers::getRawPointer(this->precedingLayer().outputs());[m
	    fn1.stdNoise   = helpers::getRawPointer(this->m_noiseInput);[m
[36m@@ -407,18 +480,37 @@[m [mnamespace layers{[m
		  thrust::make_tuple(this->outputs().begin()           + n,[m
				     thrust::counting_iterator<int>(0) + n)),[m
	       fn1);[m
	    
	[32m}else{[m
[32m	    // Transform and generate the noise[m
[32m	    internal::vaeNoiseTransform fn1;[m
[32m	    fn1.noiseDim   = this->size();[m
[32m	    fn1.nPara      = helpers::getRawPointer(this->precedingLayer().outputs());[m
[32m	    fn1.stdNoise   = helpers::getRawPointer(this->m_noiseInput);[m
[32m	    fn1.patTypes   = helpers::getRawPointer(this->patTypes());[m

	    [31m}}[m[32mint n = timeLength * this->size();[m
[32m	    thrust::for_each([m
[32m               thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputs().begin(),[m
[32m				     thrust::counting_iterator<int>(0))),[m
[32m	       thrust::make_zip_iterator([m
[32m		  thrust::make_tuple(this->outputs().begin()           + n,[m
[32m				     thrust::counting_iterator<int>(0) + n)),[m
[32m	       fn1);[m
[32m	}[m
    }[m

    template <typename TDevice>[m
    void VaeMiddleLayer<TDevice>::computeForwardPass(const int timeStep, const int nnState)[m
    {[m
	[32mif (this->precedingLayer().getSaveMemoryFlag()){[m
[32m	    throw std::runtime_error("The layer before vae is reduced in mem");[m
[32m	}[m

	if (nnState == [31mNN_STATE_GAN_GENERATION_STAGE[m[32mNN_STATE_GENERATION_STAGE[m && this->size() == 2 && m_vaeUsageOpt == 1){
	    printf("Plot manifold");[m
	    [32m// No need to generate anything; data has been loaded in loadSequences()[m
	    return;[m
	}[m

[36m@@ -452,14 +544,32 @@[m [mnamespace layers{[m

	}[m

	if (nnState == [31mNN_STATE_GAN_GENERATION_STAGE[m[32mNN_STATE_GENERATION_STAGE[m && m_vaeUsageOpt == 2){
	    // copy the noise (all frames) to the output[m
	    // no need to loop over time to generate noise frame by frame[m
	    [32m{{[m
[32m	    internal::vaeNoiseNontransform fn1;[m
[32m	    fn1.noiseDim   = this->size();[m
[32m	    fn1.nPara      = helpers::getRawPointer(this->precedingLayer().outputs());[m
[32m	    fn1.stdNoise   = helpers::getRawPointer(this->m_noiseInput);[m
[32m	    fn1.patTypes   = helpers::getRawPointer(this->patTypes());[m

[32m	    //int n = timeLength * this->size();[m
[32m	    thrust::for_each([m
[32m               thrust::make_zip_iterator([m
[32m		thrust::make_tuple(this->outputs().begin()           + timeStep * this->size(),[m
[32m				   thrust::counting_iterator<int>(0) + timeStep * this->size())),[m
[32m	       thrust::make_zip_iterator([m
[32m	        thrust::make_tuple(this->outputs().begin()           +(timeStep+1) * this->size(),[m
[32m				   thrust::counting_iterator<int>(0) +(timeStep+1) * this->size())),[m
[32m	       fn1);[m
[32m	    }}[m
[32m		    /*[m
	    if (timeStep == 0){[m
		int n = this->curMaxSeqLength() * this->parallelSequences() * this->size();[m
		thrust::copy(m_noiseInput.begin(), m_noiseInput.begin()+n, this->outputs().begin());[m
		printf("vaeGenMethod=2 ");[m
		[31m}[m[32m}*/[m
	}else{[m
	    // Transform and generate the noise[m
	    {{[m
