diff --git a/LayerFactory.cu b/LayerFactory.cu
index 7ee2f55..d128e6c 100644
--- a/LayerFactory.cu
+++ b/LayerFactory.cu
@@ -35,6 +35,7 @@
 #include "layers/BinaryClassificationLayer.hpp"
 #include "layers/MulticlassClassificationLayer.hpp"
 #include "layers/Amalgamate.hpp"
+#include "layers/BatchNorm.hpp"
 #include "activation_functions/Tanh.cuh"
 #include "activation_functions/Logistic.cuh"
 #include "activation_functions/Identity.cuh"
@@ -91,6 +92,8 @@ layers::Layer<TDevice>* LayerFactory<TDevice>::createLayer(
     	return new FeedBackLayer<TDevice>(layerChild, weightsSection, *precedingLayer);
     else if (layerType == "amalgamate")
     	return new AmalgamateLayer<TDevice>(layerChild, weightsSection, *precedingLayer);
+    else if (layerType == "batchnorm")
+    	return new BatchNormLayer<TDevice>(layerChild, weightsSection, *precedingLayer);
     
     /*
     // not implemented yet
diff --git a/layers/FeedForwardLayer.cu b/layers/FeedForwardLayer.cu
index a28af7e..fa2b595 100644
--- a/layers/FeedForwardLayer.cu
+++ b/layers/FeedForwardLayer.cu
@@ -32,6 +32,9 @@
 #include "../activation_functions/Identity.cuh"
 #include "../activation_functions/Relu.cuh"
 
+#include "../helpers/JsonClasses.hpp"
+#include "../Configuration.hpp"
+
 #include <thrust/transform.h>
 #include <thrust/transform_reduce.h>
 #include <thrust/for_each.h>
@@ -113,20 +116,219 @@ namespace {
         }
 	};*/
 
+    // 
+    struct BatchSize
+    {
+	// over time t * parallel sentence
+	const char *patTypes;
+	
+	__host__ __device__ real_t operator() (const thrust::tuple<const real_t&, int> &t) const
+	{
+	    int timeIdx = t.get<1>();
+	    if (patTypes[timeIdx] == PATTYPE_NONE)
+		return 0.0;// skip dummy node
+	    else
+		return 1.0;
+	}
+    };
+    
+    struct PrepareForMeanStd
+    {
+	int layerSize;
+	bool   meanNotVar;
 
+	
+	const char *patTypes;   
+	real_t     *data;
+	real_t     *outdata;
+	real_t     *mean;
+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const
+	{
+	    int dataIdx = t.get<1>();
+	    int timeIdx = dataIdx / layerSize;
+	    int dimIdx  = dataIdx % layerSize;
+	    if (patTypes[timeIdx] == PATTYPE_NONE){
+		// skip dummy node
+		outdata[dataIdx] = 0.0; //
+	    }else{
+		if (meanNotVar)
+		    outdata[dataIdx] = data[dataIdx]; //
+		else
+		    outdata[dataIdx] = (data[dataIdx]-mean[dimIdx]) * (data[dataIdx]-mean[dimIdx]);
+	    }
+	}
+    };
+    struct PrepareGrad
+    {
+	int    layerSize;
+	bool   alphaNotBeta;
+	const char *patTypes;   
+	real_t     *grad;
+	real_t     *data;
+	
+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const
+	{
+	    int dataIdx = t.get<1>();
+	    int timeIdx = dataIdx / layerSize;
+	    if (patTypes[timeIdx] == PATTYPE_NONE){
+		t.get<0>() = 0.0; // skip dummy node
+	    }else{
+		if (alphaNotBeta)
+		    t.get<0>() = grad[dataIdx] * data[dataIdx];
+		else
+		    t.get<0>() = grad[dataIdx];
+	    }
+	}
+    };
+
+    struct GetStd
+    {
+	real_t  stdConst;
+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const
+	{
+	    int dimIdx = t.get<1>();
+	    t.get<0>() = sqrt(t.get<0>() +stdConst);
+	}
+    };
+
+    struct AveMeanStd
+    {
+	real_t *meanStdBuf;
+	real_t  cnt;
+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const
+	{
+	    int dimIdx = t.get<1>();
+	    meanStdBuf[dimIdx] += (t.get<0>() - meanStdBuf[dimIdx]) / cnt;
+	}
+    };
+    
+    
+
+    template <typename TActFn>
+    struct ComputeBatchNorm_Transform
+    {
+	int layerSize;
+
+	const char *patTypes;   
+	real_t *data;
+	real_t *outdata;
+	real_t *meanStd;
+	real_t *meanStdBuf;
+	real_t *scale;
+	bool    trainFlag;
+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const
+	{
+	    int dataIdx = t.get<1>();
+	    int dimIdx  = dataIdx % layerSize;
+	    int timeIdx = dataIdx / layerSize;
+	    int varIdx  = dimIdx  + layerSize;
+	    if (patTypes[timeIdx] == PATTYPE_NONE){
+		// skip dummy node
+	    }else{
+		// \hat{x} = (x - \mu) / \sigma
+		if (trainFlag)
+		    data[dataIdx] = (data[dataIdx]-meanStd[dimIdx])/meanStd[varIdx];
+		else
+		    data[dataIdx] = (data[dataIdx]-meanStdBuf[dimIdx])/meanStdBuf[varIdx];
+
+		// y =f(\alpha \hat{x} + \beta)
+		outdata[dataIdx]   = TActFn::fn(data[dataIdx] * scale[dimIdx] + scale[varIdx]);
+	    }
+	}
+    };
+    
+    struct ComputeBatchGradient_output
+    {
+	
+	int layerSize;
+
+	const char *patTypes;   
+	real_t *errors;
+	real_t *outNormed;
+	real_t *meanStd;
+	real_t *scale;
+	real_t *scaleGrad;	
+	real_t  batchSize;
+	
+	__host__ __device__ void operator() (const thrust::tuple<real_t&, int> &t) const
+	{
+	    int dataIdx      = t.get<1>();
+	    int dimIdx       = dataIdx % layerSize;
+	    int timeIdx      = dataIdx / layerSize;
+	    
+	    if (patTypes[timeIdx] == PATTYPE_NONE){
+		errors[dataIdx] = 0.0;
+	    }else{
+		// gradient =
+		// alpha / std * (\deltaE/\delta{y} - \deltaE/\deltaBeta / batchSize -
+		//                \deltaE/\deltaAlpha * dataNormed / batchSize)
+		errors[dataIdx] = ((errors[dataIdx] -
+				    scaleGrad[dimIdx] * outNormed[dataIdx]/ batchSize -
+				    scaleGrad[dimIdx + layerSize] / batchSize ) *
+				   scale[dimIdx] / meanStd[dimIdx + layerSize]);
+	    }
+	}
+    };
+    
 } // anonymous namespace
 } // namespace internal
 
 
 namespace layers {
 
+    // Additional weight due to batch normalization
+    int wNum(const helpers::JsonValue &layerChild){
+	if (layerChild->HasMember("batchnorm") && ((*layerChild)["batchnorm"].GetInt()))
+	    return 3; // alpha, mean, std
+	else
+	    return 0;
+    }
+    
     template <typename TDevice, typename TActFn>
     FeedForwardLayer<TDevice, TActFn>::FeedForwardLayer(const helpers::JsonValue &layerChild, 
 							const helpers::JsonValue &weightsSection, 
 							Layer<TDevice> &precedingLayer)
-        : TrainableLayer<TDevice>(layerChild, weightsSection, 1, 0, precedingLayer)
+        : TrainableLayer<TDevice>(layerChild, weightsSection, 1, wNum(layerChild), precedingLayer)
     {
+	m_batchNorm = (wNum(layerChild)>0)? true : false;
 	
+	if (m_batchNorm){
+	    // initialization
+	    m_stdConst  = 0.001;
+	    m_batchCnt  = 0.0;
+	    m_preEpoch  = 1;
+	    
+	    // mean, std
+	    Cpu::real_vector tmp;
+	    tmp.resize(this->size() * 2, 0.0); 
+	    m_stats     = tmp;
+
+	    // all-one vector for vector summation
+	    tmp.resize(this->outputs().size()/this->size(), 1.0);
+	    m_oneVector = tmp;
+
+	    // a tempopary buff
+	    m_buff      = this->outputs();
+	    m_outNormed = this->outputs();
+		    
+	    if (weightsSection.isValid() && weightsSection->HasMember(this->name().c_str())) {
+		// read 
+	    
+	    }else{
+		// initialize 
+		int transMatrixWeightNum = this->size() * this->precedingLayer().size();
+		
+		// alpha = 1.0
+		thrust::fill(this->weights().begin() + transMatrixWeightNum,
+			     this->weights().begin() + transMatrixWeightNum + this->size(), 1.0);
+		// beta, mean, std
+		thrust::fill(this->weights().begin() + transMatrixWeightNum + this->size(),
+			     this->weights().end(), 0.0);
+	    }
+
+	    const Configuration &config = Configuration::instance();
+	    m_trainFlag = config.trainingMode();
+	}
     }
 
     template <typename TDevice, typename TActFn>
@@ -156,9 +358,13 @@ namespace layers {
     template <typename TDevice, typename TActFn>
     void FeedForwardLayer<TDevice, TActFn>::computeForwardPass()
     {
-		
-        // collect outputs from preceding layer
-        {{
+
+	// Fine, I am lazy to merge the code
+	if (!m_batchNorm){
+	    
+	    // The conventional feedforward part
+	    // collect outputs from preceding layer
+	    {{
             helpers::Matrix<TDevice> weightsMatrix  (&this->weights(),                  
 						     this->precedingLayer().size(), this->size());
 	    
@@ -174,10 +380,10 @@ namespace layers {
 						     this->parallelSequences());
 
             outputsMatrix.assignProduct(weightsMatrix, true, plOutputsMatrix, false);
-        }}
-
-        // calculate the outputs of the layer
-        {{
+	    }}
+	
+	    // calculate the outputs of the layer
+	    {{
             internal::ComputeOutputFn<TActFn> fn;
             fn.layerSize        = this->size();
             fn.bias             = this->bias();
@@ -192,13 +398,160 @@ namespace layers {
                 this->_outputs().begin(),
                 fn
                 );
-        }}
+	   }}
+	    
+	}else{
+	    // if batch normalization is used
+	    int transMatrixWeightNum = this->size() * this->precedingLayer().size();
+	    
+	    // Re-initialize the batch mean and variance
+	    if (m_trainFlag && m_preEpoch > 0 && m_preEpoch != this->getCurrTrainingEpoch()){
+		// always update the mean, std for each epoch
+		m_batchCnt = 0;
+		thrust::fill(this->weights().begin() + transMatrixWeightNum + 2 * this->size(),
+			     this->weights().end(),  0.0);
+		m_preEpoch = this->getCurrTrainingEpoch();
+	    }
+
+	    // Wx
+	    {{
+		helpers::Matrix<TDevice> weightsMatrix  (&this->weights(),                  
+							 this->precedingLayer().size(),
+							 this->size());
+		helpers::Matrix<TDevice> plOutputsMatrix(&this->precedingLayer().outputs(), 
+							 this->precedingLayer().size(), 
+							 this->curMaxSeqLength() * 
+							 this->parallelSequences());
+		helpers::Matrix<TDevice> outputsMatrix  (&this->m_outNormed,                 
+							 this->size(),                  
+							 this->curMaxSeqLength() * 
+							 this->parallelSequences());
+		outputsMatrix.assignProduct(weightsMatrix, true, plOutputsMatrix, false);
+	    }}	    
+
+	    // normalize the data
+	    m_batchCnt++;
+	    {{
+	       int maxFrameNum = this->curMaxSeqLength() * this->parallelSequences();
+	       int maxDataNum  = maxFrameNum * this->size();
+	       
+	       // Step1. calculate the batch size
+	       //        For parallel sentences, there is dummy node. BatchSize should not count it.
+	       internal::BatchSize fn0;
+	       fn0.patTypes = helpers::getRawPointer(this->patTypes());
+	       m_batchSize  = thrust::transform_reduce(
+				thrust::make_zip_iterator(
+				    thrust::make_tuple(
+					this->m_buff.begin(), 
+					thrust::counting_iterator<int>(0))),
+				thrust::make_zip_iterator(
+				    thrust::make_tuple(
+					this->m_buff.begin()              + maxFrameNum, 
+					thrust::counting_iterator<int>(0) + maxFrameNum)),
+				fn0, (real_t)0.0, thrust::plus<real_t>());
+	       
+	       thrust::fill(this->m_oneVector.begin(), this->m_oneVector.end(), 1.0/m_batchSize);
+	       
+	       // Step2. accumulate the mean
+	       internal::PrepareForMeanStd fn1;
+	       fn1.layerSize  = this->size();
+	       fn1.meanNotVar = true;
+	       fn1.mean       = NULL;
+	       fn1.patTypes   = helpers::getRawPointer(this->patTypes());
+	       fn1.data       = helpers::getRawPointer(this->m_outNormed);
+	       fn1.outdata    = helpers::getRawPointer(this->m_buff);	   
+	       thrust::for_each(
+		 thrust::make_zip_iterator(
+			thrust::make_tuple(this->outputs().begin(), 
+					   thrust::counting_iterator<int>(0))),
+		 thrust::make_zip_iterator(
+			thrust::make_tuple(this->outputs().begin()           + maxDataNum, 
+					   thrust::counting_iterator<int>(0) + maxDataNum)),
+		 fn1);
+	   
+	       helpers::Matrix<TDevice> onevec  (&this->m_oneVector, maxFrameNum,  1);
+	       helpers::Matrix<TDevice> data    (&this->m_buff,      this->size(), maxFrameNum);
+	       helpers::Matrix<TDevice> meanVec (&this->m_stats,     this->size(), 1);
+	       meanVec.assignProduct(data, false, onevec, false);
+
+	       // Step3. accumulate the var
+	       fn1.meanNotVar = false;
+	       fn1.mean       = helpers::getRawPointer(this->m_stats);; 
+	       thrust::for_each(
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->outputs().begin(), 
+					   thrust::counting_iterator<int>(0))),
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->outputs().begin()           + maxDataNum, 
+					   thrust::counting_iterator<int>(0) + maxDataNum)),
+		fn1);
+	       
+	       helpers::Matrix<TDevice> data2   (&this->m_buff,  this->size(), maxFrameNum);
+	       helpers::Matrix<TDevice> stdVec  (&this->m_stats, this->size(), 1, this->size());
+	       stdVec.assignProduct(data2, false, onevec, false);
+	       
+	       internal::GetStd fn3;
+	       fn3.stdConst = m_stdConst;
+	       thrust::for_each(
+		thrust::make_zip_iterator(
+			thrust::make_tuple(m_stats.begin() + this->size(), 
+					   thrust::counting_iterator<int>(0))),
+		thrust::make_zip_iterator(
+			thrust::make_tuple(m_stats.begin() + this->size() * 2, 
+					   thrust::counting_iterator<int>(0) + this->size())),
+		fn3);
+
+	       // Step4. accumulate the mean and std, for generation stage
+	       if (m_trainFlag){
+		   internal::AveMeanStd fn5;
+		   fn5.meanStdBuf = (helpers::getRawPointer(this->weights()) +
+				     transMatrixWeightNum + this->size() * 2);
+		   fn5.cnt        = m_batchCnt;
+		   thrust::for_each(
+		     thrust::make_zip_iterator(
+			thrust::make_tuple(m_stats.begin(), 
+					   thrust::counting_iterator<int>(0))),
+		     thrust::make_zip_iterator(
+			thrust::make_tuple(m_stats.begin() + this->size() * 2, 
+					   thrust::counting_iterator<int>(0) + this->size() * 2)),
+		     fn5);
+	       }
+	   
+	       // Step5: normalize and scale the data
+	       internal::ComputeBatchNorm_Transform<TActFn> fn2;
+	       fn2.layerSize = this->size();
+	       fn2.patTypes  = helpers::getRawPointer(this->patTypes());
+	       fn2.data      = helpers::getRawPointer(this->m_outNormed);
+	       fn2.outdata   = helpers::getRawPointer(this->outputs());
+	       fn2.scale     = helpers::getRawPointer(this->weights()) + transMatrixWeightNum;
+	       fn2.meanStd   = helpers::getRawPointer(this->m_stats);
+	       fn2.meanStdBuf= (helpers::getRawPointer(this->weights()) +
+				transMatrixWeightNum + this->size() * 2);
+	       fn2.trainFlag = m_trainFlag;
+	   
+	       thrust::for_each(
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->outputs().begin(), 
+					   thrust::counting_iterator<int>(0))),
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->outputs().begin()           + maxDataNum, 
+					   thrust::counting_iterator<int>(0) + maxDataNum)),
+		fn2);
+
+	    }}
+	}
+	
+	// done
     }
 
 
     template <typename TDevice, typename TActFn>
     void FeedForwardLayer<TDevice, TActFn>::computeForwardPass(const int timeStep)
     {
+	if (m_batchNorm){
+	    throw std::runtime_error("Error: batchnorm not available for online processing");
+	}
+	
 	int effTimeStep = timeStep * this->parallelSequences();
 	// collect outputs from preceding layer
         {{
@@ -239,23 +592,95 @@ namespace layers {
     template <typename TDevice, typename TActFn>
     void FeedForwardLayer<TDevice, TActFn>::computeBackwardPass()
     {
-        // compute deltas
-        {{
+	// compute deltas
+	{{
             internal::ComputeDeltaFn<TActFn> fn;
 
             int n = this->curMaxSeqLength() * this->parallelSequences() * this->size();
 
             thrust::for_each(
-                thrust::make_zip_iterator(thrust::make_tuple(this->outputErrors().begin(),   
-							     this->outputs().begin())),
-                thrust::make_zip_iterator(thrust::make_tuple(this->outputErrors().begin()+n, 
-							     this->outputs().begin()+n)),
-                fn
-                );
-        }}
+               thrust::make_zip_iterator(
+		  thrust::make_tuple(this->outputErrors().begin(),   this->outputs().begin())),
+	       thrust::make_zip_iterator(
+		  thrust::make_tuple(this->outputErrors().begin()+n, this->outputs().begin()+n)),
+                fn);
+	}}
+
+	if (m_batchNorm) {
+	    // for batch normalization
+	    int maxFrameNum          = this->curMaxSeqLength() * this->parallelSequences();
+	    int maxDataNum           = maxFrameNum * this->size();
+	    int transMatrixWeightNum = this->size() * this->precedingLayer().size();
+
+	    thrust::fill(m_oneVector.begin(),            m_oneVector.end(),            1.0);
+	    thrust::fill(m_buff.begin(),                 m_buff.end(),                 0.0);
+	    thrust::fill(this->_weightUpdates().begin(), this->_weightUpdates().end(), 0.0);
+	    
+	    
+	    // Step1. Calculate \deltaE/\delta{\alpha}
+	    internal::PrepareGrad fn1;
+	    fn1.layerSize    = this->size();
+	    fn1.alphaNotBeta = true;
+	    fn1.patTypes     = helpers::getRawPointer(this->patTypes());
+	    fn1.grad         = helpers::getRawPointer(this->outputErrors());
+	    fn1.data         = helpers::getRawPointer(this->m_outNormed);
+	    
+	    thrust::for_each(
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->m_buff.begin(), 
+					   thrust::counting_iterator<int>(0))),
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->m_buff.begin() + maxDataNum, 
+					   thrust::counting_iterator<int>(0) + maxDataNum)),
+		fn1);
+	   
+	    helpers::Matrix<TDevice> onevec    (&this->m_oneVector, maxFrameNum, 1);
+	    helpers::Matrix<TDevice> data      (&this->m_buff,      this->size(), maxFrameNum);
+	    helpers::Matrix<TDevice> gradAlpha (&this->_weightUpdates(), this->size(), 1,
+						transMatrixWeightNum);
+	   gradAlpha.assignProduct(data, false, onevec, false);
+
+	   // Step2. Calculate \deltaE/\delta{\beta}
+	   fn1.alphaNotBeta = false;	   
+	   thrust::for_each(
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->m_buff.begin(), 
+					   thrust::counting_iterator<int>(0))),
+		thrust::make_zip_iterator(
+			thrust::make_tuple(this->m_buff.begin() + maxDataNum, 
+					   thrust::counting_iterator<int>(0) + maxDataNum)),
+		fn1);
+	   
+	   helpers::Matrix<TDevice> gradBeta (&this->_weightUpdates(), this->size(),1,
+					      transMatrixWeightNum + this->size());
+	   gradBeta.assignProduct(data, false, onevec, false);
+	   
+
+	   // Step3. Calculate \deltaE/\delta{x}
+	   internal::ComputeBatchGradient_output fn2;
+	   fn2.layerSize = this->size();
+	   fn2.patTypes  = helpers::getRawPointer(this->patTypes());
+	   fn2.errors    = helpers::getRawPointer(this->outputErrors());
+	   fn2.outNormed = helpers::getRawPointer(m_outNormed);
+	   fn2.meanStd   = helpers::getRawPointer(m_stats);
+	   fn2.scale     = helpers::getRawPointer(this->weights())        + transMatrixWeightNum;
+	   fn2.scaleGrad = helpers::getRawPointer(this->_weightUpdates()) + transMatrixWeightNum;
+	   fn2.batchSize = m_batchSize;
+	   
+	   thrust::for_each(
+		thrust::make_zip_iterator(
+			thrust::make_tuple(m_outNormed.begin(), 
+					   thrust::counting_iterator<int>(0))),
+		thrust::make_zip_iterator(
+			thrust::make_tuple(m_outNormed.begin() + maxDataNum, 
+					   thrust::counting_iterator<int>(0) + maxDataNum)),
+		fn2);
+
+	}
 
-        // back-propagate the error to the preceding layer
-        {{
+	
+	// back-propagate the error to the preceding layer
+	{{
             TrainableLayer<TDevice> *pl = 
 		dynamic_cast<TrainableLayer<TDevice>*>(&this->precedingLayer());
 	    
@@ -295,10 +720,10 @@ namespace layers {
 		    plErrorsMatrix.assignProduct(weightsMatrix, false, deltasMatrix, false);
 		}
 	    }
-        }}
+	}}
 
-        // compute the input weight updates
-        {{
+	// compute the input weight updates
+	{{
             helpers::Matrix<TDevice> weightUpdatesMatrix(&this->_weightUpdates(),           
 							 this->precedingLayer().size(), 
 							 this->size());
@@ -314,10 +739,11 @@ namespace layers {
 							 this->parallelSequences());
 
             weightUpdatesMatrix.assignProduct(plOutputsMatrix, false, deltasMatrix, true);
-        }}
+	}}
 
-        // compute the bias weight updates
-        {{
+	if (!m_batchNorm){
+	    // compute the bias weight updates
+	    {{
             internal::ComputeBiasWeightUpdateFn fn;
             fn.layerSize     = this->size();
             fn.patternsCount = this->curMaxSeqLength() * this->parallelSequences();
@@ -330,24 +756,29 @@ namespace layers {
                 this->_weightUpdates().begin() + this->precedingLayer().size() * this->size(),
                 fn
                 );
-        }}
-	
-	/*
-	if (this->_optOpt()){
-	    
-	    {{
-		internal::GradientAverage fn;
-		fn.timeStep = (real_t)(this->curMaxSeqLength() * this->parallelSequences());
-		fn.gradients= helpers::getRawPointer(this->_weightUpdates());		
-		thrust::for_each(
-				 thrust::counting_iterator<int>(0),
-				 thrust::counting_iterator<int>(0) + this->_weightUpdates().size(),
-				 fn
-                );
-
 	    }}
-	    
-	    }*/
+	}
+	/* Gradient averaging ?
+	      if (this->_optOpt()){
+	      {{
+	      internal::GradientAverage fn;
+	      fn.timeStep = (real_t)(this->curMaxSeqLength() * this->parallelSequences());
+	      fn.gradients= helpers::getRawPointer(this->_weightUpdates());		
+	      thrust::for_each(
+	      thrust::counting_iterator<int>(0),
+	      thrust::counting_iterator<int>(0) + this->_weightUpdates().size(),
+	      fn);
+	      }}
+	      }*/
+    }
+
+    template <typename TDevice, typename TActFn>
+    void FeedForwardLayer<TDevice, TActFn>::exportLayer(
+	const helpers::JsonValue     &layersArray, 
+	const helpers::JsonAllocator &allocator) const
+    {
+        TrainableLayer<TDevice>::exportLayer(layersArray, allocator);
+        (*layersArray)[layersArray->Size() - 1].AddMember("batchnorm", (int)m_batchNorm, allocator);
     }
 
 
diff --git a/layers/FeedForwardLayer.hpp b/layers/FeedForwardLayer.hpp
index c794243..20bd8e4 100644
--- a/layers/FeedForwardLayer.hpp
+++ b/layers/FeedForwardLayer.hpp
@@ -37,6 +37,24 @@ namespace layers {
     template <typename TDevice, typename TActFn>
     class FeedForwardLayer : public TrainableLayer<TDevice>
     {
+	typedef typename TDevice::real_vector real_vector;
+	typedef typename TDevice::int_vector  int_vector;
+	typedef typename TDevice::bool_vector bool_vector;
+	typedef typename TDevice::pattype_vector pattype_vector;
+
+	bool m_batchNorm;            // whether to use batch normalization
+	real_vector m_stats;         // mean and variance of each batch
+	real_vector m_outNormed;     // normed data output without being scaled
+	
+	real_t      m_stdConst;      // const floor for the var
+	real_t      m_batchCnt;
+	bool        m_trainFlag;
+	int         m_preEpoch;
+	real_t      m_batchSize;     //
+
+	real_vector m_oneVector;     // all-one vector
+	real_vector m_buff;
+
     public:
         /**
          * Constructs the Layer
@@ -76,7 +94,12 @@ namespace layers {
 	 * 
 	 */
 	virtual void computeForwardPass(const int timeStep);
-	
+
+
+	// export
+	virtual void exportLayer(const helpers::JsonValue &layersArray, 
+				 const helpers::JsonAllocator &allocator) const;
+
     };
 
 } // namespace layers
